{
    "version": "https://jsonfeed.org/version/1",
    "title": "Keep Moving • All posts by \"ai\" tag",
    "description": "🌸学习笔记🌸",
    "home_page_url": "http://qianqiu-cell.github.io",
    "items": [
        {
            "id": "http://qianqiu-cell.github.io/2024/10/05/AI/MAE/",
            "url": "http://qianqiu-cell.github.io/2024/10/05/AI/MAE/",
            "title": "MAE",
            "date_published": "2024-10-04T16:00:00.000Z",
            "content_html": "<h1 id=\"一-mae概述\"><a class=\"markdownIt-Anchor\" href=\"#一-mae概述\">#</a> 一、MAE 概述</h1>\n<p>  深度学习在计算机视觉领域取得了显著进展，但随着模型规模的增长，对数据的需求也在增加。在自然语言处理（ <code>NLP</code> ）领域，通过自监督预训练的方法（如 <code>BERT</code>  和 <code>GPT</code> ）成功解决了数据需求问题，这些方法通过预测数据中被 <code>masked</code>  的部分来训练模型。然而，在计算机视觉领域，尽管存在相关研究，自监督学习方法的发展仍然滞后于 <code>NLP</code> 。</p>\n<p>  这篇论文使用掩码自编码器 ( <code>masked autoencoders (MAE)</code> ) 进行自监督学习。这种类型自监督学习的另一个著名的例子就是 <code>BERT</code> 。</p>\n<p>  对于 <code>BERT</code>  模型而言，一个 <code>sentence</code>  中间盖住一些 <code>tokens</code> ，让模型去预测，令得到的预测结果与真实的 <code>tokens</code>  之间的误差作为损失。它告诉了我们直接 <code>reconstruct sentence</code>  也可以做到很 <code>work</code> 。</p>\n<p>  对于 <code>MAE</code>  模型而言，一个 <code>image</code>  中间盖住一些  <code>patches</code> ，让模型去预测，令得到的预测结果与真实的 <code>image patches</code>  之间的误差作为损失。它告诉了我们直接 <code>reconstruct image</code>  原图也可以做到很 <code>work</code> 。</p>\n<p>  为什么 <code>BERT (2018)</code>  提出这么久以后，直到 <code>BEIT (2021.6)</code>  和 <code>MAE (2021.11)</code>  之前，一直在 <code>CV</code>  领域都没有一个很类似的 <code>CV BERT</code>  出现？这里 <code>Kaiming</code>  提出了 3 条看法：</p>\n<ul>\n<li><strong>CV 和 NLP 主流架构不同</strong>：直到 <code>ViT (2020.12)</code>  出现之前， <code>CV</code>  的主流架构一直是以<strong>卷积神经网络</strong>为主， <code>NLP</code>  的主流架构一直是以 <code>Transformer</code>  为主。卷积核作用在一个个的 <code>grid</code>  上面，直观来讲没法产生像 <code>Transformer</code>  一样的 <code>token</code>  的概念，也就是说如果我们只使用卷积网络，那么 <code>image token</code>  概念的建立就不那么直观。所以，像 <code>Transformer</code>  那样在 <code>token</code>  的基础上进行自监督学习就不太适用，这是第一个难点。</li>\n<li><strong>语言和图片 (视频) 的信息密度不同</strong>：语言是人类造就的信号，它 <code>highly semantic</code> ， <code>information-dense</code> 。而图片 (视频) 是自然产生的信号，它 <code>heavy spatial redundancy</code> 。即挡住图片的一部分 <code>patches</code> ，可以很容易地通过看它周围的 <code>patches</code>  而想象出它的样子来。所以，语言和图像，一个信息密度高，一个信息密度低，这是第二个难点。解决的办法是什么呢？作者提出了一个简单的策略：<strong>即挡住图片的 patches 的比例高一些</strong>。比如之前你挡住一张图片的 <code>30%</code>  的 <code>patches</code> ，能够轻松通过周围的 <code>patches</code>  预测出来；那现在如果<strong>挡住图片的 90% 的 patches</strong>，还能够轻松通过周围的 <code>patches</code>  预测出来吗？</li>\n<li><strong>AutoEncoder 里面的 Decoder 部分在 CV 和 NLP 中充当的角色不同</strong>：在 <code>CV</code>  领域， <code>Decoder</code>  的作用是重建 <code>image pixels</code> ，所以 <code>Decoder</code>  的输出语义级别很低。在 <code>NLP</code>  领域， <code>Decoder</code>  的作用是重建 <code>sentence words</code> ，所以 <code>Decoder</code>  的输出语义级别很丰富。</li>\n</ul>\n<p><img data-src=\"/images/AI/MAE/0.0.png\" alt=\"\"></p>\n<p>  基于以上分析，作者提出了 <code>MAE</code>  方法，其架构如上图所示。 <code>MAE</code>  的方法很简单： <code>Mask</code>  掉输入图像的随机的 <code>patches</code>  并重建它们。它基于两个核心理念：</p>\n<ul>\n<li>作者开发了一个非对称 <code>Encoder-Decoder</code>  架构，其中一个 <code>Encoder</code>  只对可见的 <code>patch</code>  子集进行操作 (<strong>即没有被 mask 掉的 token</strong>)，另一个简单轻量化的 <code>Decoder</code>  可以<strong>从潜在表征和被 masked 掉的 token 重建原始图像</strong>。</li>\n<li>研究人员进一步发现，<strong>Mask 掉大部分输入图像</strong> (例如 <code>75%</code> ) 会产生重要且有意义的自监督任务。</li>\n</ul>\n<p>  结合这两种设计，就能高效地训练大型模型：提升训练速度至 3 倍或更多，并提高准确性。</p>\n<p>   <code>MAE</code>  方法严格来讲属于一种去噪自编码器 ( <code>Denoising Auto-Encoders (DAE)</code> )， <code>DAE</code>  是一类自动编码器，它破坏输入信号，并学会重构原始的、未被破坏的信号。 <code>MAE</code>  的  <code>Encoder</code>  和 <code>Decoder</code>  结构不同，是非对称式的。 <code>Encoder</code>  将输入编码为 <code>latent representation</code> ，而 <code>Decoder</code>  将从 <code>latent representation</code>  重建原始信号。</p>\n<p>   <code>MAE</code>  和 <code>ViT</code>  的做法一致，将图像划分成规则的，不重叠的 <code>patches</code> 。然后按照均匀分布不重复地选择一些 <code>patches</code>  并且 <code>mask</code>  掉剩余的 <code>patches</code> 。作者采用的 <code>mask ratio</code>  足够高，因此大大减小了 <code>patches</code>  的冗余信息，使得在这种情况下重建 <code>images</code>  不那么容易。</p>\n<h1 id=\"二-mae方法\"><a class=\"markdownIt-Anchor\" href=\"#二-mae方法\">#</a> 二、MAE 方法</h1>\n<h2 id=\"21-mask\"><a class=\"markdownIt-Anchor\" href=\"#21-mask\">#</a> 2.1 Mask</h2>\n<p>  根据 <code>ViT</code> ， <code>MAE</code>  将图像划分为规则的不重叠的 <code>patch</code> 。然后对 <code>patch</code>  的子集进行采样，并 <code>mask</code>  (即删除) 剩余的 <code>patch</code> 。 <code>MAE</code>  的采样策略很简单：对随机 <code>patch</code>  进行采样，遵循均匀分布。简单地称之为 “随机抽样”。</p>\n<p>   <code>具有高</code>  mask ratio`(即去除斑块的比例) 的随机采样在很大程度上消除了冗余，从而创建了一个不能通过从可见的邻近斑块外推轻松解决的任务。均匀分布防止了潜在的中心偏差 (即在图像中心附近有更多的掩蔽斑块)。最后，高度稀疏的输入为设计高效的编码器创造了机会。</p>\n<h2 id=\"22-mae-encoder\"><a class=\"markdownIt-Anchor\" href=\"#22-mae-encoder\">#</a> 2.2 MAE encoder</h2>\n<p>   <code>MAE</code>  的编码器是 <code>ViT</code> ，但只应用于可见的，未 <code>mask</code>  的 <code>patches</code> 。就像在标准 <code>ViT</code>  中一样， <code>MAE</code>  的编码器通过添加 <code>positional embedding</code>  的线性投影获得每个 <code>patch</code>  的 <code>embedding</code> ，然后通过一系列 <code>transformer</code>  块处理结果集。 <code>MAE</code>  的编码器只在完整 <code>patches</code>  的一小部分 (例如，25%) 上运行。 <code>masked patches</code>  被移除；且不使用掩码令牌。这允许 <code>MAE</code>  只用一小部分的计算和内存<strong>训练非常大的编码器</strong>。完整的 <code>patches</code>  由一个轻量级解码器处理。</p>\n<h2 id=\"23-mae-decoder\"><a class=\"markdownIt-Anchor\" href=\"#23-mae-decoder\">#</a> 2.3 MAE decoder</h2>\n<p>   <code>MAE</code>  解码器的输入是一个完整的 <code>tokens</code> ，由 (i) 编码的 <code>visible patches</code>  和 (ii) <code>masked tokens</code>  组成。每个 <code>masked tokens</code>  是一个共享的、可学习的向量，表示待预测的 <code>missing patch</code> 。为这个完整 <code>tokens</code>  中的所有 <code>token</code>  添加 <code>position embedding</code> ；解码器有另一系列的 <code>transformer</code>  块。</p>\n<p>   <code>MAE</code>  解码器<strong>仅在预训练期间用于执行图像重建任务 (仅编码器用于生成用于识别的图像表示)</strong>。因此，<strong>解码器架构可以以一种独立于编码器设计的方式灵活设计</strong>。 <code>MAE</code>  用非常小的解码器做实验，比编码器更窄更浅。例如，与编码器相比，我们的默认解码器每个令牌的计算量 &lt; 10%。通过这种不对称设计，整个标记集只由轻量级解码器处理，<strong>这大大减少了预训练时间</strong>。</p>\n<h2 id=\"24-reconstruction-target\"><a class=\"markdownIt-Anchor\" href=\"#24-reconstruction-target\">#</a> 2.4 Reconstruction target</h2>\n<p>   <code>MAE</code>  通过预测每个 <code>masked patch</code>  的像素值来重构输入。解码器输出中的每个元素都是表示一个 <code>patch</code>  的像素值向量。解码器的最后一层是线性投影，<strong>其输出通道的数量等于 <code>patch</code>  中的像素值的数量</strong>。 <code>MAE</code>  的<strong>损失函数在像素空间中计算重建图像和原始图像之间的均方误差</strong>（ <code>MSE</code> ）。 <code>MAE</code>  只计算 <code>masked patches</code>  上的损失，类似于 <code>BERT</code> 。</p>\n<p>   <code>MAE</code>  还研究了一种变体，其重建目标是每个 <code>masked patch</code>  的归一化像素值。具体地说， <code>MAE</code>  计算一个 <code>patch</code>  中所有像素的平均值和标准差，并使用它们来归一化该 <code>patch</code> 。在 <code>MAE</code>  的实验中，使用归一化像素作为重建目标改善了表示质量。</p>\n<h2 id=\"25-simple-implementation\"><a class=\"markdownIt-Anchor\" href=\"#25-simple-implementation\">#</a> 2.5 Simple implementation</h2>\n<p>   <code>MAE</code>  的预训练可以有效地实现，并且重要的是，不需要任何专门的稀疏操作。</p>\n<ul>\n<li>首先，我们为每个输入 <code>patch</code>  生成一个 <code>token</code> （通过添加 <code>position embedding</code>  的线性投影）。</li>\n<li>接下来，我们随机打乱 <code>token</code>  列表，并根据 <code>mask ratio</code>  删除列表的最后一部分。此过程为编码器生成一个小的 <code>token</code>  子集。</li>\n<li>在编码之后，我们将 <code>mask token</code>  的列表添加至编码的 <code>token</code>  列表，并还原此完整的列表（反转随机打乱操作），以将所有 <code>token</code>  与其目的对齐。</li>\n<li>解码器应用于该完整列表（添加了位置嵌入）。</li>\n</ul>\n<p>  如上所述，不需要稀疏操作。由于打乱和还原操作是快速的，因此这种简单的实现引入了可忽略的开销。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/09/21/AI/ViT/",
            "url": "http://qianqiu-cell.github.io/2024/09/21/AI/ViT/",
            "title": "ViT",
            "date_published": "2024-09-20T16:00:00.000Z",
            "content_html": "<h1 id=\"一-引言\"><a class=\"markdownIt-Anchor\" href=\"#一-引言\">#</a> 一、引言</h1>\n<p>  虽然  <code>Transformer</code>  架构已成为  <code>NLP</code>  任务的首选模型，但它在  <code>CV</code>  中的应用仍然有限。在视觉上，注意力要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，同时保持其整体结构。而这种对  <code>CNNs</code>  的依赖是不必要的，直接应用于图像块序列  ( <code>sequences of image patches</code> ) 的纯  <code>Transformer</code>  可以很好地执行图像分类任务。当对大量数据进行预训练并迁移到多个中小型图像识别基准时 ( <code>ImageNet</code> 、 <code>CIFAR-100</code> 、 <code>VTAB</code>  等)，与  <code>SOTA</code>  的  <code>CNN</code>  相比， <code>Vision Transformer (ViT)</code>  可获得更优异的结果，同时仅需更少的训练资源。</p>\n<h1 id=\"二-方法\"><a class=\"markdownIt-Anchor\" href=\"#二-方法\">#</a> 二、方法</h1>\n<p>  在模型设计中， <code>ViT</code>  尽可能地遵循原始  <code>Transformer</code> 。 这种有意简单设置的优势在于，可扩展的 <code>NLP Transformer</code>  架构及其高效实现几乎可以开箱即用。</p>\n<p><img data-src=\"/images/AI/ViT/2.1.png\" alt=\"\"></p>\n<h2 id=\"21-图像块嵌入patch-embeddings\"><a class=\"markdownIt-Anchor\" href=\"#21-图像块嵌入patch-embeddings\">#</a> 2.1 图像块嵌入 (Patch Embeddings)</h2>\n<p>  模型概述如图 1 所示。标准 <code>Transformer</code>  使用一维标记嵌入序列 ( <code>Sequence of token embeddings</code> ) 作为输入。为了处理 <code>2D</code>  图像，将图像<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x\\in \\mathbb{R}^{H\\times W\\times C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">C</span></span></span></span></span></span></span></span></span></span></span></span>  <code>reshape</code>  为一个展平的 <code>2D patches</code>  序列</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>x</mi><mi>p</mi></msub><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>N</mi><mo>×</mo><mo stretchy=\"false\">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo separator=\"true\">⋅</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_p \\in \\mathbb{R}^{N\\times (P^2·C)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8252079999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0369199999999998em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0369199999999998em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913142857142857em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mpunct mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p>  其中 (H，W) 是原始图像的分辨率，C 是通道的数目，（P，P）是每个图像片的分辨率，并且<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mi>W</mi><mi mathvariant=\"normal\">/</mi><msup><mi>P</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">N = HW/P^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.064108em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\">/</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span> 是得到图像 patch 的数，也是 <code>Transformer</code>  的有效输入序列长度。 <code>Transformer</code>  在其所有层中使用恒定的潜在向量大小 <code>D</code> ，因此 <code>ViT</code>  将 pathches 展平，并使用 ** 可训练的线性投影 (FC 层)** 将<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>P</mi><mn>2</mn></msup><mo separator=\"true\">⋅</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">P^2·C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mpunct\">⋅</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 映射到 <code>D</code>  维，同时保持图像 <code>patches</code>  数 <code>N</code>  不变。该投影的输出被称为 <code>patch embeddings</code> 。</p>\n<p>  上述投影输出即为图像块嵌入 (Patch Embeddings)，本质就是对每一个展平后的 <code>patch vector</code>  <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>p</mi></msub><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>N</mi><mo>×</mo><mo stretchy=\"false\">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo separator=\"true\">⋅</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_p\\in \\mathbb{R}^{N\\times (P^2·C)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8252079999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9869199999999998em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9869199999999998em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913142857142857em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mpunct mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span></span></span></span> 做一个线性变换 / 全连接层 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo separator=\"true\">⋅</mo><mi>C</mi><mo stretchy=\"false\">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E\\in \\mathbb{R}^{(P^2·C)\\times D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9869199999999998em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9869199999999998em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913142857142857em;\"><span style=\"top:-2.931em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mpunct mtight\">⋅</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">C</span><span class=\"mclose mtight\">)</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span></span></span></span></span></span></span></span>，由<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>P</mi><mn>2</mn></msup><mo>×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">P^2\\times C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 降维至<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span> 维，得到<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>p</mi></msub><mi>E</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x_pE\\in \\mathbb{R}^{N\\times N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">p</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span></span></span></span></span></span></span></span>。类似于 NLP 中的词嵌入（Word Embeddings）。</p>\n<p>图像块嵌入的参考程序为：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">PatchEmbed</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token triple-quoted-string string\">\"\"\" Image to Patch Embedding \"\"\"</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre> </pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> img_size<span class=\"token operator\">=</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span> patch_size<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> in_chans<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> embed_dim<span class=\"token operator\">=</span><span class=\"token number\">768</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>        <span class=\"token comment\"># (H, W)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>        img_size <span class=\"token operator\">=</span> to_2tuple<span class=\"token punctuation\">(</span>img_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>        <span class=\"token comment\"># (P, P)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>        patch_size <span class=\"token operator\">=</span> to_2tuple<span class=\"token punctuation\">(</span>patch_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>        <span class=\"token comment\"># N = (H // P) * (W // P)</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>        num_patches <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>img_size<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">//</span> patch_size<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>img_size<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">//</span> patch_size<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>        </pre></td></tr><tr><td data-num=\"13\"></td><td><pre>        self<span class=\"token punctuation\">.</span>img_size <span class=\"token operator\">=</span> img_size</pre></td></tr><tr><td data-num=\"14\"></td><td><pre>        self<span class=\"token punctuation\">.</span>patch_size <span class=\"token operator\">=</span> patch_size</pre></td></tr><tr><td data-num=\"15\"></td><td><pre>        self<span class=\"token punctuation\">.</span>num_patches <span class=\"token operator\">=</span> num_patches</pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        </pre></td></tr><tr><td data-num=\"17\"></td><td><pre>        <span class=\"token comment\"># 可训练的线性投影 - 获取输入嵌入</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>        self<span class=\"token punctuation\">.</span>proj <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span>in_chans<span class=\"token punctuation\">,</span> embed_dim<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span>patch_size<span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span>patch_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre> </pre></td></tr><tr><td data-num=\"20\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>        B<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">,</span> H<span class=\"token punctuation\">,</span> W <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape</pre></td></tr><tr><td data-num=\"22\"></td><td><pre>        <span class=\"token comment\"># (B, C, H, W) -> (B, D, (H//P), (W//P)) -> (B, D, N) -> (B, N, D)</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>        <span class=\"token comment\">#   D=embed_dim=768, N=num_patches=(H//P)*(W//P)</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>        <span class=\"token comment\">#   torch.flatten (input, start_dim=0, end_dim=-1)  # 形参：展平的起始维度和结束维度    </span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>        <span class=\"token comment\"># 可见 Patch Embedding 操作 1 行代码 3 步到位</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>proj<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>flatten<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>        <span class=\"token keyword\">return</span> x</pre></td></tr></table></figure><h2 id=\"22-可学习的嵌入learnable-embedding\"><a class=\"markdownIt-Anchor\" href=\"#22-可学习的嵌入learnable-embedding\">#</a> 2.2 可学习的嵌入（Learnable Embedding）</h2>\n<p>  类似于 BERT 的 [class] token， <code>ViT</code>  为图像 patch 嵌入序列预设一个可学习的嵌入<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>z</mi><mn>0</mn></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">z_0=x_{class}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">c</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">s</span><span class=\"mord mathnormal mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，其在 Transformer 编码器输出的状态 / 特征<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">z^0_L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.089439em;vertical-align:-0.275331em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-2.424669em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.275331em;\"><span></span></span></span></span></span></span></span></span></span> 用作图像表示。在预训练和微调期间，将分类头附加到<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">z^0_L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.089439em;vertical-align:-0.275331em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-2.424669em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.275331em;\"><span></span></span></span></span></span></span></span></span></span> 之后，从而用于图像分类。分类头在预训练时由具有一个隐藏层的 <code>MLP</code>  实现，在微调时由单个线性层实现。</p>\n<p>  更明确地，假设将图像分为 N 个图像块，输入到 Transformer 编码器中就有 N 个向量，但是这些向量都不适合用来作为分类预测。一个合理的做法是手动添加一个可学习的嵌入向量作为用于分类的类别向量同时与其他图像块嵌入向量一起输入到 Transformer 编码器中，最后取追加的首个可学习的嵌入向量作为类别预测结果。所以，追加的首个类别向量可理解为其他 N 个图像块寻找的类别信息。从而，最终输入 Transformer 的嵌入向量总长度为 N+1。可学习嵌入 在训练时随机初始化，然后通过训练得到，其具体实现为：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">### 随机初始化</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>self<span class=\"token punctuation\">.</span>cls_token <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Parameter<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> embed_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># shape = (1, 1, D)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre> </pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># 按通道拼接 获取 N+1 维 Embeddings</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>cls_tokens<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># shape = (B, N+1, D)</span></pre></td></tr></table></figure><h2 id=\"23-位置嵌入-position-embeddings\"><a class=\"markdownIt-Anchor\" href=\"#23-位置嵌入-position-embeddings\">#</a> 2.3 位置嵌入 (Position Embeddings)</h2>\n<p>  位置嵌入<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E_{pos}\\in \\mathbb{R}^{(N+1)\\times D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8879999999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span></span></span></span></span></span></span></span> 也被加入图像块嵌入，以保留输入图像块之间的空间位置信息。若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。ViT 论文中对比了几种不同的位置编码方案：</p>\n<ul>\n<li>位置嵌入</li>\n<li>1-D 位置嵌入 (1D-PE)：考虑把 2-D 图像块视为 1-D 序列</li>\n<li>2-D 位置嵌入 (2D-PE)：考虑图像块的 2-D 位置 (x, y)</li>\n<li>相对位置嵌入 (RPE)：考虑图像块的相对位置</li>\n</ul>\n<p>  最后发现如果 不提供位置编码效果会差，但其它各种类型的编码效果效果都接近，这主要是因为 ViT 的输入是相对较大的图像块而非像素，所以学习位置信息相对容易很多。</p>\n<p>  Transformer 原文中默认采用 固定位置编码，ViT 则采用 标准可学习 / 训练的 1-D 位置编码嵌入，因为尚未观察到使用更高级的 2-D-aware 位置嵌入 (附录 D.4) 能够带来显著的性能提升 (当然，后续的很多 ViT 变体也使用了 2-D 位置嵌入)。在输入 Transformer 编码器之前直接 将图像块嵌入和位置嵌入按元素相加：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># 多 +1 是为了加入上述的 class token</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># embed_dim 即 patch embed_dim</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>self<span class=\"token punctuation\">.</span>pos_embed <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Parameter<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> num_patches <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> embed_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> </pre></td></tr><tr><td data-num=\"4\"></td><td><pre> </pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\"># patch emded + pos_embed ：图像块嵌入 + 位置嵌入</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>x <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>pos_embed</pre></td></tr></table></figure>",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/09/19/AI/SimCLR/",
            "url": "http://qianqiu-cell.github.io/2024/09/19/AI/SimCLR/",
            "title": "SimCLR",
            "date_published": "2024-09-18T16:00:00.000Z",
            "content_html": "<h1 id=\"一-引言\"><a class=\"markdownIt-Anchor\" href=\"#一-引言\">#</a> 一、引言</h1>\n<p>  在没有人类监督的情况下学习有效的视觉表征是一个长期存在的问题。大多数主流方法可分为两类：生成式或判别式。生成式方法学习在输入空间中生成或以其他方式建模像素。然而，像素级生成在计算上是昂贵的，并且对于表示学习可能不是必需的。判别方法使用类似于用于监督学习的目标函数来学习表征，但训练网络执行借口任务，其中输入和标签都来自未标记的数据集。许多这样的方法都依赖于启发式来设计借口任务，这可能会限制学习表征的一般性。基于潜在空间中对比学习的判别方法最近显示出很大的前景，取得了最先进的结果。</p>\n<p>  其中 <code>SimCLR</code>  为视觉表征的对比学习引入了一个简单的框架。 <code>SimCLR</code>  不仅优于以前的工作，而且更简单，不需要专门的架构，也不是记忆库。通过系统地研究 <code>SimCLR</code>  框架，表明了以下促成了良好对比表征学习的主要原因:</p>\n<ul>\n<li><strong>多个数据增强操作的组合</strong>对于定义产生<strong>有效表示的对比预测任务</strong>至关重要。此外，与有监督学习相比，<strong>无监督对比学习受益于更强的数据增强</strong>。</li>\n<li><strong>在表示和对比损失之间引入可学习的非线性变换</strong>，大大提高了学习到的表示的质量。</li>\n<li>具有<strong>对比交叉熵损失的表征学习</strong>得益于<strong>归一化嵌入和适当调整的温度参数</strong>。</li>\n<li>对比学习与监督学习相比，<strong>得益于更大的批处理规模和更长的训练时间</strong>。与监督式学习一样，<strong>对比式学习也受益于更深入、更广泛的网络</strong>。</li>\n</ul>\n<h1 id=\"二-对比学习框架\"><a class=\"markdownIt-Anchor\" href=\"#二-对比学习框架\">#</a> 二、对比学习框架</h1>\n<p>  受最近的对比学习算法的启发， <code>SimCLR</code>  通过潜在空间中的对比损失来最大化相同数据示例的不同增强视图之间的一致性来学习表示。如下图所示，该框架由以下四个主要组件组成：</p>\n<p><img data-src=\"/images/AI/SimCLR/2.1.png\" alt=\"\"></p>\n<ul>\n<li><strong>一种随机数据增强模块<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">)</mo><mo>∼</mo><mi>τ</mi></mrow><annotation encoding=\"application/x-tex\">t(t&#x27;)\\sim \\tau</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span></span></span></span></strong>，<strong>它将任意给定的数据示例<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span> 通过数据增强随机转换为同一示例的两个相关视图</strong>，表示为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9539679999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>，我们将其视为正样本对。在 <code>SimCLR</code>  中，依次应用三种简单的增强：随机裁剪，然后将大小调整回原始大小，随机颜色失真和随机高斯模糊。随机裁剪和颜色失真的结合对于获得良好的性能至关重要。</li>\n<li><strong>一种神经网络基础编码器<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mo separator=\"true\">⋅</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(·)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mpunct\">⋅</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mclose\">)</span></span></span></span></strong>，<strong>从增强的数据示例中提取表示向量</strong>。 <code>SimCLR</code>  允许在没有任何限制的情况下选择各种网络架构。为了简单起见，采用常用的 ResNet 得到<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>ResNet</mtext><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">h_i=f(\\tilde{x}_i)=\\text{ResNet}(\\tilde{x}_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">ResNet</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>，其中<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>∈</mo><msub><mi mathvariant=\"double-struck\">R</mi><mi>d</mi></msub></mrow><annotation encoding=\"application/x-tex\">h_i\\in \\mathbb{R}_d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83889em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">d</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 是平均池化层后的输出。</li>\n<li><strong>一个小的神经网络投影头<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mo separator=\"true\">⋅</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(·)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mpunct\">⋅</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mclose\">)</span></span></span></span></strong>，<strong>它表示映射到应用对比损失的空间</strong>。 <code>SimCLR</code>  使用具有一个隐藏层的 <code>MLP</code>  得到<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mi>σ</mi><mo stretchy=\"false\">(</mo><msup><mi>W</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><msub><mi>h</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">z_i = g(h_i) = W^{(2)}\\sigma(W^{(1)}h_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.138em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">2</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8879999999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>，其中<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span> 是一个 <code>ReLU</code>  非线性。将对比损失定义在<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">z_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 上而不是<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">h_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">h</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 上是有益的。</li>\n<li><strong>为对比预测任务定义的对比损失函数</strong>。给定一个集合<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，其中包含一对正的例子<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9539679999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>，对比预测任务旨在识别给定<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8178599999999999em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，在<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mo fence=\"true\">{</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>k</mi></msub><mo fence=\"true\">}</mo></mrow><mrow><mi>k</mi><mo mathvariant=\"normal\">≠</mo><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\left \\{ \\tilde{x}_k \\right \\} _{k\\ne i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.185808em;vertical-align:-0.435808em;\"></span><span class=\"minner\"><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">{</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\">}</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1864079999999999em;\"><span style=\"top:-2.4003000000000005em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel mtight\"><span class=\"mrel mtight\"><span class=\"mord vbox mtight\"><span class=\"thinbox mtight\"><span class=\"rlap mtight\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel mtight\"></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel mtight\">=</span></span><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.435808em;\"><span></span></span></span></span></span></span></span></span></span> 中的<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9539679999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">~</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>。</li>\n</ul>\n<p>  通过随机抽取一个由 <code>n</code>  个样本组成的小批量样本，对该小批量样本进行数据增强，得到 <code>2N</code>  个数据点。对于每一个增强后的数据，都对应一个正样本，将增强后数据中的其他 2 (N−1) 个增强示例视为负示例。设<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>sim</mtext><mo stretchy=\"false\">(</mo><mi>u</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>u</mi><mtext>T</mtext></msup><mi>v</mi><mi mathvariant=\"normal\">/</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>u</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>v</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(u, v)=u^{\\textrm{T}}v/||u||||v||</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">sim</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">u</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">u</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord textrm mtight\">T</span></span></span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord\">/</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">u</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span></span></span></span> 表示归一化<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">u</span></span></span></span> 与<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span></span></span></span> 之间的点积 (即余弦相似度)。则一对正样本<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(i, j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">i</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span><span class=\"mclose\">)</span></span></span></span> 的损失函数定义为</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"normal\">ℓ</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold-italic\">z</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold-italic\">z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">/</mi><mi>τ</mi><mo stretchy=\"false\">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mn>2</mn><mi>N</mi></mrow></munderover><msub><mn mathvariant=\"double-struck\">1</mn><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo mathvariant=\"normal\">≠</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow></msub><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"bold-italic\">z</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"bold-italic\">z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">/</mi><mi>τ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\ell_{i,j}=-\\log\\frac{\\exp(\\sin(\\boldsymbol{z}_i,\\boldsymbol{z}_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\sin(\\boldsymbol{z}_i,\\boldsymbol{z}_k)/\\tau)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.980548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\">ℓ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.653431em;vertical-align:-1.2264309999999998em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.128769em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.981231em;\"><span style=\"top:-2.40029em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29971000000000003em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\">1</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">[</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel mtight\"><span class=\"mrel mtight\"><span class=\"mord vbox mtight\"><span class=\"thinbox mtight\"><span class=\"rlap mtight\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel mtight\"></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel mtight\">=</span></span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">]</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">/</span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">/</span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2264309999999998em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>  其中<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mn mathvariant=\"double-struck\">1</mn><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo mathvariant=\"normal\">≠</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{1}_{[k\\neq i]}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9996399999999999em;vertical-align:-0.3551999999999999em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\">1</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">[</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel mtight\"><span class=\"mrel mtight\"><span class=\"mord vbox mtight\"><span class=\"thinbox mtight\"><span class=\"rlap mtight\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel mtight\"></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel mtight\">=</span></span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">]</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span></span></span></span> 是当<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo mathvariant=\"normal\">≠</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">k\\ne i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 时取值为 1 的指标函数，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>τ</mi></mrow><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span></span></span></span> 表示温度参数。为了方便起见，称其为 <code>NT-Xent</code> （the normalized temperature-scaled cross entropy loss，归一化温度标度交叉熵损失）。</p>\n<p>  如下的流程图总结了 SimCLR 的算法流程：</p>\n<p><img data-src=\"/images/AI/SimCLR/2.2.png\" alt=\"\"></p>\n<h1 id=\"三-对比表征学习的数据增强\"><a class=\"markdownIt-Anchor\" href=\"#三-对比表征学习的数据增强\">#</a> 三、对比表征学习的数据增强</h1>\n<p>  为了系统地研究数据扩充的影响， <code>SimCLR</code>  这里考虑几个常见的扩充。一种类型的增强涉及数据的空间 / 几何变换，例如裁剪和调整大小 (水平翻转)，旋转和切出。另一种类型的增强涉及外观转换，如颜色失真 (包括颜色下降，亮度，对比度，饱和度，色调)，高斯模糊和索贝尔滤波。下图显示了 <code>SimCLR</code>  中学习的增强。</p>\n<p><img data-src=\"/images/AI/SimCLR/3.1.png\" alt=\"\"></p>\n<p>  下图显示了单独和组合转换下的线性评估结果。可以观察到，即使模型几乎可以完美地识别对比任务中的正对，也没有一个单一的变换足以学习到良好的表示。在组合增强时，对比预测任务变得更加困难，但表示质量显著提高。</p>\n<p><img data-src=\"/images/AI/SimCLR/3.2.png\" alt=\"\"></p>\n<p>  增强的一个组成突出：<strong>随机裁剪和随机颜色失真</strong>。可以推测，当只使用随机裁剪作为数据增强时，一个严重的问题是来自图像的大多数补丁共享相似的颜色分布。下图显示，颜色直方图本身就足以区分图像。神经网络可以利用这一捷径来解决预测任务。因此，为了学习可推广的特征，将裁剪与颜色失真组合是至关重要的。</p>\n<p><img data-src=\"/images/AI/SimCLR/3.3.png\" alt=\"\"></p>\n<h1 id=\"四-编码器和投影头的架构\"><a class=\"markdownIt-Anchor\" href=\"#四-编码器和投影头的架构\">#</a> 四、编码器和投影头的架构</h1>\n<p>  下图显示，<strong>增加深度和宽度都会提高性能</strong>，这也许并不令人惊讶。虽然类似的发现适用于监督学习，我们发现监督模型和在无监督模型上训练的线性分类器之间的差距随着模型大小的增加而缩小，这表明无监督学习比监督学习更受益于更大的模型。</p>\n<p><img data-src=\"/images/AI/SimCLR/4.1.png\" alt=\"\"></p>\n<p>  然后， <code>SimCLR</code>  研究包括投影头的重要性，即<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">g(h)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">h</span><span class=\"mclose\">)</span></span></span></span>。下图示出了使用头部的三种不同架构的线性评估结果：（1）无投影；（2）线性投影；（3）默认的非线性投影与一个额外的隐藏层（ReLU 激活）。可以观察到<strong>非线性投影</strong>比线性投影好（+3%），并且比没有投影好得多（&gt;10%）。当使用投影头时，<strong>无论输出尺寸如何，都观察到类似的结果</strong>。</p>\n<p><img data-src=\"/images/AI/SimCLR/4.2.png\" alt=\"\"></p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/09/14/AI/BERT/",
            "url": "http://qianqiu-cell.github.io/2024/09/14/AI/BERT/",
            "title": "BERT",
            "date_published": "2024-09-13T16:00:00.000Z",
            "content_html": "<h1 id=\"一-引言\"><a class=\"markdownIt-Anchor\" href=\"#一-引言\">#</a> 一、引言</h1>\n<p>   <code>BERT</code> （ <code>Bidirectional Encoder Representations from Transformers</code> ）是一种基于深度学习的自然语言处理（ <code>NLP</code> ）模型。它是由 <code>Google</code>  在 <code>2018</code>  年提出的，采用了 <code>Transformer</code>  架构，并在大规模语料库上进行了预训练。 <code>BERT</code>  的特点之一是其双向（ <code>Bidirectional</code> ）处理能力，它能够同时考虑到句子中所有单词的上下文，而不仅仅是单词之前或之后的部分。这种双向性使得 <code>BERT</code>  在许多 <code>NLP</code>  任务中表现出色，例如文本分类、问答和命名实体识别等。</p>\n<h1 id=\"二-bert\"><a class=\"markdownIt-Anchor\" href=\"#二-bert\">#</a> 二、BERT</h1>\n<p>   <code>BERT</code>  的框架中有两个步骤：预训练和微调。在预训练过程中，模型在<strong>不同的预训练任务上对未标记数据</strong>进行训练。对于微调，首先使用预训练的参数初始化 <code>BERT</code>  模型，然后使用<strong>来自下游任务的标记数据</strong>对<strong>所有参数</strong>进行<strong>微调</strong>。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。</p>\n<p><img data-src=\"/images/AI/BERT/2.1.png\" alt=\"\"></p>\n<p>   <code>BERT</code>  的一个显著特征是其跨不同任务的统一架构。预训练的体系结构和最终的下游体系结构之间的差别很小。除了输出层，在预训练和微调中使用了相同的架构。使用相同的预训练模型参数来初始化不同下游任务的模型。在微调期间，对所有参数进行微调。[CLS] 是添加在每个输入示例前面的特殊符号，[SEP] 是一个特殊的分隔符号 (例如，分隔问题 / 答案)。</p>\n<h2 id=\"21-模型结构\"><a class=\"markdownIt-Anchor\" href=\"#21-模型结构\">#</a> 2.1 模型结构</h2>\n<p>   <code>BERT</code>  的模型架构是一个多层双向 <code>transformer</code>  编码器，使用<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span> 表示层数 (即 <code>transformer</code>  块)，隐藏大小为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span>，自注意头的数量为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span>。 <code>BERT</code>  主要有两种模型大小： <code>BERT BASE</code>  (L=12, H=768, A=12, Total Parameters=110M) 和 <code>BERT LARGE</code> (L=24, H=1024, A=16, Total Parameters=340M)。</p>\n<p>  为了进行比较，选择 <code>BERT BASE</code>  与 <code>OpenAI GPT</code>  具有相同的模型大小。然而，关键的是， <code>BERT Transformer</code>  使用双向自关注，而 <code>GPT Transformer</code>  使用约束自关注，其中每个 <code>token</code>  只能关注其左侧的上下文。</p>\n<h2 id=\"22-输入输出表示\"><a class=\"markdownIt-Anchor\" href=\"#22-输入输出表示\">#</a> 2.2 输入 / 输出表示</h2>\n<p>  为了使 <code>BERT</code>  处理各种下游任务，我们的输入表示能够在一个 <code>token</code>  序列中明确地表示单个句子和一对句子 (例如，&lt;Question, Answer&gt;)。在整个工作中，一个 “句子” 可以是一个连续文本的任意跨度，而不是一个实际的语言句子。“序列” 指的是 <code>BERT</code>  的输入 <code>token</code>  序列，它可以是一个句子或两个句子组合在一起。</p>\n<p>   <code>BERT</code>  使用具有 <code>30,000</code>  个词汇的 <code>WordPiece embeddings</code> 。每个序列的第一个标记总是一个特殊的分类标记 ([CLS])。 <code>BERT</code>  用两种方法区分句子。首先，用一个特殊的 <code>token</code>  ([SEP]) 将它们分开。其次，为每一个 <code>token</code>  添加一个可学习 <code>embedding</code> ，表明它属于句子 <code>A</code>  还是句子 <code>B</code> 。如图 1 所示，将输入 <code>embedding</code>  表示为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span></span></span></span>，特殊 [CLS] 标记的最终隐藏向量表示为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>H</mi></msup></mrow><annotation encoding=\"application/x-tex\">C\\in \\mathbb{R}^H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span></span></span></span></span></span></span></span></span></span></span>，第 i 个输入标记的最终隐藏向量表示为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>H</mi></msup></mrow><annotation encoding=\"application/x-tex\">T_i\\in \\mathbb{R}^H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span></span></span></span></span></span></span></span></span></span></span>。</p>\n<p>  对于给定的 <code>token</code> ， <code>BERT</code>  输入表示是通过将相应的 <code>token</code> 、 <code>segment</code>  (段) 和 <code>position</code>  (位置) <code>embedding</code>  相加来构建的。图 2 显示了该结构的可视化。</p>\n<p><img data-src=\"/images/AI/BERT/2.2.png\" alt=\"\"></p>\n<h2 id=\"23-预训练bert\"><a class=\"markdownIt-Anchor\" href=\"#23-预训练bert\">#</a> 2.3 预训练 BERT</h2>\n<p>  使用两个无监督任务对 <code>BERT</code>  进行预训练。</p>\n<h3 id=\"1-任务-1掩码语言模型masked-lm-mlm\"><a class=\"markdownIt-Anchor\" href=\"#1-任务-1掩码语言模型masked-lm-mlm\">#</a> (1) 任务 1：掩码语言模型（Masked LM, MLM）</h3>\n<p>  为了训练深度双向表示， <code>BERT</code>  随机屏蔽一定比例的输入 <code>token</code> ，然后预测这些被屏蔽的 <code>token</code> 。这一过程称为 <code>Masked LM</code> ( <code>MLM</code> )，在文献中它通常被称为完形填空任务。</p>\n<p>  训练数据生成器随机选择 15% 的 <code>token</code>  位置进行预测。如果选择了第<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 个 <code>token</code> ，那么 80% 的概率使用 [MASK] <code>token</code> ，10% 的概率使用随机 <code>token</code> ，10% 的概率使用未更改的第<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 个 <code>token</code>  来替换第<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span> 个 <code>token</code> 。然后，利用<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">T_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 来预测具有交叉熵损失的原始 <code>token</code> 。</p>\n<h3 id=\"2任务-2下一个句子预测next-sentence-prediction-nsp\"><a class=\"markdownIt-Anchor\" href=\"#2任务-2下一个句子预测next-sentence-prediction-nsp\">#</a> （2）任务 2：下一个句子预测（Next Sentence Prediction, NSP）</h3>\n<p>  许多重要的下游任务，如问答 ( <code>QA</code> ) 和自然语言推理 ( <code>NLI</code> )，都是基于理解两个句子之间的关系，这是语言建模无法直接捕获的。为了训练一个理解句子关系的模型， <code>BERT</code>  对一下一个句子预测任务进行了预训练，该任务可以从任何单语语料库中轻松生成。具体来说，当为每个预训练示例选择句子 <code>A</code>  和 <code>B时</code> ， <code>50%</code>  的时间 <code>B</code>  是 <code>A</code>  之后的下一个句子 (标记为 <code>IsNext</code> )，  <code>50%</code>  的时间 <code>B</code>  是语料库中的随机句子 (标记为 <code>NotNext</code> )。如图 1 所示，<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 用于下一个句子预测。</p>\n<h3 id=\"预训练的数据\"><a class=\"markdownIt-Anchor\" href=\"#预训练的数据\">#</a> 预训练的数据</h3>\n<p>  预训练过程在很大程度上遵循现有的文献语言模型预训练。对于预训练语料库， <code>BERT</code>  使用 <code>BooksCorpus</code> （8 亿单词）和英语维基百科（2,500 万字）。对于维基百科， <code>BERT</code>  只提取文本段落，忽略列表，表格和标题。使用文档级语料库而不是诸如十亿字基准，以便提取长的连续序列。</p>\n<h2 id=\"23-微调bert\"><a class=\"markdownIt-Anchor\" href=\"#23-微调bert\">#</a> 2.3 微调 BERT</h2>\n<p>  在海量的语料上训练完 BERT 之后，便可以将其应用到 NLP 的各个任务中了。 微调 (Fine-Tuning) 的任务包括：基于句子对的分类任务，基于单个句子的分类任务，问答任务，命名实体识别等。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/09/14/AI/BLIP/",
            "url": "http://qianqiu-cell.github.io/2024/09/14/AI/BLIP/",
            "title": "BLIP",
            "date_published": "2024-09-13T16:00:00.000Z",
            "content_html": "<h1 id=\"一-引言\"><a class=\"markdownIt-Anchor\" href=\"#一-引言\">#</a> 一、引言</h1>\n<p>  自从 <code>CLIP</code>  横空出世，各种 视觉语言预训练 ( <code>Vision-Language Pre-training, VLP</code> ) 模型逐渐涌现，显著提高了各种视觉语言任务的性能。然而，现有的 <code>VLP</code>  方法主要存在以下两个问题：</p>\n<ul>\n<li>\n<p><strong>模型角度</strong>：大多数方法都是基于 编码器模型 ( <code>encoder-based model</code> ) 或编码器 - 解码器模型 ( <code>encoder-decoder models</code> )，前者难以完成文本生成任务，后者无法完成图像文本检索任务，这两项任务无法兼顾；</p>\n</li>\n<li>\n<p><strong>数据角度</strong>：以 <code>CLIP</code>  为代表的方法都是从互联网上收集海量图像 - 文本对作为样本进行预训练，这些带噪声的文本作为视觉语言学习的监督信号显然不是最优解；</p>\n</li>\n</ul>\n<p>  为此， <code>Salesforce</code>  的研究者们提出了 <code>BLIP (Bootstrapping language - image Pre-training)</code> ，用于统一的视觉语言理解和生成。 <code>BLIP</code>  是一种新的 <code>VLP</code>  框架，比现有方法能够实现更广泛的下游任务。它分别从模型和数据的角度介绍了两个贡献:</p>\n<ul>\n<li>\n<p><strong>编码器 - 解码器的多模态混合</strong> ( <code>MED, Multimodal mixture of Encoder-Decoder</code> ): 一种有效的多任务预训练和灵活迁移学习的新模型架构。 <code>MED</code>  可以作为单模编码器、基于图像的文本编码器或基于图像的文本解码器操作。该模型采用图像文本对比学习、图像文本匹配和图像条件化语言建模三个视觉语言目标进行联合预训练。</p>\n</li>\n<li>\n<p><strong>字幕和过滤</strong> ( <code>CapFilt, Captioning and Filtering</code> ): 一种新的数据集 <code>bootstrap</code>  方法，用于从噪声图像 - 文本对中学习。 <code>BLIP</code>  将预训练的 <code>MED</code>  调整为两个模块：一个 <code>captioner</code>  用于生成给定 web 图像的合成字幕，一个 <code>filter</code>  用于从原始 <code>web</code>  文本和合成文本中去除嘈杂的字幕。</p>\n</li>\n</ul>\n<h1 id=\"二-相关工作\"><a class=\"markdownIt-Anchor\" href=\"#二-相关工作\">#</a> 二、相关工作</h1>\n<h2 id=\"21-视觉语言预训练\"><a class=\"markdownIt-Anchor\" href=\"#21-视觉语言预训练\">#</a> 2.1 视觉语言预训练</h2>\n<p>  视觉语言预训练 ( <code>VLP</code> ) 旨在通过对大规模图像 - 文本对模型进行预训练来提高下游视觉和语言任务的性能。由于获取人工注释文本的费用过高，大多数方法使用从网络抓取的图像和替代文本对。尽管使用了简单的基于规则的过滤器，噪声在网络文本中仍然普遍存在。然而，噪声的负面影响在很大程度上被忽略了，被通过扩大数据集获得的性能增益所掩盖。 <code>BLIP</code>  的论文表明，噪声网络文本对于视觉语言学习是次优的，并提出 <code>CapFilt</code>  以更有效的方式利用网络数据集的。</p>\n<p>  已经有很多尝试将各种视觉和语言任务统一到一个框架中。最大的挑战是设计既能执行基于理解的任务 (如图像文本检索) 又能执行基于生成的任务 (如图像字幕) 的模型架构。两者都不是基于编码器的模型和编码器 - 解码器模型可以在这两种类型的任务中表现出色，而单一的统一编码器 - 解码器也限制了模型的能力。 <code>BLIP</code>  提出的多模态编码器 - 解码器混合模型在广泛的下游任务上提供了更大的灵活性和更好的性能，同时保持了预训练的简单和高效。</p>\n<h2 id=\"22-知识蒸馏\"><a class=\"markdownIt-Anchor\" href=\"#22-知识蒸馏\">#</a> 2.2 知识蒸馏</h2>\n<p>  知识蒸馏（ <code>KD, Knowledge Distillation</code> ）旨在通过从教师模型中提取知识来提高学生模型的性能。自我蒸馏是 <code>KD</code>  的一个特例，其中教师和学生的规模相等。已经证明只是蒸馏对于图像分类，以及 <code>VLP</code>  是有效的。与大多数现有的 <code>KD</code>  方法只是简单地强制学生具有与教师相同的类预测不同， <code>BLIP</code>  提出的 <code>CapFilt</code>  可以被解释为在 <code>VLP</code>  环境下执行 <code>KD</code>  的一种更有效的方法，其中字幕器通过语义丰富的合成字幕提取其知识，而过滤器通过去除噪声字幕提取其知识。</p>\n<h2 id=\"23-数据增强\"><a class=\"markdownIt-Anchor\" href=\"#23-数据增强\">#</a> 2.3 数据增强</h2>\n<p>  虽然数据增强（ <code>DA, Data Augmentation</code> ）已被广泛应用于计算机视觉，但用于语言任务的 <code>DA</code>  并不那么直接。近年来，生成语言模型已被用于合成各种 <code>NLP</code>  任务的示例。不同于这些方法只关注低资源的语言任务， <code>BLIP</code>  的方法展示了合成字幕在大规模视觉语言预训练中的优势。</p>\n<h1 id=\"三-方法\"><a class=\"markdownIt-Anchor\" href=\"#三-方法\">#</a> 三、方法</h1>\n<h2 id=\"31-模型架构\"><a class=\"markdownIt-Anchor\" href=\"#31-模型架构\">#</a> 3.1 模型架构</h2>\n<p><img data-src=\"/images/AI/BLIP/3.1.png\" alt=\"\"></p>\n<p>   <code>BLIP</code>  采用了编码器 - 解码器的多模态混合结构 ( <code>MED</code> )，包括两个单模态编码器、一个以图像为基础的文本编码器和一个以图像为基础的文本解码器：</p>\n<ul>\n<li>\n<p><strong>单模态编码器</strong>  <code>lmage Encoder</code> ：基于 <code>transformer</code>  的 <code>ViT</code>  的架构，将输入图像分割为多个的 <code>patch</code>  并将它们编码为一系列 <code>Image Embedding</code> ，并使用 [CLS] token 来表示全局的图像特征。 <code>lmage Encoder</code>  用来提取图像特征做对比学习，相当于 <code>CLIP</code>  中的  <code>Image Encoder</code> ；</p>\n</li>\n<li>\n<p><strong>单模态编码器</strong>  <code>Text Encoder</code> ：基于 <code>BERT</code>  的架构，将 [CLS] token 加到输入文本的开头以总结句子。 <code>Text Encoder</code>  用来提取文本特征做对比学习，相当于 <code>CLIP</code>  中的 <code>Text Encoder</code> ；</p>\n</li>\n<li>\n<p><strong>以图像为基础的编码器</strong>  <code>Image-grounded text encoder</code> ：对 <code>Text Encoder</code>  的每一个 <code>transformer</code>  块，在 自注意力 ( <code>self-attention, SA</code> ) 层和前馈网络之间添加一个交叉注意 ( <code>cross-attention, CA</code> ) 层用来注入视觉信息，还将 [Encode] token 加到输入文本的开头以标识特定任务。 <code>Image-grounded text encoder</code>  用来提取文本特征并将其和图像特征对齐，相当于 <code>CLIP</code>  中更精细化的 <code>Text-Image</code>  对齐；</p>\n</li>\n<li>\n<p><strong>以图像为基础的解码器</strong>  <code>Image-grounded text decoder</code> ：将 <code>Image-grounded text encoder</code>  的 <code>self-attention</code>  层换成 <code>causal self-attention</code>  层，还将 [Decode] token 和 [EOS] token 加到输入文本的开头和结尾以标识序列的开始和结束。 <code>Image-grounded text decoder</code>  用来生成符合图像和文本特征的文本描述，这是 <code>CLIP</code>  中所没有的；</p>\n</li>\n</ul>\n<div class=\"note primary no-icon\">\n<p>  注意， <code>Image-grounded text decoder</code>  生成的文本描述不同于输入的图像 - 文本对中的文本，图像 - 文本对中的文本是互联网上找到的图片的上下文，拥有大量的噪音；而 <code>Image-grounded text decoder</code>  生成的文本描述是针对图像特征和文本特征的特定描述，更加精炼。这里其实就已经是 <code>Captioner</code>  的雏形了，下文的 <code>Captioner</code>  也是基于训练完成的 <code>Image-grounded text decoder</code>  再进行的优化和微调。</p>\n</div>\n<h2 id=\"32-预训练目标\"><a class=\"markdownIt-Anchor\" href=\"#32-预训练目标\">#</a> 3.2 预训练目标</h2>\n<p>   <code>BLIP</code>  在预训练期间联合优化三个目标，其中两个基于理解的目标和一个基于生成的目标。每个图像 - 文本对仅需要通过计算量较大的视觉 <code>Transformer</code>  的一次正向传递，以及通过文本 <code>Transformer</code>  的三次正向传递，其中不同的功能被激活以计算如下所述的三个损失。</p>\n<ul>\n<li>\n<p><strong>图文对比损失</strong> ( <code>Image-Text Contrastive Loss, ITC</code> )： <code>ITC</code>  用于训练 <code>lmage Encoder</code>  和 <code>Text Encoder</code> ，通过对比学习对齐图像和文本的特征空间。具体方法是最大化正样本图像 - 文本对的相似度且最小化负样本图像 - 文本对的相似度。 <code>BLIP</code>  还使用了 <code>ALBEF</code>  中的动量编码器来生成特征，并从动量编码器创建软标签作为训练目标，以解释负样本对中的潜在正样本；</p>\n</li>\n<li>\n<p><strong>图文匹配损失</strong> ( <code>Image-Text Matching Loss, ITM</code> )： <code>ITM</code>  用于训练 <code>Image-grounded text encoder</code> ，通过学习图像 - 文本对的联合表征实现视觉和语言之间的细粒度对齐。具体方法是通过一个二分类任务，预测图像文本对是正样本还是负样本。这里还使用了 <code>ALBEF</code>  中的 <code>hard negative mining</code>  技术以更好地捕捉负样本信息；</p>\n</li>\n</ul>\n<div class=\"note primary no-icon\">\n<p>   <code>ITM</code>  与 <code>ITC</code>  并不一样， <code>ITC</code>  虽然也对齐了图像和文本的特征空间，但并没有显式地区分正负样本，主要是用于评价图像和文本的匹配情况，给出任意输入图像 - 文本对的相似度。而 <code>ITM</code>  则是通过计算样本的损失达到区分正负样本的目的，激励正样本的图像和文本使其更加匹配。</p>\n</div>\n<ul>\n<li><strong>语言建模损失</strong> ( <code>Language Modeling Loss, LM</code> )： <code>LM</code>  用于训练 <code>Image-grounded text decoder</code> ，实现生成图像的文本描述任务。具体方法是通过优化交叉熵损失函数，训练模型以自回归的方式最大化文本的概率。在计算损失时， <code>BLIP</code>  应用 0.1 的标签平滑。与 <code>VLP</code>  中广泛使用的 <code>MLM</code>  损失相比， <code>LM</code>  使该模型具有泛化能力，能够将视觉信息转换为连贯的字幕。</li>\n</ul>\n<div class=\"note primary no-icon\">\n<p>  为了在利用多任务学习的同时执行有效的预训练，文本编码器和文本解码器共享除了 <code>SA</code>  层之外的所有参数。原因是编码和解码任务之间的差异最好由 <code>SA</code>  层来捕获。具体地，编码器采用双向自注意来构建当前输入标记的表示，而解码器采用因果自注意来预测下一个标记。另一方面，在编码和解码任务之间，嵌入层、 <code>CA</code>  层和 <code>FFN</code>  的功能类似，因此共享这些层可以提高训练效率，同时受益于多任务学习。</p>\n</div>\n<h2 id=\"33-capfilt机制\"><a class=\"markdownIt-Anchor\" href=\"#33-capfilt机制\">#</a> 3.3 CapFilt 机制</h2>\n<p>   <code>COCO</code>  数据集的高质量人工注释图像 - 文本对<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>h</mi></msub><mo separator=\"true\">,</mo><msub><mi>T</mi><mi>h</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">{(I_h,T_h)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">h</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">h</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span> 数量有限，因此 <code>CLIP</code> 、 <code>BLIP</code>  等 <code>VLP</code>  都是从网络中收集大量图像 - 文本对<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>w</mi></msub><mo separator=\"true\">,</mo><msub><mi>T</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">{(I_w,T_w)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span> 作为训练集。但这些网络图像 - 文本对的文本来自图像上下文，难以精准地描述图像内容，存在大量噪声。于是， <code>BLIP</code>  提出了字幕和过滤 ( <code>Captioning and Filtering, CapFilt</code> ) 机制，下图给出了 <code>CapFilt</code>  的说明。</p>\n<p><img data-src=\"/images/AI/BLIP/3.2.png\" alt=\"\"></p>\n<p>  上图给出了 <code>CapFilt</code>  的图示，包含字幕器和过滤器两个模块：</p>\n<ul>\n<li><strong>字幕器</strong>  <code>Captioner</code> ：一个视觉文本解码器，基于 <code>Image-grounded text decoder</code> ，用于生成给定图像的字幕，使用 <code>LM</code>  损失函数在 <code>COCO</code>  数据集上进行微调。给定网络图片<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>I</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">I_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>，Captioner 生成字幕<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>T</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">T_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>。<br>\n​</li>\n<li><strong>过滤器</strong>  <code>Filter</code> ：一个视觉文本编码器，基于 <code>Image-grounded text encoder</code> ，用于去除文本噪声，使用 <code>ITC</code>  和 <code>ITM</code>  损失函数在 <code>COCO</code>  数据集上进行微调。 <code>Filter</code>  通过比对文本和图像的匹配情况，删除原始 <code>Web</code>  文本<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>T</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">T_w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 和合成文本<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>T</mi><mi>s</mi></msub></mrow><annotation encoding=\"application/x-tex\">T_s</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">s</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 中的噪声。</li>\n</ul>\n<div class=\"note primary no-icon\">\n<p><code>Captioner</code>  和 <code>Filter</code>  都是在预训练完成后的 <code>MED</code>  模型上初始化的，并各自在人工注释的图像 - 文本对数据集 <code>COCO</code>  上进行微调 ( <code>finetune</code> )。最后，将过滤后的图像 - 文本对与人工注释对相结合，形成一个新的数据集（这也是 <code>Bootstrapping</code>  的由来），最后再用它来训练一遍模型。</p>\n</div>\n<h2 id=\"34-整体思路整理\"><a class=\"markdownIt-Anchor\" href=\"#34-整体思路整理\">#</a> 3.4 整体思路整理</h2>\n<ul>\n<li>先使用含有噪声的网络数据训练一遍 <code>BLIP</code> ；</li>\n<li>再在 <code>COCO</code>  数据集上进行微调以训练 <code>Captioner</code>  和 <code>Filter</code> ；</li>\n<li>然后使用 <code>Filter</code>  从原始网络文本和 <code>Captioner</code>  合成的文本中删除嘈杂的字幕，得到干净的数据；</li>\n<li>最后再使用干净的数据训练一遍得到高性能的 <code>BLIP</code> 。</li>\n</ul>\n<h1 id=\"四-参考程序\"><a class=\"markdownIt-Anchor\" href=\"#四-参考程序\">#</a> 四、参考程序</h1>\n<p>参考：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9TYWxlc2ZvcmNlL2JsaXAtaW1hZ2UtY2FwdGlvbmluZy1sYXJnZQ==\">https://huggingface.co/Salesforce/blip-image-captioning-large</span></p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">from</span> PIL <span class=\"token keyword\">import</span> Image</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> BlipProcessor<span class=\"token punctuation\">,</span> BlipForConditionalGeneration</pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>processor <span class=\"token operator\">=</span> BlipProcessor<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"E:/python/else/LLM_learn/5_BLIP/model\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>model <span class=\"token operator\">=</span> BlipForConditionalGeneration<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"E:/python/else/LLM_learn/5_BLIP/model\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>raw_image <span class=\"token operator\">=</span> Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'1.jpg'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>convert<span class=\"token punctuation\">(</span><span class=\"token string\">'RGB'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\"># conditional image captioning</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>text <span class=\"token operator\">=</span> <span class=\"token string\">\"a photography of\"</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>inputs <span class=\"token operator\">=</span> processor<span class=\"token punctuation\">(</span>raw_image<span class=\"token punctuation\">,</span> text<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">\"pt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>out <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>inputs<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>processor<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre></pre></td></tr><tr><td data-num=\"16\"></td><td><pre><span class=\"token comment\"># unconditional image captioning</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>inputs <span class=\"token operator\">=</span> processor<span class=\"token punctuation\">(</span>raw_image<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">\"pt\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">\"cuda\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>out <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>inputs<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>processor<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> skip_special_tokens<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure>",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/09/12/AI/CLIP/",
            "url": "http://qianqiu-cell.github.io/2024/09/12/AI/CLIP/",
            "title": "CLIP",
            "date_published": "2024-09-11T16:00:00.000Z",
            "content_html": "<h1 id=\"一-简介\"><a class=\"markdownIt-Anchor\" href=\"#一-简介\">#</a> 一、简介</h1>\n<h2 id=\"11-前言\"><a class=\"markdownIt-Anchor\" href=\"#11-前言\">#</a> 1.1 前言</h2>\n<p>   <code>CLIP</code>  是 <code>OpenAI</code>  在 <code>2021</code>  年 <code>2</code>  月发表的一篇文章，其全称为 <code>Contrastive Language-Image Pre-training</code> ，即一种基于对比文本 - 图像对的预训练方法。 <code>CLIP</code>  用文本作为监督信号来训练可迁移的视觉模型，使得最终模型的 <code>zero-shot</code>  效果堪比 <code>ResNet50</code> ，泛化性非常好。</p>\n<p>   <code>zero-shot</code>  就是直接推理，用见过的图片特征去判断没见过的图片的类别，而完全不用下游任务训练集进行微调。（相当于把模型用作特征提取，但是没有分类头）</p>\n<p>  作者在 30 多个不同的计算机视觉数据集上进行基准测试，（这些数据集涵盖了 <code>OCR</code> ( <code>Optical Character Recognition</code> , 光学字符识别)、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务）， <code>CLIP</code>  通常都能够与监督模型的 <code>baseline</code>  效果相媲美。</p>\n<p>  例如在 <code>ImageNet</code>  数据集上， <code>CLIP</code>  模型在不使用 <code>ImageNet</code>  数据集的任何一张图片进行训练的的情况下，最终模型精度能跟一个有监督的训练好的 <code>ResNet-50</code>  打成平手（在 <code>ImageNet</code>  上 <code>zero-shot</code>  精度为 <code>76.2%</code> ，这在之前一度被认为是不可能的）。</p>\n<h2 id=\"12-自然语言监督的优势\"><a class=\"markdownIt-Anchor\" href=\"#12-自然语言监督的优势\">#</a> 1.2 自然语言监督的优势</h2>\n<p>  使用自然语言监督信号来训练视觉模型，有两个最重要的优势：</p>\n<ul>\n<li>\n<p><strong>不需要采用特别的标注数据</strong>，扩展性更强。比如 <code>ImageNet</code>  需要先定义好 <code>1000</code>  个类，然后根据这些类去下载图片，清理数据集，再去标注所有图片，过程很复杂。而 CLIP 不要求这种经典的 &quot;机器学习兼容&quot; 的标注格式，只需要下载文字 - 图片对；且没有 n 选 1 的标签之后，模型的输入输出自由度大了很多。</p>\n</li>\n<li>\n<p><code>CLIP</code>  学习到的是图像结合文字的多模态特征，从而<strong>实现灵活的 zero-shot 迁移</strong>。如果只是单模态的特征，无论是类似 <code>MOCO</code>  还是 <code>MAE</code> ，都很难做到这一点（ <code>zero-shot</code>  必须要加入文字特征才能做到）。</p>\n</li>\n</ul>\n<h1 id=\"13-总结\"><a class=\"markdownIt-Anchor\" href=\"#13-总结\">#</a> 1.3 总结</h1>\n<p>  现有的 <code>CV</code>  模型基本都是基于人工标注的数据集进行训练的，然后用来预测一组提前定义好的物体类别。这种提前定义好的标签集合，会大大简化问题本身（比如 <code>ImageNet</code>  固定的 <code>1000</code>  个类， <code>COCO</code>  数据集固定 <code>80</code>  个类等等）。但正因如此，这种受限的监督信号限制了模型的泛化性和可用性。比如大多数模型都只能预测已知的图像类别。对于没有见过的图像类别，需要额外的信息才能识别。这样<strong>每次新增一些类别，都需要重新收集数据，训练一个新的模型</strong>。</p>\n<p>  作者认为，直接从自然语言中得到监督信息是一个很有前途的选择，因为其涵盖的范围更广（只要是语言描述过的物体，都有可能让视觉模型去识别）。 <code>CLIP</code>  利用多模态的对比学习，使得自然语言可以引导模型学习到视觉概念，从而实现非常灵活的 <code>zero-shot</code>  迁移（把分类问题转化为了跨模态检索问题）。</p>\n<p>  之前使用自然语言监督进行图像表示学习的工作很少，并且效果往往不如有监督模型，主要有两个原因：</p>\n<ul>\n<li><strong>早期 nlp 模型不太好学</strong>。比如早期的 <code>n-gram</code>  模型非常复杂，不好跨模态训练。但是随着 <code>transformer</code>  的兴起，像 <code>BERT</code>  和 <code>GPT</code>  这种具有上下文表示的自监督训练模型做的越来越好， <code>nlp</code>  模型也终于有了取之不尽的文本监督信号，而且使用简单，泛化性好，为多模态训练铺平了道路。</li>\n<li>数据集或模型的规模不够。比如 <code>VirTex</code>  和 <code>ICMLM</code>  都只训练了<strong>十几万的图片</strong>； <code>ConVIRT</code>  非常类似 <code>CLIP</code> ，但<strong>只在医疗图像上做了预训练</strong>。从本质上来讲， <code>CLIP</code>  其实并没有太大的创新，它只是<strong>将 ConVIRT 方法进行简化，并采用更大规模的文本 - 图像对数据集来训练</strong>。也可以说，相对于之前的对比学习， <code>CLIP</code>  只是将单模态的样本，换成了多模态的样本。</li>\n</ul>\n<h1 id=\"二-方法\"><a class=\"markdownIt-Anchor\" href=\"#二-方法\">#</a> 二、方法</h1>\n<h2 id=\"21-模型结构\"><a class=\"markdownIt-Anchor\" href=\"#21-模型结构\">#</a> 2.1 模型结构</h2>\n<p>  如下图所示， <code>CLIP</code>  的输入是一对对配对好的的<strong>图片 - 文本对</strong>（比如输入是一张狗的图片，对应文本也表示这是一只狗）。这些文本和图片分别通过 <code>Text Encoder</code>  和 <code>Image Encoder</code>  输出对应的特征。然后在这些输出的文字特征和图片特征上进行<strong>对比学习</strong>。</p>\n<p><img data-src=\"/images/AI/CLIP/2.1.png\" alt=\"\"></p>\n<p>  假如模型输入的是<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 对图片 - 文本对，那么这<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 对互相配对的图像–文本对是<strong>正样本</strong>（下图输出特征矩阵对角线上标识蓝色的部位），其它<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>−</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n^2 - n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 对样本都是<strong>负样本</strong>。这样模型的训练过程就是最大化<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 个正样本的相似度，同时最小化<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>−</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n^2 - n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 个负样本的相似度。</p>\n<p>其中：</p>\n<ul>\n<li>\n<p><code>Text Encoder</code>  可以采用 <code>NLP</code>  中常用的 <code>text transformer</code>  模型；而 <code>Image Encoder</code>  可以采用常用 <code>CNN</code>  模型或者 <code>vision transformer</code>  等模型</p>\n</li>\n<li>\n<p>相似度是计算文本特征和图像特征的余弦相似性 <code>cosine similarity</code></p>\n</li>\n<li>\n<p>为了训练 <code>CLIP</code> ， <code>OpenAI</code>  从互联网收集了共<strong> 4 个亿的文本 - 图像对</strong>，论文称之为 <code>WIT(Web Image Text)</code> 。 <code>WIT</code>  质量很高，而且清理的非常好，其规模相当于 <code>JFT-300M</code> ，这也是 <code>CLIP</code>  如此强大的原因之一（后续在 <code>WIT</code>  上还孕育出了 <code>DALL-E</code>  模型）</p>\n</li>\n</ul>\n<p><strong>分类</strong></p>\n<p>   <code>CLIP</code>  可以直接实现 <code>zero-shot</code>  的图像分类，即不需要任何训练和微调，这也是 <code>CLIP</code>  亮点和强大之处。用 <code>CLIP</code>  实现 <code>zero-shot</code>  分类只需要简单的两步：</p>\n<ul>\n<li>\n<p><strong>根据任务的分类标签构建每个类别的描述文本</strong>： <code>A photo of &#123;label&#125;</code> ，然后将这些文本送入 <code>Text Encoder</code>  得到对应的文本特征。如果类别数目为<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>，那么将得到<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 个文本特征；</p>\n</li>\n<li>\n<p>将要预测的图像送入 <code>Image Encoder</code>  得到图像特征，然后与<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> 个文本特征计算缩放的余弦相似度（和训练过程保持一致），然后<strong>选择相似度最大的文本对应的类别作为图像分类预测结果</strong>。进一步地，可以将这些相似度看成 <code>logits</code> ，送入 <code>softmax</code>  后可以到每个类别的预测概率。</p>\n</li>\n</ul>\n<p>  我们不再需要预先定义好的标签（类别）列表，直接将图片喂给不同的文本句子，就可以知道图片中是否有我们感兴趣的物体。即， <code>CLIP</code>  的多模态特性（利用文本监督信号）为具体的任务构建了动态的分类器，<strong>使得模型不再受限于预先定义好的类别，更加具有通用性和可用性</strong>。</p>\n<h2 id=\"22-预训练方法\"><a class=\"markdownIt-Anchor\" href=\"#22-预训练方法\">#</a> 2.2 预训练方法</h2>\n<p>   <code>CV</code>  领域的模型都很大，训练起来也很贵。比如 <code>noise student</code>  之前在 <code>ImageNet</code>  一直霸榜，但是这个模型需要在一个 <code>TPUv3</code>  上训练 <code>33</code>  年，这还只是在包含 <code>1000</code>  类的 <code>ImageNet</code>  上预训练的，而且只训练视觉特征。</p>\n<p>  由于训练数据量和模型计算量都很大，训练效率成为一个至关重要的因素。作者做了很多尝试，最终选择了对比学习：</p>\n<ul>\n<li>\n<p><code>VirTex</code>  模型：预测文本，对应下图蓝色线 <code>Transformer Language Model</code> ： <code>Image Encoder</code>  使用 <code>CNN</code>  模型， <code>Text Encoder</code>  使用 <code>transformer</code>  模型，两个模型一起从头训练，任务是预测图片对应的文本（ <code>image caption</code> ）。这种方法的训练效率太慢，因为根据图片进行文本描述，可能性太多了，你可以从各个角度去描述一张图片。</p>\n</li>\n<li>\n<p><code>Bag of Words Prediction</code> （橘色线）：不要求每个词都是按顺序的进行预测，所有词都预测出来就行。这样放宽了约束，训练速度提高了三倍。</p>\n</li>\n<li>\n<p><code>CLIP</code> ：简化版的 <code>ConVIRT</code> ，基于对比学习。只需要判断图文是否配对，进一步简化了训练任务，训练效率一下子提升 <code>4</code>  倍（绿色线）训练任务更加合理。因为训练数据所包含的文本 - 图像对是从互联网收集来的，它们存在一定的噪音，二者并不完全匹配。适当的降低训练目标，反而能取得更好的收敛。</p>\n</li>\n</ul>\n<p><img data-src=\"/images/AI/CLIP/2.2.png\" alt=\"\"></p>\n<p>(1)  <code>Text Encoder</code>  架构</p>\n<p>最终 <code>Text Encoder</code>  固定选择一个包含 <code>63M</code>  参数的 <code>text transformer</code>  模型，</p>\n<p>(2)  <code>Image Encoder</code>  架构</p>\n<ul>\n<li>\n<p><code>ResNet</code> ： <code>ResNet50</code> ， <code>ResNet101</code> ， <code>RN50x4</code> ， <code>RN50x16</code>  和 <code>RNx64</code> （后面三个模型是按照 <code>EfficientNet</code>  缩放规则对 <code>ResNet</code>  分别增大 <code>4x</code> ， <code>16x</code>  和 <code>64x</code>  得到）</p>\n</li>\n<li>\n<p><code>ViT</code> ： <code>ViT-B/32</code> ， <code>ViT-B/16</code>  和 <code>ViT-L/14</code> 。</p>\n</li>\n</ul>\n<p>(3) 所有的模型都训练 32 个 epochs，采用 AdamW 优化器，batch size=32768</p>\n<p>(4) 只在 ResNet50 上训练一个 epoch 进行超参搜索，没有进行进一步的调参</p>\n<p>(5) <strong>数据集非常大，几乎不会出现过拟合，所以 Image Encoder 和 Text Encoder 不需要提前进行预训练</strong>。</p>\n<p>(6) 只使用线性投射层（线性非线性影响不大）</p>\n<p>(7) 数据增强只使用图片的随机剪裁，这是因为数据集非常大</p>\n<p>(8) 对比学习目标函数中的超参数 τ，设置成可学习的标量，在训练中自动优化，而不用慢慢调参（还是因为数据集太大，训练很贵）。</p>\n<h2 id=\"23-伪代码\"><a class=\"markdownIt-Anchor\" href=\"#23-伪代码\">#</a> 2.3 伪代码</h2>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># image_encoder - ResNet or Vision Transformer</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># text_encoder - CBOW or Text Transformer</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># I [n, h, w, c] - 输入图片维度</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># T [n, l] - 输入文本维度，l 表示序列长度</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\"># W_i[d_i, d_e] - learned proj of image to embed</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># W_t[d_t, d_e] - learned proj of text to embed</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\"># t - learned temperature parameter</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\">#  分别提取图像特征和文本特征</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>I_f <span class=\"token operator\">=</span> image_encoder<span class=\"token punctuation\">(</span>I<span class=\"token punctuation\">)</span> <span class=\"token comment\">#[n, d_i]</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>T_f <span class=\"token operator\">=</span> text_encoder<span class=\"token punctuation\">(</span>T<span class=\"token punctuation\">)</span> <span class=\"token comment\">#[n, d_t]</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre></pre></td></tr><tr><td data-num=\"14\"></td><td><pre><span class=\"token comment\"># 对两个特征进行线性投射，得到相同维度的特征 d_e，并进行 l2 归一化，保持数据尺度的一致性</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre><span class=\"token comment\"># 多模态 embedding [n, d_e]</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>I_e <span class=\"token operator\">=</span> l2_normalize<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>I_f<span class=\"token punctuation\">,</span> W_i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>T_e <span class=\"token operator\">=</span> l2_normalize<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>T_f<span class=\"token punctuation\">,</span> W_t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre></pre></td></tr><tr><td data-num=\"19\"></td><td><pre><span class=\"token comment\"># 计算缩放的余弦相似度：[n, n]</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>logits <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>I_e<span class=\"token punctuation\">,</span> T_e<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre></pre></td></tr><tr><td data-num=\"22\"></td><td><pre><span class=\"token comment\"># symmetric loss function</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>labels <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span> <span class=\"token comment\">#  对角线元素的 labels</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>loss_i <span class=\"token operator\">=</span> cross_entropy_loss<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># image loss</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre>loss_t <span class=\"token operator\">=</span> cross_entropy_loss<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># text loss</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>loss_i <span class=\"token operator\">+</span> loss_t<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">2</span> <span class=\"token comment\"># 对称式的目标函数</span></pre></td></tr></table></figure><p>  在 <code>MOCO</code>  中，真实标签都是 <code>0</code> ，因为其正样本都是放在第一位，所以正样本对应的索引永远是 <code>0</code> ；但是在 <code>CLIP</code>  中，正样本都是在对角线上，即<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>I</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>T</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>I</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><msub><mi>T</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>…</mo></mrow><annotation encoding=\"application/x-tex\">I_1,T_1,I_2,T_2,\\dots</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span></span></span></span>，所以真实标签为 <code>np.arange(n)</code> 。</p>\n<h1 id=\"三-参考程序\"><a class=\"markdownIt-Anchor\" href=\"#三-参考程序\">#</a> 三、参考程序</h1>\n<p>参考：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9vcGVuYWkvY2xpcC12aXQtbGFyZ2UtcGF0Y2gxNA==\">https://huggingface.co/openai/clip-vit-large-patch14</span></p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">from</span> PIL <span class=\"token keyword\">import</span> Image</pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">from</span> transformers <span class=\"token keyword\">import</span> CLIPProcessor<span class=\"token punctuation\">,</span> CLIPModel</pre></td></tr><tr><td data-num=\"4\"></td><td><pre></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>model <span class=\"token operator\">=</span> CLIPModel<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"E:/python/else/LLM_learn/4_CLIP/model\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>processor <span class=\"token operator\">=</span> CLIPProcessor<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span><span class=\"token string\">\"E:/python/else/LLM_learn/4_CLIP/model\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>image <span class=\"token operator\">=</span> Image<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'1.jpg'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>inputs <span class=\"token operator\">=</span> processor<span class=\"token punctuation\">(</span>text<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"a photo of a cat\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"a photo of a dog\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> images<span class=\"token operator\">=</span>image<span class=\"token punctuation\">,</span> return_tensors<span class=\"token operator\">=</span><span class=\"token string\">\"pt\"</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>outputs <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>inputs<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>logits_per_image <span class=\"token operator\">=</span> outputs<span class=\"token punctuation\">.</span>logits_per_image  <span class=\"token comment\"># this is the image-text similarity score</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>logits_per_image<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>probs <span class=\"token operator\">=</span> logits_per_image<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># we can take the softmax to get the label probabilities</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>probs<span class=\"token punctuation\">)</span></pre></td></tr></table></figure>",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/07/29/AI/LLM_finetune/",
            "url": "http://qianqiu-cell.github.io/2024/07/29/AI/LLM_finetune/",
            "title": "大模型微调",
            "date_published": "2024-07-28T16:00:00.000Z",
            "content_html": "<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0L2FydGljbGUvZGV0YWlscy8xMzEyOTM5NDA=\">https://blog.csdn.net/qq_56591814/article/details/131293940</span>、<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVdrYnplVUVWRC8/c3BtX2lkX2Zyb209MzMzLjg4MC5teV9oaXN0b3J5LnBhZ2UuY2xpY2smYW1wO3ZkX3NvdXJjZT1lMDExNzJlYTI5MmMxYzYwNWIzNDYxMDFkNzAwNmM2MQ==\">https://www.bilibili.com/video/BV1WkbzeUEVD/?spm_id_from=333.880.my_history.page.click&amp;vd_source=e01172ea292c1c605b346101d7006c61</span></p>\n<h1 id=\"一-为什么要对大模型进行微调\"><a class=\"markdownIt-Anchor\" href=\"#一-为什么要对大模型进行微调\">#</a> 一、为什么要对大模型进行微调</h1>\n<p>  通常，要对大模型进行微调，有以下一些原因：</p>\n<ul>\n<li>\n<p>第一个原因是，因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低；</p>\n</li>\n<li>\n<p>第二个原因是， <code>Prompt Engineering</code>  的方式是一种相对来说容易上手的使用大模型的方式，但是它的缺点也非常明显。因为通常大模型的实现原理，都会对输入序列的长度有限制， <code>Prompt Engineering</code>  的方式会把 <code>Prompt</code>  搞得很长。越长的 <code>Prompt</code> ，大模型的推理成本越高，因为推理成本是跟 <code>Prompt</code>  长度的平方正向相关的。另外， <code>Prompt</code>  太长会因超过限制而被截断，进而导致大模型的输出质量打折口，这也是一个非常严重的问题。对于个人使用者而言，如果是解决自己日常生活、工作中的一些问题，直接用 <code>Prompt Engineering</code>  的方式，通常问题不大。但对于对外提供服务的企业来说，要想在自己的服务中接入大模型的能力，推理成本是不得不要考虑的一个因素，微调相对来说就是一个更优的方案。</p>\n</li>\n<li>\n<p>第三个原因是， <code>Prompt Engineering</code>  的效果达不到要求，企业又有比较好的自有数据，能够通过自有数据，更好的提升大模型在特定领域的能力。这时候微调就非常适用。</p>\n</li>\n<li>\n<p>第四个原因是，要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案。</p>\n</li>\n<li>\n<p>第五个原因是，数据安全的问题。如果数据是不能传递给第三方大模型服务的，那么搭建自己的大模型就非常必要。通常这些开源的大模型都是需要用自有数据进行微调，才能够满足业务的需求，这时候也需要对大模型进行微调。</p>\n</li>\n</ul>\n<h1 id=\"二-大模型微调的技术手段\"><a class=\"markdownIt-Anchor\" href=\"#二-大模型微调的技术手段\">#</a> 二、大模型微调的技术手段</h1>\n<p>  根据微调对整个预训练模型的调整程度，微调可以分为全微调和部分微调两个方法：</p>\n<ul>\n<li>\n<p>全微调（ <code>Full Fine-tuning, FFT</code> ）： <code>FFT</code>  是指对整个预训练模型进行微调，包括所有的模型参数。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。这种微调方法通常适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况。 <code>FFT</code>  需要较大的计算资源和时间，但可以获得更好的性能。</p>\n</li>\n<li>\n<p>参数高效微调（ <code>Parameter-Efficient Fine-Tuning, PEFT</code> ）： <code>PEFT</code>  旨在通过最小化微调参数数量和计算复杂度，提升预训练模型在新任务上的表现，从而减轻大型预训练模型的训练负担。 <code>PEFT</code>  方法可以通过多种方式进行分类，比如根据其基本方法或结构进行区分 —— 是否向模型引入新的参数，还是仅微调不分现有的参数；根据微调目的进行分类 —— 是否旨在最小化内存占用或仅追求存储效率。我们首先基于基本方法 &amp; 结构进行分类，下图展示了这个分类体系的 <code>30</code>  种 <code>PEFT</code>  方法。接下来对 <code>PEFT</code>  的分类进行详细介绍。</p>\n</li>\n</ul>\n<p><img data-src=\"/images/AI/LLM_finetune/2.1.png\" alt=\"\"></p>\n<h2 id=\"21-additive-methods\"><a class=\"markdownIt-Anchor\" href=\"#21-additive-methods\">#</a> 2.1 Additive methods</h2>\n<p>  主要思想是通过<mark>添加额外的参数或层来扩充现有的预训练模型，并仅训练新添加的参数</mark>。到目前为止，这是参数高效微调方法中最大且广泛探索的类别。这种方法又分为：</p>\n<ul>\n<li><code>Adapters</code> ：即在 <code>Transformer</code>  子层后引入小型全连接网络，这种方法被广泛采用。 <code>Adapters</code>  有多种变体，例如修改适配器的位置、剪枝以及使用重参数化来减少可训练参数的数量。</li>\n<li><code>Soft Prompts</code> ： <code>GPT-2</code>  旨在通过修改输入文本来控制语言模型的行为。然而，这些方法很难进行优化，且存在模型输入长度、训练示例的数量等限制，由此引入了 <code>soft</code>  概念。 <code>Soft Prompts</code> <mark> 将模型的一部分输入嵌入通过梯度下降进行微调</mark>，将在离散空间中寻找提示的问题转化为连续优化问题。 <code>Soft Prompts</code>  可以仅对输入层进行训练（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvZ3B0LXVuZGVyc3RhbmRzLXRvbw==\">《GPT Understands, Too》</span>、<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvdGhlLXBvd2VyLW9mLXNjYWxlLWZvci1wYXJhbWV0ZXItZWZmaWNpZW50\">Prompt Tuning</span>），也可以对所有层进行训练（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvcHJlZml4LXR1bmluZy1vcHRpbWl6aW5nLWNvbnRpbnVvdXMtcHJvbXB0cw==\">Prefix-Tuning</span>）。</li>\n<li><code>others</code> ：例如 <code>LeTS</code> ,  <code>LST</code>  和 <code>(IA)^3</code></li>\n</ul>\n<p>  尽管这些方法引入了额外的参数到网络中，但它们通过减少梯度和优化器状态的大小，减少了训练时间，提升了内存效率。此外可以对冻结的模型参数进行量化（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU2NTkxODE0L2FydGljbGUvZGV0YWlscy8xMzEyOTM5NDA=\">参考论文</span>）， <code>additive PEFT</code>  方法能够微调更大的网络或使用更大的批次大小，这提高了在 <code>GPU</code>  上的训练吞吐量。此外，在分布式设置中优化较少的参数大大减少了通信量。</p>\n<h2 id=\"22-selective-methods\"><a class=\"markdownIt-Anchor\" href=\"#22-selective-methods\">#</a> 2.2 Selective methods</h2>\n<p>  最早的 <code>selective PEFT</code>  方法是仅微调网络的几个顶层（冻结前层），现代方法通常基于层的类型（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvb24tdGhlLXN0cmVuZ3Rocy1vZi1jcm9zcy1hdHRlbnRpb24taW4=\">Cross-Attention is All You Need</span>）或内部结构，例如仅微调模型的偏置（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvYml0Zml0LXNpbXBsZS1wYXJhbWV0ZXItZWZmaWNpZW50LWZpbmUtdHVuaW5n\">BitFit</span>）或仅特定的行（<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20vcGFwZXIvZWZmaWNpZW50LWZpbmUtdHVuaW5nLW9mLWJlcnQtbW9kZWxzLW9uLXRoZQ==\">Efficient Fine-Tuning of BERT Models on the Edge</span>）。</p>\n<h2 id=\"23-reparametrization-based-peft重参数化\"><a class=\"markdownIt-Anchor\" href=\"#23-reparametrization-based-peft重参数化\">#</a> 2.3 Reparametrization-based PEFT（重参数化）</h2>\n<p>  利用低秩表示来最小化可训练参数的数量。Aghajanyan 等人（2020）证明了在低秩子空间中可以有效地进行微调，对于更大的模型或经过更长时间预训练的模型，需要进行调整的子空间更小。最知名的基于重参数化的方法 <code>LoRa</code> ，它将参数矩阵进行简单的低秩分解来更新权重。最近的研究（Karimi Mahabadi 等，2021；Edalati 等，2022）还探索了 <code>Kronecker product reparametrization</code>  的使用，它在秩和参数数量之间取得了更有利的权衡。</p>\n<p>   <code>LoRA</code>  背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。</p>\n<p>  大白话说：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。</p>\n<p>   <code>LoRA</code>  的基本思路，包括以下几步：</p>\n<ul>\n<li>\n<p>首先，要适配特定的下游任务，要训练一个特定的模型，将 <code>Y=WX</code>  变成 <code>Y=(W+∆W)X</code> ，这里面 <code>∆W</code>  主是我们要微调得到的结果；</p>\n</li>\n<li>\n<p>其次，将 <code>∆W</code>  进行低维分解 <code>∆W=AB</code>  ( <code>∆W</code>  为 <code>m*n</code>  维， <code>A</code>  为 <code>m*r</code>  维， <code>B</code>  为 <code>r*n</code>  维， <code>r</code>  就是上述假设中的低维)；</p>\n</li>\n<li>\n<p>接下来，用特定的训练数据，训练出 <code>A</code>  和 <code>B</code>  即可得到 <code>∆W</code> ，在推理的过程中直接将 <code>∆W</code>  加到 <code>W</code>  上去，再没有额外的成本。</p>\n</li>\n<li>\n<p>另外，如果要用 <code>LoRA</code>  适配不同的场景，切换也非常方便，做简单的矩阵加法即可： <code>(W+∆W)-∆W+∆W'</code> 。</p>\n</li>\n</ul>\n<p>  该方法认为模型权重矩阵在特定微调后具有较低的本征秩，故基于秩分解的概念，将预训练模型的现有权重矩阵分成两个较小的矩阵。</p>\n<p><img data-src=\"/images/AI/LLM_finetune/2.2.png\" alt=\"\"></p>\n<h2 id=\"24-hybrid-methods\"><a class=\"markdownIt-Anchor\" href=\"#24-hybrid-methods\">#</a> 2.4 Hybrid methods</h2>\n<p>  混合多种 <code>PEFT</code>  方法，例如， <code>MAM Adapter</code>  结合了 <code>Adapters</code>  和 <code>Prompt tuning</code> ； <code>UniPELT</code>  加入了 <code>LoRa</code> ,  <code>Compacter</code>  和 <code>KronAB</code>  对适配器进行了重参数化以减少其参数数量；最后， <code>S4</code>  是一个自动化算法搜索的结果，它结合了所有的 <code>PEFT</code>  类别，额外参数数量增加 0.5% 的情况下最大化准确性。</p>\n<h1 id=\"三-使用llama-factory微调qwen2模型\"><a class=\"markdownIt-Anchor\" href=\"#三-使用llama-factory微调qwen2模型\">#</a> 三、使用 LLaMA-Factory 微调 Qwen2 模型</h1>\n<h2 id=\"31-运行qwen2模型\"><a class=\"markdownIt-Anchor\" href=\"#31-运行qwen2模型\">#</a> 3.1 运行 Qwen2 模型</h2>\n<p>  首先进入下载 Qwen2 的<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL1F3ZW5MTS9Rd2VuMg==\"> github 网页</span>的运行文件。运行 <code>Qwen2/examples/demo/web_demo.py</code>  即可在网页端运行 <code>Qwen2</code>  模型。</p>\n<p>  若本地没有大模型参数文件，则会下载 <code>hugging face</code>  中的参数文件。但是 <code>hugging face</code>  由于网络原因会导致模型下载失败。因此选择国内的<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cubW9kZWxzY29wZS5jbi9teS9vdmVydmlldw==\"> ModelScope</span> 网站下载所需要的 <code>Qwen2</code>  大模型参数文件。找到对应的模型参数文件，依次点击模型文件 - 下载模型 - SDK 下载，即可获得模型参数文件的下载方式，一个示例下载的 <code>python</code>  程序如下所示：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\">#模型下载</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">from</span> modelscope <span class=\"token keyword\">import</span> snapshot_download</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>model_dir <span class=\"token operator\">=</span> snapshot_download<span class=\"token punctuation\">(</span><span class=\"token string\">'qwen/Qwen2-1.5B-Instruct'</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>  待下载完成模型文件后，更改 <code>web_demo.py</code>  文件的 <code>DEFAULT_CKPT_PATH</code>  参数为所下载模型参数文件的路径，一个示例的路径为： <code>DEFAULT_CKPT_PATH = 'E:/python/9_LLM/2_FineTuning/4_Qwen/qwen/Qwen2-1___5B-Instruct'</code></p>\n<p>  之后即可成功运行 <code>web_demo.py</code> ，并与所下载的大模型参数文件对应的大模型进行对话。</p>\n<p><img data-src=\"/images/AI/LLM_finetune/3.1.png\" alt=\"\"></p>\n<h2 id=\"32-下载并运行llama-factory\"><a class=\"markdownIt-Anchor\" href=\"#32-下载并运行llama-factory\">#</a> 3.2 下载并运行 LLaMA-Factory</h2>\n<p>  首先进入 <code>github</code>  的<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL2hpeW91Z2EvTExhTUEtRmFjdG9yeQ==\"> LLaMA-Factory 网页</span>，下载 <code>LLaMA-Factory</code>  工具箱。运行 <code>LLaMA-Factory-main/src/webui.py</code>  即可运行 <code>LLaMA-Factory</code>  的网页可视化界面。可视化界面如下所示：</p>\n<p><img data-src=\"/images/AI/LLM_finetune/3.2.png\" alt=\"\"></p>\n<h2 id=\"33-准备数据集\"><a class=\"markdownIt-Anchor\" href=\"#33-准备数据集\">#</a> 3.3 准备数据集</h2>\n<p>  在 <code>LLaMA-Factory-main/data</code>  文件夹下保存了几组示例数据集的 <code>json</code>  文件。其中 <code>dataset_info.json</code>  包含了所有可用的数据集。参考<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL2hpeW91Z2EvTExhTUEtRmFjdG9yeS9ibG9iL21haW4vZGF0YS9SRUFETUVfemgubWQ=\"> LLaMA-Factory 的说明文件</span>，如果希望使用自定义数据集，需要在 <code>dataset_info.json</code>  文件中添加数据集描述，目前 <code>LLaMA-Factory</code>  仅支持 <code>alpaca</code>  格式和 <code>sharegpt</code>  格式的数据集。完整的数据集描述如下，具体的示例可以参考初始 <code>dataset_info.json</code>  文件：</p>\n<figure class=\"highlight markdown\"><figcaption data-lang=\"markdown\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>\"数据集名称\": &#123;</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>  \"hf_hub_url\": \"Hugging Face 的数据集仓库地址（若指定，则忽略 script_url 和 file_name）\",</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>  \"ms_hub_url\": \"ModelScope 的数据集仓库地址（若指定，则忽略 script_url 和 file_name）\",</pre></td></tr><tr><td data-num=\"4\"></td><td><pre>  \"script_url\": \"包含数据加载脚本的本地文件夹名称（若指定，则忽略 file_name）\",</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>  \"file_name\": \"该目录下数据集文件夹或文件的名称（若上述参数未指定，则此项必需）\",</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>  \"formatting\": \"数据集格式（可选，默认：alpaca，可以为 alpaca 或 sharegpt）\",</pre></td></tr><tr><td data-num=\"7\"></td><td><pre>  \"ranking\": \"是否为偏好数据集（可选，默认：False）\",</pre></td></tr><tr><td data-num=\"8\"></td><td><pre>  \"subset\": \"数据集子集的名称（可选，默认：None）\",</pre></td></tr><tr><td data-num=\"9\"></td><td><pre>  \"split\": \"所使用的数据集切分（可选，默认：train）\",</pre></td></tr><tr><td data-num=\"10\"></td><td><pre>  \"folder\": \"Hugging Face 仓库的文件夹名称（可选，默认：None）\",</pre></td></tr><tr><td data-num=\"11\"></td><td><pre>  \"num_samples\": \"该数据集所使用的样本数量。（可选，默认：None）\",</pre></td></tr><tr><td data-num=\"12\"></td><td><pre>  \"columns（可选）\": &#123;</pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    \"prompt\": \"数据集代表提示词的表头名称（默认：instruction）\",</pre></td></tr><tr><td data-num=\"14\"></td><td><pre>    \"query\": \"数据集代表请求的表头名称（默认：input）\",</pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    \"response\": \"数据集代表回答的表头名称（默认：output）\",</pre></td></tr><tr><td data-num=\"16\"></td><td><pre>    \"history\": \"数据集代表历史对话的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"17\"></td><td><pre>    \"messages\": \"数据集代表消息列表的表头名称（默认：conversations）\",</pre></td></tr><tr><td data-num=\"18\"></td><td><pre>    \"system\": \"数据集代表系统提示的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"19\"></td><td><pre>    \"tools\": \"数据集代表工具描述的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"20\"></td><td><pre>    \"images\": \"数据集代表图像输入的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"21\"></td><td><pre>    \"chosen\": \"数据集代表更优回答的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"22\"></td><td><pre>    \"rejected\": \"数据集代表更差回答的表头名称（默认：None）\",</pre></td></tr><tr><td data-num=\"23\"></td><td><pre>    \"kto_tag\": \"数据集代表 KTO 标签的表头名称（默认：None）\"</pre></td></tr><tr><td data-num=\"24\"></td><td><pre>  &#125;,</pre></td></tr><tr><td data-num=\"25\"></td><td><pre>  \"tags（可选，用于 sharegpt 格式）\": &#123;</pre></td></tr><tr><td data-num=\"26\"></td><td><pre>    \"role_tag\": \"消息中代表发送者身份的键名（默认：from）\",</pre></td></tr><tr><td data-num=\"27\"></td><td><pre>    \"content_tag\": \"消息中代表文本内容的键名（默认：value）\",</pre></td></tr><tr><td data-num=\"28\"></td><td><pre>    \"user_tag\": \"消息中代表用户的 role_tag（默认：human）\",</pre></td></tr><tr><td data-num=\"29\"></td><td><pre>    \"assistant_tag\": \"消息中代表助手的 role_tag（默认：gpt）\",</pre></td></tr><tr><td data-num=\"30\"></td><td><pre>    \"observation_tag\": \"消息中代表工具返回结果的 role_tag（默认：observation）\",</pre></td></tr><tr><td data-num=\"31\"></td><td><pre>    \"function_tag\": \"消息中代表工具调用的 role_tag（默认：function_call）\",</pre></td></tr><tr><td data-num=\"32\"></td><td><pre>    \"system_tag\": \"消息中代表系统提示的 role_tag（默认：system，会覆盖 system column）\"</pre></td></tr><tr><td data-num=\"33\"></td><td><pre>  &#125;</pre></td></tr><tr><td data-num=\"34\"></td><td><pre>&#125;</pre></td></tr></table></figure><p>  添加完成数据集描述后，即可在在 <code>LLaMA-Factory-main/data</code>  文件夹内创建对应数据集名称的 <code>json</code>  文件，即可完成自定义数据集的添加。数据集的格式需要和数据集描述一致，详细的示例可以参考初始在 <code>LLaMA-Factory-main/data</code>  文件夹下的其他 <code>json</code>  数据集文件。</p>\n<h2 id=\"34-使用llama-factory进行大模型微调\"><a class=\"markdownIt-Anchor\" href=\"#34-使用llama-factory进行大模型微调\">#</a> 3.4 使用 LLaMA-Factory 进行大模型微调</h2>\n<p>  在准备好微调的数据集之后，即可再次运行 <code>LLaMA-Factory-main/src/webui.py</code> ，启动 LLaMA-Factory 的可视化界面，其中的部分参数定义如下，需要注意的是数据路径应该指定为本地计算机 <code>LLaMA-Factory-main/data</code>  文件夹的绝对路径：</p>\n<p><img data-src=\"/images/AI/LLM_finetune/3.2.png\" alt=\"\"></p>\n<p>  定义完成训练参数后即可点击 “开始” 按钮，开始模型的微调训练。模型训练完毕后，点击 <code>Chat</code>  选项卡，检查点路径选择训练好的大模型，即可开始与微调完成的大模型进行在线对话。点击 <code>Export</code>  选项卡，指定导出目录以及其他设置，点击 “开始导出”，即可导出训练完毕的大模型。至此，已经完成使用 <code>LLaMA-Factory</code>  进行大模型微调的全部过程。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/07/23/AI/Prompt_engineering/",
            "url": "http://qianqiu-cell.github.io/2024/07/23/AI/Prompt_engineering/",
            "title": "大模型提示词工程（Prompt Engineering）",
            "date_published": "2024-07-22T16:00:00.000Z",
            "content_html": "<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL3dzeWFkYy9nZW5lcmF0aXZlLWFpLWZvci1iZWdpbm5lcnM=\">https://github.com/wsyadc/generative-ai-for-beginners</span></p>\n<h1 id=\"一-什么是提示词工程\"><a class=\"markdownIt-Anchor\" href=\"#一-什么是提示词工程\">#</a> 一、什么是提示词工程</h1>\n<p>  提示工程被定义为设计和优化文本输入（提示）以提供一致且高质量响应（完成）的过程，以实现特定的应用目标和模型。提示工程是创建将产生所需结果的提示的过程。 提示工程不仅仅是编写文本提示。提示工程不是一门工程学科，它更像是一组可以应用以获得所需结果的技术。我们可以将其视为一个两步过程：</p>\n<ul>\n<li>为特定模型和目标设计初始提示</li>\n<li>迭代地优化提示以提高响应质量</li>\n</ul>\n<p>  这必然是一个需要用户直觉和努力的试错过程，以获得最佳结果。那么，为什么这很重要？要回答这个问题，我们首先需要理解三个概念：</p>\n<ul>\n<li>Tokenization = 模型如何 “看到” 提示</li>\n<li>Base LLMs = 基础模型如何 “处理” 提示</li>\n<li>Instruction-Tuned LLMs = 模型现在如何看到 “任务”</li>\n</ul>\n<h1 id=\"二-为什么我们需要提示词工程\"><a class=\"markdownIt-Anchor\" href=\"#二-为什么我们需要提示词工程\">#</a> 二、为什么我们需要提示词工程</h1>\n<p>  现在我们知道了 LLMs 如何处理提示，让我们谈谈为什么我们需要提示工程。 答案在于，当前的 LLMs 的算法也有许多挑战，如果不及时优化，就很难实现 “可靠且一致的补全”。 例如：</p>\n<ul>\n<li>\n<p>== 模型响应是随机的。== 相同的提示可能会针对不同的模型或模型版本产生不同的响应。 甚至可能在不同时间使用相同模型产生不同的结果。 提示工程技术可以通过提供更好帮助我们最大限度地减少这些变化所带来的影响。</p>\n</li>\n<li>\n<p>== 模型可以产生幻觉响应。== 模型是使用大型但有限数据集进行预训练的，这意味着它们缺乏有关训练范围之外的概念的知识。 因此，它们可能会产生不准确、虚构或与已知事实直接矛盾的完成结果。 提示工程技术可以帮助用户识别和减轻幻觉，例如通过向人工智能询问出处或推理过程。</p>\n</li>\n<li>\n<p>== 模型功能会有所不同。 == 较新的模型或模型迭代将具有更丰富的功能，但也会带来独特的怪癖以及成本和复杂性方面的平衡。 提示工程可以帮助我们开发最佳实践和工作流程，以可扩展和无缝的方式消除差异并适应特定于模型的要求。</p>\n</li>\n</ul>\n<h1 id=\"三-提示工程的技巧\"><a class=\"markdownIt-Anchor\" href=\"#三-提示工程的技巧\">#</a> 三、提示工程的技巧</h1>\n<p>  简单提示的局限性：简单提示可能无法得到你想要的结果，原因如下：（1）大话题；（2）没有限定输出格式。我们可以使用一些基本技巧来提示 LLM。</p>\n<h2 id=\"31-zero-shot-prompting-零样本提示\"><a class=\"markdownIt-Anchor\" href=\"#31-zero-shot-prompting-零样本提示\">#</a> 3.1 Zero-shot prompting （零样本提示）</h2>\n<p>  这种提示风格非常简单，它只有一个提示。当你开始学习 LLM 时，可能就会用到这种方法。下面是一个例子：</p>\n<ul>\n<li>Prompt: “What is Algebra?”</li>\n<li>Answer: “Algebra is a branch of mathematics that studies mathematical symbols and the rules for manipulating these symbols.”</li>\n</ul>\n<h2 id=\"32-few-shot-prompting-少样本提示\"><a class=\"markdownIt-Anchor\" href=\"#32-few-shot-prompting-少样本提示\">#</a> 3.2 Few-shot prompting （少样本提示）</h2>\n<p>  这种提示方式通过在提出请求的同时提供一些示例来帮助模型。它由单个提示和附加的特定任务数据组成。下面是一个例子：</p>\n<ul>\n<li>Prompt: &quot;以莎士比亚的风格写一首诗。下面是一些莎士比亚十四行诗的例子：十四行诗第 18 首：&quot; 我要把你比作夏日吗？你更可爱，更有节制…' 第 116 首十四行诗：&quot; 让我不要为真正心灵的结合设置障碍。爱不是爱，当它发现改变时就会改变…' 十四行诗第 132 首：“我爱你的眼睛，它们怜悯我，知道你的心在折磨我，对我不屑一顾… 现在，请写一首关于月亮之美的十四行诗。”</li>\n<li>Answer：“在天空中，月亮闪烁着柔和的光芒，散发着温柔的光辉，…”</li>\n</ul>\n<p>  示例为 LLM 提供了所需输出的背景、格式或风格。它们有助于模型理解具体任务，并生成更准确、更相关的回复。</p>\n<h2 id=\"33-chain-of-thought-cot-思维链\"><a class=\"markdownIt-Anchor\" href=\"#33-chain-of-thought-cot-思维链\">#</a> 3.3 Chain-of-thought （CoT, 思维链）</h2>\n<p>  链式思维是一种非常有趣的技术，因为它涉及让大型语言模型（LLM）经过一系列步骤。这个想法是以一种方式指导 LLM，使其理解如何完成某项任务。请考虑以下带有和不带链式思维的示例：</p>\n<ul>\n<li>Prompt: “Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?”</li>\n<li>Answer: 5</li>\n</ul>\n<p>  LLM 给出的答案为 5，这是不正确的。 根据计算结果 (5 -3 -2 + 1 = 1)，正确答案是 1 个苹果。</p>\n<p>  那么我们怎样才能教 LLM 正确地做到这一点呢？让我们尝试一下思维链。 应用思维链意味着：</p>\n<p>  给 LLM 一个类似的例子。展示计算结果，以及如何正确计算。提供原始提示。</p>\n<ul>\n<li>Prompt: “Lisa has 7 apples, throws 1 apple, gives 4 apples to Bart and Bart gives one back: 7 -1 = 6 6 -4 = 2 2 +1 = 3<br>\nAlice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?”</li>\n<li>Answer: 1</li>\n</ul>\n<p>  请注意我们如何用另一个示例、计算和原始提示编写更长的提示，然后得出正确答案 1。正如您所看到的，思维链是一种非常强大的技术。</p>\n<p>  其他更加丰富的 CoT，如零样本思维链，自洽性等可以参考<span class=\"exturl\" data-url=\"aHR0cHM6Ly9wcm9tcHRkZXYuYWkvemgtSGFucy9kb2NzL2ludGVybWVkaWF0ZS9zZWxmX2NvbnNpc3RlbmN5\"> https://promptdev.ai/zh-Hans/docs/intermediate/self_consistency</span>。</p>\n<h2 id=\"34-generated-knowledge-知识生成\"><a class=\"markdownIt-Anchor\" href=\"#34-generated-knowledge-知识生成\">#</a> 3.4 Generated knowledge （知识生成）</h2>\n<p>  生成的知识方法（Generated Knowledge Approach）1 要求 LLM 在生成响应之前生成与问题相关的可能有用的信息。该方法由两个中间步骤组成，即<mark>知识生成和知识集成</mark>。</p>\n<p><img data-src=\"/images/AI/Prompt_engineering/3.4.1.png\" alt=\"\"></p>\n<p>（1）知识生成</p>\n<p>  在知识生成步骤中，要求 LLM 生成有关问题的一组事实。大语言模型将以 few-shot 方式进行提示，如下所示。使用相同提示生成 M 个不同的完成。</p>\n<p><img data-src=\"/images/AI/Prompt_engineering/3.4.2.png\" alt=\"\"></p>\n<p>（2）知识集成</p>\n<p>  接下来，我们生成 “知识增强” 问题，并用它们提示 LLM 获得最终答案。最好的理解方法是通过一个例子来说明。</p>\n<p>  假设我们正在尝试回答问题 “大多数袋鼠有 <mask> 肢体”。假设在知识生成步骤中，我们生成了 2 个知识（M=2）：</p>\n<ul>\n<li>\n<p>知识 1：“袋鼠是生活在澳大利亚的有袋动物。”</p>\n</li>\n<li>\n<p>知识 2：“袋鼠是有 5 条肢体的有袋动物。”</p>\n</li>\n</ul>\n<p>  现在，我们将每个知识与问题连接起来，生成知识增强的问题：</p>\n<ul>\n<li>\n<p>知识增强问题 1：“大多数袋鼠有 <mask> 肢体。袋鼠是生活在澳大利亚的有袋动物。”</p>\n</li>\n<li>\n<p>知识增强问题 2：“大多数袋鼠有 <mask> 肢体。袋鼠是有 5 条肢体的有袋动物。”</p>\n</li>\n</ul>\n<p>  然后，我们用这些知识增强的问题提示 LLM，并获得最终答案的提案：</p>\n<ul>\n<li>\n<p>答案 1：“4”</p>\n</li>\n<li>\n<p>答案 2：“5”</p>\n</li>\n</ul>\n<p>  我们选择概率最高的答案作为最终答案。最高概率可能是答案令牌的 softmax 概率，或答案令牌的对数概率。</p>\n<h2 id=\"35-least-to-most-ltm-从少到多\"><a class=\"markdownIt-Anchor\" href=\"#35-least-to-most-ltm-从少到多\">#</a> 3.5 Least to Most （LtM, 从少到多）</h2>\n<p>  从最少到最多提示 (LtM) 1 使 CoT 提示更进一步，首先将问题分解为子问题，然后解决每个问题。这是一种受现实世界儿童教育策略启发的技术。</p>\n<p>  与 CoT 提示一样，要解决的问题被分解为一组相互构建的子问题。第二步，将这些子问题一一解决。与思维链相反，先前子问题的解决方案被输入到尝试解决下一个问题的提示中。</p>\n<p><img data-src=\"/images/AI/Prompt_engineering/3.5.1.png\" alt=\"\"></p>\n<h2 id=\"36-self-refine-critique-the-results-自我完善质疑结果\"><a class=\"markdownIt-Anchor\" href=\"#36-self-refine-critique-the-results-自我完善质疑结果\">#</a> 3.6 Self-refine, critique the results （自我完善，质疑结果）</h2>\n<p>  对于生成式人工智能和 LLMs，你不能相信其输出。 你需要验证一下。 毕竟， LLMs 只是向您展示下一个最有可能说的话，而不是正确的内容。 因此，一个好主意是要求 LLMs 自我批评，这引导我们自我完善技术。</p>\n<p>  其工作原理是按照以下步骤操作：</p>\n<ul>\n<li>要求 LLM 解决问题的初始提示</li>\n<li>LLM 产生答案</li>\n<li>质疑答案并要求人工智能改进</li>\n<li>LLM 再次回答，这次考虑了质疑并提出了解决方案</li>\n</ul>\n<p>  您可以根据需要多次重复此过程。</p>\n<h2 id=\"37-maieutic-prompting-多维度的提示\"><a class=\"markdownIt-Anchor\" href=\"#37-maieutic-prompting-多维度的提示\">#</a> 3.7 Maieutic prompting （多维度的提示）</h2>\n<p>  多维度的提示是一种类似于自我完善的技术，但它更多的是要求 LLMs 解释自己。 目标是减少 LLMs 输出不一致，以确保得出正确的答案。 要遵循的工作流程是：</p>\n<ul>\n<li>请 LLM 回答问题</li>\n<li>对于答案的每一部分，请 LLM 更深入地解释。</li>\n<li>如果存在不一致，则丢弃不一致的部分。</li>\n</ul>\n<p>  重复 2 和 3，直到您完成所有部分并对答案感到满意为止。</p>\n<h1 id=\"四-提示的关键要素\"><a class=\"markdownIt-Anchor\" href=\"#四-提示的关键要素\">#</a> 四、提示的关键要素</h1>\n<p>  在之前的页面中，我们已经讨论了几种不同的提示策略。本页将提供一般建议，这些建议对于提示的实际编写很重要</p>\n<p>（1）基本事实的重要性不大</p>\n<p>  令人惊讶的是，在提示中提供少量 exemplars 时，实际答案 (gold) 并不重要。即使在样本中提供随机标签，性能也几乎不受影响。</p>\n<p>（2）标签空间很重要</p>\n<p>  尽管样本中的黄金标签并不重要，但 labelspace 很重要。即使从标签空间中提供随机标签，也有助于大语言模型更好地理解标签空间并提高结果。此外，正确地在示例中表示标签空间的分布很重要。与在示例中从标签空间中均匀采样不同，最好按照标签的真实分布进行采样。</p>\n<p>（3）格式很重要</p>\n<p>  样本的格式或许是最重要的部分，因为它指示大语言格式如何正确地格式化其对提示的答案。</p>\n<p>  例如，请考虑以下样本。它们使用全大写的单词作为答案。尽管这些答案完全错误（2+2 不是 50），但 GPT-3 正确地回答了最后一个问题，并按照其他样本的格式进行回答。</p>\n<figure class=\"highlight markdown\"><figcaption data-lang=\"markdown\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>2+2等于多少？ </pre></td></tr><tr><td data-num=\"2\"></td><td><pre>五十</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>20+5等于多少？</pre></td></tr><tr><td data-num=\"4\"></td><td><pre>四十三</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>12+9等于多少？</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>二十一</pre></td></tr></table></figure><p>  以下是一些值得考虑的良好做法：</p>\n<ul>\n<li>指定上下文。 上下文很重要，您可以指定的领域、主题等越多越好。</li>\n<li>限制输出。 如果您想要特定数量的项目或特定长度，请指定。</li>\n<li>指定内容和方式。 请记住提及您想要什么以及您想要如何实现，例如 “创建一个包含路由产品和客户的 Python Web API，将其分为 3 个文件”。</li>\n<li>使用模板。 通常，您会希望使用公司的数据来丰富提示。 使用模板来执行此操作。 模板可以包含用实际数据替换的变量。</li>\n<li>拼写正确。 LLMs 可能会为您提供正确的答案，但如果您拼写正确，您会得到更好的答案。</li>\n</ul>\n<h1 id=\"五-din-sql和pet-sql\"><a class=\"markdownIt-Anchor\" href=\"#五-din-sql和pet-sql\">#</a> 五、DIN-SQL 和 PET-SQL</h1>\n<p>  用于参赛《中兴捧月大赛 - 精言妙喻》，主要参考论文有 DIN-SQL：《DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction》，PET-SQL：《PET-SQL: A Prompt-Enhanced Two-Round Refinement of Text-to-SQL with Cross-consistency》。两篇论文的主要思路如下：</p>\n<p><img data-src=\"/images/AI/Prompt_engineering/5.1.jpg\" alt=\"\"></p>\n<h1 id=\"六-中兴捧月大赛-精言妙喻个人初赛代码及思路\"><a class=\"markdownIt-Anchor\" href=\"#六-中兴捧月大赛-精言妙喻个人初赛代码及思路\">#</a> 六、《中兴捧月大赛 - 精言妙喻》个人初赛代码及思路</h1>\n<p>  具体文件可见 U 盘备份–&gt; 科研文件–&gt;9、第十四届中兴捧月全球精英挑战赛等。</p>\n",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/05/16/AI/Transformer/",
            "url": "http://qianqiu-cell.github.io/2024/05/16/AI/Transformer/",
            "title": "Transformer模型",
            "date_published": "2024-05-15T16:00:00.000Z",
            "content_html": "<p><img data-src=\"/images/AI/Transformer/0.1.jpg\" alt=\"\"></p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93YXlsYW5kemhhbmcuZ2l0aHViLmlvL2VuL3RyYW5zZm9ybWVyLWFyY2hpdGVjdHVyZS5odG1sIzQtNy1jYWxjdWxhdGUtdi1hdHRlbnRpb24=\">https://waylandzhang.github.io/en/transformer-architecture.html#4-7-calculate-v-attention</span>、<span class=\"exturl\" data-url=\"aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMzU0NjYxMTUyNzQ1MzE2MT9zcG1faWRfZnJvbT0zMzMuNzg4LjAuMA==\">https://space.bilibili.com/3546611527453161?spm_id_from=333.788.0.0</span></p>\n<p>   <code>Transformer</code>  模型由两部分组成：编码器和解码器。一般来说，仅编码器的架构精通于从文本中提取信息，用于分类和回归等任务，而仅解码器的模型专门用于生成文本。例如，<mark>专注于文本生成的 <code>GPT</code>  属于仅解码器模型的类别</mark>。</p>\n<p>   <code>Transformer</code>  的大致过程如下：</p>\n<ul>\n<li>首先，我们需要一系列输入字符作为训练数据。这些输入被转换成矢量嵌入格式。</li>\n<li>接下来，我们将位置编码添加到矢量嵌入中，以捕获序列中每个字符的位置。</li>\n<li>随后，该模型通过一系列计算操作处理这些输入嵌入，最终为给定的输入文本生成可能的下一个字符的概率分布。</li>\n<li>该模型根据训练数据集中的实际后续特征来评估预测结果，并相应地调整概率或 “权重”。</li>\n<li>最后，该模型迭代地细化这个过程，不断更新其参数，以提高未来预测的精度。</li>\n</ul>\n<h1 id=\"一-tokenizer\"><a class=\"markdownIt-Anchor\" href=\"#一-tokenizer\">#</a> 一、Tokenizer</h1>\n<p>   <code>Tokenizer</code>  分词算法是 <code>NLP</code>  大模型最基础的组件，基于 <code>Tokenizer</code>  可以<mark>将文本转换成独立的 <code>token</code>  列表，进而转换成输入的向量成为计算机可以理解的输入形式</mark>。</p>\n<p>   <code>tiktoken</code>  是一种快速  <code>BPE</code>  标记器，可与 <code>OpenAI</code>  模型一起使用。</p>\n<p>   <code>tiktoken</code>  使用方法为：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">import</span> tiktoken</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># Using TikToken (Same as GPT3) to tokenize the source text</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>encoding <span class=\"token operator\">=</span> tiktoken<span class=\"token punctuation\">.</span>get_encoding<span class=\"token punctuation\">(</span><span class=\"token string\">\"cl100k_base\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># text 保存了 str 变量类型的文本内容 --> 输出为以单词为单位的 int 列表</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>tokenized_text <span class=\"token operator\">=</span> encoding<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\"># 将 tokenized_text 转换为 Tensor.int64 类型的张量</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>tokenized_text <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>  上述的代码提供了 <code>encoding</code>  的编码过程，同时还可以根据 <code>tokenized</code>  的编码结果进行解码，只需要通过 <code>decode</code>  函数输入 <code>int</code>  类型的列表集合返回原始的 str 类型文本</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># 输出单个编码对应的 str 文本</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>encoding<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># 输出多个编码对应的 str 文本</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>encoding<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"二-embedding\"><a class=\"markdownIt-Anchor\" href=\"#二-embedding\">#</a> 二、Embedding</h1>\n<p>   <code>Tokenize</code>  完的下一步就是将 <code>token</code>  的 <code>one-hot</code>  编码转换成更 <code>dense</code>  的 <code>embedding</code>  编码。</p>\n<p>   <code>Embedding</code>  矩阵的本质就是一个查找表。由于输入向量是 <code>one-hot</code>  的， <code>embedding</code>  矩阵中有且仅有一行被激活。行间互不干扰。如下图所示，假设词汇表一共有 <code>6</code>  个词，则 <code>one-hot</code>  表示的长度为 <code>6</code> 。现在我们有三个单词组成一个句子，则输入矩阵的形状为 <code>(3,6)</code>  。然后我们学出来一个 <code>embedding</code>  矩阵，根据上面的推导，如果我们的 <code>embedding size</code> （编码向量的长度）为 <code>4</code> ，则 <code>embedding</code>  矩阵的形状应该为 <code>(6,4)</code> 。这样乘出来的输出矩阵的形状应为 <code>(3,4)</code> 。</p>\n<p><img data-src=\"/images/AI/Transformer/2.1.jpg\" alt=\"\"></p>\n<p>  对于第一个单词 <code>I</code> ，假设其 <code>one-hot</code>  编码为 <code>[0,0,1,0,0,0]</code> ，将其与 <code>embedding</code>  矩阵相乘，相当于取出 <code>embedding</code>  矩阵的第 <code>3</code>  行（ <code>index</code>  为 <code>2</code> ）。同理，对于单词 <code>love</code> ，相当于取出 <code>embedding</code>  矩阵的第二行（ <code>index</code>  为 <code>1</code> ）。因此 <code>embedding</code>  矩阵的本质是一个查找表，每个单词会定位这个表中的某一行，而这一行就是这个单词学习到的在嵌入空间的语义。</p>\n<p>  首先准备所需要的数据，在 transformer 的解码器中，需要输入 <code>n_batch * context_length * d_model</code>  维度的 <code>Tensor</code>  数据，其中 <code>n_batch</code>  表示批次大小， <code>contex_length</code>  表示一次输入的单次数量， <code>d_model</code>  表示编码向量的长度。将 <code>Tokenizer</code>  中获得的 <code>tokenized_text</code>  进行数据预处理：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># Split train and validation</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>split_idx <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>train_data <span class=\"token operator\">=</span> tokenized_text<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>split_idx<span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>val_data <span class=\"token operator\">=</span> tokenized_text<span class=\"token punctuation\">[</span>split_idx<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\"># Get input embedding batch</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>data <span class=\"token operator\">=</span> train_data</pre></td></tr><tr><td data-num=\"8\"></td><td><pre>idxs <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span>low<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> high<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> context_length<span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>x_batch <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">:</span>idx <span class=\"token operator\">+</span> context_length<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> idxs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>y_batch <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data<span class=\"token punctuation\">[</span>idx <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>idx <span class=\"token operator\">+</span> context_length <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> idxs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>  之后便可以利用 <code>torch</code>  中的 <code>nn.Embedding</code>  函数构造 <code>Embedding</code>  层。其中 <code>Embedding.weight.data</code>  是一个 <code>max_token_value * d_model</code>  维度的 <code>Tensor</code>  变量，是模型需要训练的参数，同时也是上图中对应的 <code>Embedding</code>  查找表。</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># define input embedding table</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># 获取 tokenized_text 中的最大值 + 1，用于构造 Embedding 的行</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>max_token_value <span class=\"token operator\">=</span> tokenized_text<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># 使用 nn.Embedding 函数构造 Embedding 层</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\"># `Embedding.weight.data` 是一个 `max_token_value * d_model` 维度的 `Tensor` 变量</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>token_embedding_lookup_table <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_embeddings<span class=\"token operator\">=</span>max_token_value<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># 通过输入 x_batch 或 y_batch 即可获得对应的 Embedding 编码结果</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\"># x_batch_embedding 和 y_batch_embedding 是 `n_batch * context_length * d_model` 维度的 `Tensor` 数据</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>x_batch_embedding <span class=\"token operator\">=</span> token_embedding_lookup_table<span class=\"token punctuation\">(</span>x_batch<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>y_batch_embedding <span class=\"token operator\">=</span> token_embedding_lookup_table<span class=\"token punctuation\">(</span>y_batch<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"三-position-encoding\"><a class=\"markdownIt-Anchor\" href=\"#三-position-encoding\">#</a> 三、Position Encoding</h1>\n<p>  在 <code>transformer</code>  的 <code>encoder</code>  和 <code>decoder</code>  的输入层中，均使用了 <code>Positional Encoding</code> ，使得最终的输入满足： <code>input = input_embedding + positional_encoding</code> 。</p>\n<p>   <code>Transformer</code>  位置编码的定义为：</p>\n<p><img data-src=\"/images/AI/Transformer/3.1.png\" alt=\"\"></p>\n<p>  实现位置编码的代码为：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># get positional encoding</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>position_encoding_lookup_table <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>context_length<span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># unsqueeze 用来扩充一个维度，为了后面的逐元素计算时的广播机制</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>position <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> context_length<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">float</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\"># 根据公式计算位置编码</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>div_term <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>math<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span><span class=\"token number\">10000.0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>position_encoding_lookup_table<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sin<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>position_encoding_lookup_table<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cos<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\"># 将 context_length*d_model 的矩阵复制 n_epoch 次，形成 n_epoch*context_length*d_model 的矩阵</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>position_encoding_lookup_table <span class=\"token operator\">=</span> position_encoding_lookup_table<span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>  在获得位置编码之后即可将位置编码与 <code>Embedding</code>  进行相加，获得最终输入至网络的输入：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># add positional encoding to the input_embedding</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>x <span class=\"token operator\">=</span> x_batch_embedding <span class=\"token operator\">+</span> position_encoding_lookup_table</pre></td></tr><tr><td data-num=\"3\"></td><td><pre>y <span class=\"token operator\">=</span> y_batch_embedding <span class=\"token operator\">+</span> position_encoding_lookup_table</pre></td></tr></table></figure><h1 id=\"四-transformer-block\"><a class=\"markdownIt-Anchor\" href=\"#四-transformer-block\">#</a> 四、Transformer Block</h1>\n<p><img data-src=\"/images/AI/Transformer/4.1.png\" alt=\"\"></p>\n<p>  通过第三步，我们获得了输入 <code>x</code> ，下一步是开始实现多头注意力块（ <code>Muti-head Attention block</code> ）。</p>\n<p>   <code>Transformer</code>  模型的强大来源于 <code>self-attention</code> ，通过 <code>self-attention</code> ， <code>Transformer</code>  模型可以关注到 <code>input</code>  更加重要的部分。</p>\n<p>   <code>Multi-head attention</code>  由几个单独的 <code>heads</code>  堆叠在一起组成。所有 heads 都接收到完全相同的输入，尽管它们在计算过程中使用了自己的特定权重集。在处理输入之后，来自所有 <code>heads</code>  的输出被级联，然后通过线性层。</p>\n<p>   <code>heads</code>  的工作方式是通过三个独特的层处理，即查询（ <code>Q</code> ）、键（ <code>K</code> ）和值（ <code>V</code> ）。 <code>Attention</code>  的计算公式可以从论文《 <code>Attention is all you need</code> 》中得到：</p>\n<p><img data-src=\"/images/AI/Transformer/4.2.png\" alt=\"\"></p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># get Q, K, V</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># 所谓的多头就是把 d_model 切成多份，每一个头里面有一部分维度，然后去做这一部分的计算，最后再把所有的计算合并在一起</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>head_size <span class=\"token operator\">=</span> d_model <span class=\"token operator\">//</span> num_heads  <span class=\"token comment\"># head size should be divisible by d_model</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># (1) 计算 Q,K,V 矩阵</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>key_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>query_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>value_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>q <span class=\"token operator\">=</span> query_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>k <span class=\"token operator\">=</span> key_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>v <span class=\"token operator\">=</span> value_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token comment\"># [batch_size, context_length, num_heads, head_size]</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>q <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> head_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>k <span class=\"token operator\">=</span> k<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> head_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>v <span class=\"token operator\">=</span> v<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> head_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre><span class=\"token comment\"># [batch_size, num_heads, context_length, head_size]</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>q <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>k <span class=\"token operator\">=</span> k<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>v <span class=\"token operator\">=</span> v<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre><span class=\"token comment\"># (2) 通过 Q @ K^T /sqrt (d_k) 计算 Attention</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>attention_score <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>q @ k<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.0</span> <span class=\"token operator\">/</span> math<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>head_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre><span class=\"token comment\"># (3) 计算 Mask</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre>mask <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>triu<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span>context_length<span class=\"token punctuation\">,</span> context_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> diagonal<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">bool</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"24\"></td><td><pre>attention_score <span class=\"token operator\">=</span> attention_score<span class=\"token punctuation\">.</span>masked_fill<span class=\"token punctuation\">(</span>mask<span class=\"token punctuation\">,</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span><span class=\"token string\">'-inf'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre><span class=\"token comment\"># (4) 计算 Softmax</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre><span class=\"token comment\"># [batch_size, num_heads, context_length, context_length]</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>attention_score <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>attention_score<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre><span class=\"token comment\"># (5) 通过 $V 计算 A</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre><span class=\"token comment\"># [batch_size, num_heads, context_length, head_size]</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre>A <span class=\"token operator\">=</span> attention_score @ v</pre></td></tr><tr><td data-num=\"31\"></td><td><pre><span class=\"token comment\"># (6) 计算 Concatenate</span></pre></td></tr><tr><td data-num=\"32\"></td><td><pre><span class=\"token comment\"># [batch_size, context_length, num_heads, head_size]</span></pre></td></tr><tr><td data-num=\"33\"></td><td><pre>A <span class=\"token operator\">=</span> A<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"34\"></td><td><pre><span class=\"token comment\"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num=\"35\"></td><td><pre>A <span class=\"token operator\">=</span> A<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"36\"></td><td><pre><span class=\"token comment\"># (7) 通过 Wo 计算 Output</span></pre></td></tr><tr><td data-num=\"37\"></td><td><pre><span class=\"token comment\"># Define the output weight matrix</span></pre></td></tr><tr><td data-num=\"38\"></td><td><pre>Wo <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"39\"></td><td><pre><span class=\"token comment\"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num=\"40\"></td><td><pre>output <span class=\"token operator\">=</span> Wo<span class=\"token punctuation\">(</span>A<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"五-residual-connection-and-layer-normalization\"><a class=\"markdownIt-Anchor\" href=\"#五-residual-connection-and-layer-normalization\">#</a> 五、Residual Connection and Layer Normalization</h1>\n<p>  残差连接，有时被称为 <code>skip connection</code> ，是让原始输入 <code>X</code>  绕过一个或多个层的连接。通过将原始输入 <code>x</code>  与步骤四多头注意力层的输出 <code>output</code>  相加即可完成操作。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>+</mo><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">output = output + x\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.80952em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span></p>\n<p>  在残差连接之后，过程进入层归一化。层归一化（ <code>LayerNorm</code> ）是一种用于对网络中每一层的输出进行归一化的技术。其方法是减去输出的均值，并除以输出的标准差。使用这种技术是为了防止某一层的输出变得过大或过小，从而避免网络的不稳定性。</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># Add residual connection</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>output <span class=\"token operator\">=</span> output <span class=\"token operator\">+</span> X</pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># Add Layer Normalization</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>layer_norm <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>output <span class=\"token operator\">=</span> layer_norm<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"六-feed-forward-network\"><a class=\"markdownIt-Anchor\" href=\"#六-feed-forward-network\">#</a> 六、Feed-Forward Network</h1>\n<p>  一旦我们获得了归一化的注意力权重（概率分数），它将被传递到一个位置级前馈网络中进行处理。前馈神经网络（ <code>FFN</code> ）由两个线性层和它们之间的 ReLU 激活函数组成。</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># update x</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>x <span class=\"token operator\">=</span> output</pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># Define Feed Forward Network</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>output <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_model <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>output <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>output <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>d_model <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>output <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>dropout<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> p<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre></pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\"># Add residual connection</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>output <span class=\"token operator\">=</span> output <span class=\"token operator\">+</span> x</pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token comment\"># Add Layer Normalization</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>layer_norm <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>output <span class=\"token operator\">=</span> layer_norm<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"七-repeat-step-4-to-6\"><a class=\"markdownIt-Anchor\" href=\"#七-repeat-step-4-to-6\">#</a> 七、Repeat step 4 to 6</h1>\n<p>  以上我们完成的只是一个 <code>transformer</code>  块。在实际应用中，我们会将多个 <code>transformer</code>  块堆叠在一起，形成一个 <code>transformer</code>  解码器。</p>\n<p>  实际上，我们应该将代码封装到类中，并使用 <code>PyTorch</code>  的 <code>nn.Module</code>  来构建我们的 <code>transformer</code>  解码器。但为了演示，我们只使用一个块。</p>\n<h1 id=\"八-output-probabilities\"><a class=\"markdownIt-Anchor\" href=\"#八-output-probabilities\">#</a> 八、Output Probabilities</h1>\n<p>  应用最后一个线性层来获得我们的 <code>logits</code> ：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>logits <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> max_token_value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>  最后一步是对逻辑回归输出进行 <code>softmax</code>  操作，以获得每个 <code>token</code>  的概率：</p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># but for illustration purpose, we'll use torch.softmax here</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>probabilities <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h1 id=\"full-working-code\"><a class=\"markdownIt-Anchor\" href=\"#full-working-code\">#</a> Full Working Code</h1>\n<p>  完整的代码可以参考 <code>github</code> : <span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL3dheWxhbmR6aGFuZy9UcmFuc2Zvcm1lci1mcm9tLXNjcmF0Y2g=\">https://github.com/waylandzhang/Transformer-from-scratch</span></p>\n<figure class=\"highlight py\"><figcaption data-lang=\"Python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">import</span> os</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">import</span> requests</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token keyword\">import</span> math</pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token keyword\">import</span> tiktoken</pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token keyword\">import</span> torch</pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn</pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">import</span> functional <span class=\"token keyword\">as</span> F</pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\"># Hyperparameters</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>batch_size <span class=\"token operator\">=</span> <span class=\"token number\">4</span>  <span class=\"token comment\"># How many batches per training step</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>context_length <span class=\"token operator\">=</span> <span class=\"token number\">16</span>  <span class=\"token comment\"># Length of the token chunk each batch</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>d_model <span class=\"token operator\">=</span> <span class=\"token number\">64</span>  <span class=\"token comment\"># The size of our model token embeddings</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>num_blocks <span class=\"token operator\">=</span> <span class=\"token number\">8</span>  <span class=\"token comment\"># Number of transformer blocks</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>num_heads <span class=\"token operator\">=</span> <span class=\"token number\">4</span>  <span class=\"token comment\"># Number of heads in Multi-head attention</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>learning_rate <span class=\"token operator\">=</span> <span class=\"token number\">1e-3</span>  <span class=\"token comment\"># 0.001</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>dropout <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span>  <span class=\"token comment\"># Dropout rate</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>max_iters <span class=\"token operator\">=</span> <span class=\"token number\">5000</span>  <span class=\"token comment\"># Total of training iterations &lt;- Change this to smaller number for testing</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre>eval_interval <span class=\"token operator\">=</span> <span class=\"token number\">50</span>  <span class=\"token comment\"># How often to evaluate</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>eval_iters <span class=\"token operator\">=</span> <span class=\"token number\">20</span>  <span class=\"token comment\"># Number of iterations to average for evaluation</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>device <span class=\"token operator\">=</span> <span class=\"token string\">'cuda'</span> <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>is_available<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token string\">'cpu'</span>  <span class=\"token comment\"># Use GPU if it's available.</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>TORCH_SEED <span class=\"token operator\">=</span> <span class=\"token number\">1337</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>torch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span>TORCH_SEED<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"23\"></td><td><pre></pre></td></tr><tr><td data-num=\"24\"></td><td><pre><span class=\"token comment\"># Load training data</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre><span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span><span class=\"token string\">'data/sales_textbook.txt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre>    url <span class=\"token operator\">=</span> <span class=\"token string\">'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre>    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'data/sales_textbook.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"28\"></td><td><pre>        f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span>requests<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>url<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre></pre></td></tr><tr><td data-num=\"30\"></td><td><pre><span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'data/sales_textbook.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre>    text <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"32\"></td><td><pre></pre></td></tr><tr><td data-num=\"33\"></td><td><pre><span class=\"token comment\"># Using TikToken (Same as GPT3) to tokenize the source text</span></pre></td></tr><tr><td data-num=\"34\"></td><td><pre>encoding <span class=\"token operator\">=</span> tiktoken<span class=\"token punctuation\">.</span>get_encoding<span class=\"token punctuation\">(</span><span class=\"token string\">\"cl100k_base\"</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"35\"></td><td><pre>tokenized_text <span class=\"token operator\">=</span> encoding<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"36\"></td><td><pre>max_token_value <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>  <span class=\"token comment\"># the maximum value of the tokenized numbers</span></pre></td></tr><tr><td data-num=\"37\"></td><td><pre>tokenized_text <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># put tokenized text into tensor</span></pre></td></tr><tr><td data-num=\"38\"></td><td><pre></pre></td></tr><tr><td data-num=\"39\"></td><td><pre><span class=\"token comment\"># Split train and validation</span></pre></td></tr><tr><td data-num=\"40\"></td><td><pre>split_idx <span class=\"token operator\">=</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>tokenized_text<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"41\"></td><td><pre>train_data <span class=\"token operator\">=</span> tokenized_text<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>split_idx<span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"42\"></td><td><pre>val_data <span class=\"token operator\">=</span> tokenized_text<span class=\"token punctuation\">[</span>split_idx<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"43\"></td><td><pre></pre></td></tr><tr><td data-num=\"44\"></td><td><pre></pre></td></tr><tr><td data-num=\"45\"></td><td><pre><span class=\"token comment\"># Define Feed Forward Network</span></pre></td></tr><tr><td data-num=\"46\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">FeedForward</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"47\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"48\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"49\"></td><td><pre>        self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">=</span> d_model</pre></td></tr><tr><td data-num=\"50\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> dropout</pre></td></tr><tr><td data-num=\"51\"></td><td><pre>        self<span class=\"token punctuation\">.</span>ffn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"52\"></td><td><pre>            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"53\"></td><td><pre>            nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"54\"></td><td><pre>            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"55\"></td><td><pre>            nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>dropout<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"56\"></td><td><pre>        <span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"57\"></td><td><pre></pre></td></tr><tr><td data-num=\"58\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"59\"></td><td><pre>        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>ffn<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"60\"></td><td><pre></pre></td></tr><tr><td data-num=\"61\"></td><td><pre></pre></td></tr><tr><td data-num=\"62\"></td><td><pre><span class=\"token comment\"># Define Scaled Dot Product Attention</span></pre></td></tr><tr><td data-num=\"63\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">Attention</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"64\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head_size<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"65\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"66\"></td><td><pre>        self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">=</span> d_model</pre></td></tr><tr><td data-num=\"67\"></td><td><pre>        self<span class=\"token punctuation\">.</span>head_size <span class=\"token operator\">=</span> head_size</pre></td></tr><tr><td data-num=\"68\"></td><td><pre>        self<span class=\"token punctuation\">.</span>context_length <span class=\"token operator\">=</span> context_length</pre></td></tr><tr><td data-num=\"69\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> dropout</pre></td></tr><tr><td data-num=\"70\"></td><td><pre></pre></td></tr><tr><td data-num=\"71\"></td><td><pre>        self<span class=\"token punctuation\">.</span>key_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"72\"></td><td><pre>        self<span class=\"token punctuation\">.</span>query_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"73\"></td><td><pre>        self<span class=\"token punctuation\">.</span>value_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"74\"></td><td><pre>        self<span class=\"token punctuation\">.</span>register_buffer<span class=\"token punctuation\">(</span><span class=\"token string\">'tril'</span><span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>tril<span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"75\"></td><td><pre>            torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>context_length<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>context_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Lower triangular mask</span></pre></td></tr><tr><td data-num=\"76\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dropout<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"77\"></td><td><pre></pre></td></tr><tr><td data-num=\"78\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"79\"></td><td><pre>        B<span class=\"token punctuation\">,</span> T<span class=\"token punctuation\">,</span> C <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape  <span class=\"token comment\"># Batch size, Time steps(current context_length), Channels(dimensions)</span></pre></td></tr><tr><td data-num=\"80\"></td><td><pre>        <span class=\"token keyword\">assert</span> T <span class=\"token operator\">&lt;=</span> self<span class=\"token punctuation\">.</span>context_length</pre></td></tr><tr><td data-num=\"81\"></td><td><pre>        <span class=\"token keyword\">assert</span> C <span class=\"token operator\">==</span> self<span class=\"token punctuation\">.</span>d_model</pre></td></tr><tr><td data-num=\"82\"></td><td><pre>        q <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>query_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"83\"></td><td><pre>        k <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>key_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"84\"></td><td><pre>        v <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>value_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"85\"></td><td><pre></pre></td></tr><tr><td data-num=\"86\"></td><td><pre>        <span class=\"token comment\"># Scaled dot product attention: Q @ K^T / sqrt(d_k)</span></pre></td></tr><tr><td data-num=\"87\"></td><td><pre>        weights <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>q @ k<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.0</span> <span class=\"token operator\">/</span> math<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>k<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"88\"></td><td><pre>        <span class=\"token comment\"># Apply masked attention</span></pre></td></tr><tr><td data-num=\"89\"></td><td><pre>        weights <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span>masked_fill<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>tril<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>T<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span>T<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span><span class=\"token string\">'-inf'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"90\"></td><td><pre>        weights <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token operator\">=</span>weights<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"91\"></td><td><pre>        weights <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dropout_layer<span class=\"token punctuation\">(</span>weights<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"92\"></td><td><pre></pre></td></tr><tr><td data-num=\"93\"></td><td><pre>        <span class=\"token comment\"># Apply dot product attention: weights @ V</span></pre></td></tr><tr><td data-num=\"94\"></td><td><pre>        out <span class=\"token operator\">=</span> weights @ v</pre></td></tr><tr><td data-num=\"95\"></td><td><pre>        <span class=\"token keyword\">return</span> out</pre></td></tr><tr><td data-num=\"96\"></td><td><pre></pre></td></tr><tr><td data-num=\"97\"></td><td><pre></pre></td></tr><tr><td data-num=\"98\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">MultiHeadAttention</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"99\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head_size<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"100\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"101\"></td><td><pre>        self<span class=\"token punctuation\">.</span>num_heads <span class=\"token operator\">=</span> num_heads</pre></td></tr><tr><td data-num=\"102\"></td><td><pre>        self<span class=\"token punctuation\">.</span>head_size <span class=\"token operator\">=</span> head_size</pre></td></tr><tr><td data-num=\"103\"></td><td><pre>        self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">=</span> d_model</pre></td></tr><tr><td data-num=\"104\"></td><td><pre>        self<span class=\"token punctuation\">.</span>context_length <span class=\"token operator\">=</span> context_length</pre></td></tr><tr><td data-num=\"105\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> dropout</pre></td></tr><tr><td data-num=\"106\"></td><td><pre></pre></td></tr><tr><td data-num=\"107\"></td><td><pre>        self<span class=\"token punctuation\">.</span>heads <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ModuleList<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>Attention<span class=\"token punctuation\">(</span>head_size<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>num_heads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"108\"></td><td><pre>        self<span class=\"token punctuation\">.</span>projection_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"109\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>dropout<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"110\"></td><td><pre></pre></td></tr><tr><td data-num=\"111\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"112\"></td><td><pre>        out <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>h<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> h <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>heads<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"113\"></td><td><pre>        out <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>projection_layer<span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"114\"></td><td><pre>        out <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dropout_layer<span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"115\"></td><td><pre>        <span class=\"token keyword\">return</span> out</pre></td></tr><tr><td data-num=\"116\"></td><td><pre></pre></td></tr><tr><td data-num=\"117\"></td><td><pre></pre></td></tr><tr><td data-num=\"118\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">TransformerBlock</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"119\"></td><td><pre></pre></td></tr><tr><td data-num=\"120\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"121\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"122\"></td><td><pre>        self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">=</span> d_model</pre></td></tr><tr><td data-num=\"123\"></td><td><pre>        self<span class=\"token punctuation\">.</span>context_length <span class=\"token operator\">=</span> context_length</pre></td></tr><tr><td data-num=\"124\"></td><td><pre>        self<span class=\"token punctuation\">.</span>head_size <span class=\"token operator\">=</span> d_model <span class=\"token operator\">//</span> num_heads  <span class=\"token comment\"># head size should be divisible by d_model</span></pre></td></tr><tr><td data-num=\"125\"></td><td><pre>        self<span class=\"token punctuation\">.</span>num_heads <span class=\"token operator\">=</span> num_heads</pre></td></tr><tr><td data-num=\"126\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> dropout</pre></td></tr><tr><td data-num=\"127\"></td><td><pre></pre></td></tr><tr><td data-num=\"128\"></td><td><pre>        self<span class=\"token punctuation\">.</span>multi_head_attention_layer <span class=\"token operator\">=</span> MultiHeadAttention<span class=\"token punctuation\">(</span>head_size<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>head_size<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"129\"></td><td><pre>        self<span class=\"token punctuation\">.</span>feed_forward_layer <span class=\"token operator\">=</span> FeedForward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"130\"></td><td><pre>        self<span class=\"token punctuation\">.</span>layer_norm_1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span>normalized_shape<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"131\"></td><td><pre>        self<span class=\"token punctuation\">.</span>layer_norm_2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span>normalized_shape<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"132\"></td><td><pre></pre></td></tr><tr><td data-num=\"133\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"134\"></td><td><pre>        <span class=\"token comment\"># Note: The order of the operations is different from the original Transformer paper</span></pre></td></tr><tr><td data-num=\"135\"></td><td><pre>        <span class=\"token comment\"># The order here is: LayerNorm -> Multi-head attention -> LayerNorm -> Feed forward</span></pre></td></tr><tr><td data-num=\"136\"></td><td><pre>        x <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>multi_head_attention_layer<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>layer_norm_1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Residual connection</span></pre></td></tr><tr><td data-num=\"137\"></td><td><pre>        x <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>feed_forward_layer<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>layer_norm_2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Residual connection</span></pre></td></tr><tr><td data-num=\"138\"></td><td><pre>        <span class=\"token keyword\">return</span> x</pre></td></tr><tr><td data-num=\"139\"></td><td><pre></pre></td></tr><tr><td data-num=\"140\"></td><td><pre></pre></td></tr><tr><td data-num=\"141\"></td><td><pre><span class=\"token keyword\">class</span> <span class=\"token class-name\">TransformerLanguageModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"142\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"143\"></td><td><pre>        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"144\"></td><td><pre>        self<span class=\"token punctuation\">.</span>d_model <span class=\"token operator\">=</span> d_model</pre></td></tr><tr><td data-num=\"145\"></td><td><pre>        self<span class=\"token punctuation\">.</span>context_length <span class=\"token operator\">=</span> context_length</pre></td></tr><tr><td data-num=\"146\"></td><td><pre>        self<span class=\"token punctuation\">.</span>num_heads <span class=\"token operator\">=</span> num_heads</pre></td></tr><tr><td data-num=\"147\"></td><td><pre>        self<span class=\"token punctuation\">.</span>num_blocks <span class=\"token operator\">=</span> num_blocks</pre></td></tr><tr><td data-num=\"148\"></td><td><pre>        self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> dropout</pre></td></tr><tr><td data-num=\"149\"></td><td><pre>        self<span class=\"token punctuation\">.</span>max_token_value <span class=\"token operator\">=</span> max_token_value</pre></td></tr><tr><td data-num=\"150\"></td><td><pre>        <span class=\"token comment\"># Set up token embedding look-up table</span></pre></td></tr><tr><td data-num=\"151\"></td><td><pre>        self<span class=\"token punctuation\">.</span>token_embedding_lookup_table <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>num_embeddings<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>max_token_value <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> embedding_dim<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"152\"></td><td><pre></pre></td></tr><tr><td data-num=\"153\"></td><td><pre>        <span class=\"token comment\"># Run all the transformer blocks</span></pre></td></tr><tr><td data-num=\"154\"></td><td><pre>        <span class=\"token comment\"># Different from original paper, here we add a final layer norm after all the blocks</span></pre></td></tr><tr><td data-num=\"155\"></td><td><pre>        self<span class=\"token punctuation\">.</span>transformer_blocks <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">(</span></pre></td></tr><tr><td data-num=\"156\"></td><td><pre>                <span class=\"token punctuation\">[</span>TransformerBlock<span class=\"token punctuation\">(</span>num_heads<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>num_heads<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>num_blocks<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span></pre></td></tr><tr><td data-num=\"157\"></td><td><pre>                <span class=\"token punctuation\">[</span>nn<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"158\"></td><td><pre>        <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"159\"></td><td><pre>        self<span class=\"token punctuation\">.</span>language_model_out_linear_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> out_features<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>max_token_value<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"160\"></td><td><pre></pre></td></tr><tr><td data-num=\"161\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> idx<span class=\"token punctuation\">,</span> targets<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"162\"></td><td><pre>        B<span class=\"token punctuation\">,</span> T <span class=\"token operator\">=</span> idx<span class=\"token punctuation\">.</span>shape</pre></td></tr><tr><td data-num=\"163\"></td><td><pre>        <span class=\"token triple-quoted-string string\">\"\"\"</pre></td></tr><tr><td data-num=\"164\"></td><td><pre>        # Set up position embedding look-up table</pre></td></tr><tr><td data-num=\"165\"></td><td><pre>        # following the same approach as the original Transformer paper (Sine and Cosine functions)</pre></td></tr><tr><td data-num=\"166\"></td><td><pre>        \"\"\"</span></pre></td></tr><tr><td data-num=\"167\"></td><td><pre>        position_encoding_lookup_table <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>context_length<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"168\"></td><td><pre>        position <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>context_length<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">float</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"169\"></td><td><pre>        div_term <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>math<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span><span class=\"token number\">10000.0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> self<span class=\"token punctuation\">.</span>d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"170\"></td><td><pre>        position_encoding_lookup_table<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sin<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"171\"></td><td><pre>        position_encoding_lookup_table<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cos<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"172\"></td><td><pre>        <span class=\"token comment\"># change position_encoding_lookup_table from (context_length, d_model) to (T, d_model)</span></pre></td></tr><tr><td data-num=\"173\"></td><td><pre>        position_embedding <span class=\"token operator\">=</span> position_encoding_lookup_table<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>T<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"174\"></td><td><pre>        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>token_embedding_lookup_table<span class=\"token punctuation\">(</span>idx<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> position_embedding</pre></td></tr><tr><td data-num=\"175\"></td><td><pre>        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>transformer_blocks<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"176\"></td><td><pre>        <span class=\"token comment\"># The \"logits\" are the output values of our model before applying softmax</span></pre></td></tr><tr><td data-num=\"177\"></td><td><pre>        logits <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>language_model_out_linear_layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"178\"></td><td><pre></pre></td></tr><tr><td data-num=\"179\"></td><td><pre>        <span class=\"token keyword\">if</span> targets <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"180\"></td><td><pre>            B<span class=\"token punctuation\">,</span> T<span class=\"token punctuation\">,</span> C <span class=\"token operator\">=</span> logits<span class=\"token punctuation\">.</span>shape</pre></td></tr><tr><td data-num=\"181\"></td><td><pre>            logits_reshaped <span class=\"token operator\">=</span> logits<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>B <span class=\"token operator\">*</span> T<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"182\"></td><td><pre>            targets_reshaped <span class=\"token operator\">=</span> targets<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>B <span class=\"token operator\">*</span> T<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"183\"></td><td><pre>            loss <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>cross_entropy<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token operator\">=</span>logits_reshaped<span class=\"token punctuation\">,</span> target<span class=\"token operator\">=</span>targets_reshaped<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"184\"></td><td><pre>        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"185\"></td><td><pre>            loss <span class=\"token operator\">=</span> <span class=\"token boolean\">None</span></pre></td></tr><tr><td data-num=\"186\"></td><td><pre>        <span class=\"token keyword\">return</span> logits<span class=\"token punctuation\">,</span> loss</pre></td></tr><tr><td data-num=\"187\"></td><td><pre></pre></td></tr><tr><td data-num=\"188\"></td><td><pre>    <span class=\"token keyword\">def</span> <span class=\"token function\">generate</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> idx<span class=\"token punctuation\">,</span> max_new_tokens<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"189\"></td><td><pre>        <span class=\"token comment\"># idx is (B,T) array of indices in the current context</span></pre></td></tr><tr><td data-num=\"190\"></td><td><pre>        <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>max_new_tokens<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"191\"></td><td><pre>            <span class=\"token comment\"># Crop idx to the max size of our positional embeddings table</span></pre></td></tr><tr><td data-num=\"192\"></td><td><pre>            idx_crop <span class=\"token operator\">=</span> idx<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span>self<span class=\"token punctuation\">.</span>context_length<span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"193\"></td><td><pre>            <span class=\"token comment\"># Get predictions</span></pre></td></tr><tr><td data-num=\"194\"></td><td><pre>            logits<span class=\"token punctuation\">,</span> loss <span class=\"token operator\">=</span> self<span class=\"token punctuation\">(</span>idx_crop<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"195\"></td><td><pre>            <span class=\"token comment\"># Get the last time step from logits where the dimensions of the logits are (B,T,C)</span></pre></td></tr><tr><td data-num=\"196\"></td><td><pre>            logits_last_timestep <span class=\"token operator\">=</span> logits<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"197\"></td><td><pre>            <span class=\"token comment\"># Apply softmax to get probabilities</span></pre></td></tr><tr><td data-num=\"198\"></td><td><pre>            probs <span class=\"token operator\">=</span> F<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token operator\">=</span>logits_last_timestep<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"199\"></td><td><pre>            <span class=\"token comment\"># Sample from the probabilities' distribution.</span></pre></td></tr><tr><td data-num=\"200\"></td><td><pre>            idx_next <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>multinomial<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token operator\">=</span>probs<span class=\"token punctuation\">,</span> num_samples<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"201\"></td><td><pre>            <span class=\"token comment\"># Append the sampled indexes idx_next to idx</span></pre></td></tr><tr><td data-num=\"202\"></td><td><pre>            idx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>idx<span class=\"token punctuation\">,</span> idx_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"203\"></td><td><pre>        <span class=\"token keyword\">return</span> idx</pre></td></tr><tr><td data-num=\"204\"></td><td><pre></pre></td></tr><tr><td data-num=\"205\"></td><td><pre></pre></td></tr><tr><td data-num=\"206\"></td><td><pre><span class=\"token comment\"># Initialize the model</span></pre></td></tr><tr><td data-num=\"207\"></td><td><pre>model <span class=\"token operator\">=</span> TransformerLanguageModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"208\"></td><td><pre>model <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"209\"></td><td><pre></pre></td></tr><tr><td data-num=\"210\"></td><td><pre></pre></td></tr><tr><td data-num=\"211\"></td><td><pre><span class=\"token comment\"># Get input embedding batch</span></pre></td></tr><tr><td data-num=\"212\"></td><td><pre><span class=\"token keyword\">def</span> <span class=\"token function\">get_batch</span><span class=\"token punctuation\">(</span>split<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"213\"></td><td><pre>    data <span class=\"token operator\">=</span> train_data <span class=\"token keyword\">if</span> split <span class=\"token operator\">==</span> <span class=\"token string\">'train'</span> <span class=\"token keyword\">else</span> val_data</pre></td></tr><tr><td data-num=\"214\"></td><td><pre>    idxs <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span>low<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> high<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> context_length<span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"215\"></td><td><pre>    x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">:</span>idx <span class=\"token operator\">+</span> context_length<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> idxs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"216\"></td><td><pre>    y <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>data<span class=\"token punctuation\">[</span>idx <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>idx <span class=\"token operator\">+</span> context_length <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> idxs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"217\"></td><td><pre>    <span class=\"token keyword\">return</span> x<span class=\"token punctuation\">,</span> y</pre></td></tr><tr><td data-num=\"218\"></td><td><pre></pre></td></tr><tr><td data-num=\"219\"></td><td><pre></pre></td></tr><tr><td data-num=\"220\"></td><td><pre><span class=\"token comment\"># Calculate loss</span></pre></td></tr><tr><td data-num=\"221\"></td><td><pre><span class=\"token decorator annotation punctuation\">@torch<span class=\"token punctuation\">.</span>no_grad</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"222\"></td><td><pre><span class=\"token keyword\">def</span> <span class=\"token function\">estimate_loss</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"223\"></td><td><pre>    out <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"224\"></td><td><pre>    model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"225\"></td><td><pre>    <span class=\"token keyword\">for</span> split <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'valid'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"226\"></td><td><pre>        losses <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>eval_iters<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"227\"></td><td><pre>        <span class=\"token keyword\">for</span> k <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>eval_iters<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"228\"></td><td><pre>            x_batch<span class=\"token punctuation\">,</span> y_batch <span class=\"token operator\">=</span> get_batch<span class=\"token punctuation\">(</span>split<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"229\"></td><td><pre>            logits<span class=\"token punctuation\">,</span> loss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x_batch<span class=\"token punctuation\">,</span> y_batch<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"230\"></td><td><pre>            losses<span class=\"token punctuation\">[</span>k<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"231\"></td><td><pre>        out<span class=\"token punctuation\">[</span>split<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> losses<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"232\"></td><td><pre>    model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"233\"></td><td><pre>    <span class=\"token keyword\">return</span> out</pre></td></tr><tr><td data-num=\"234\"></td><td><pre></pre></td></tr><tr><td data-num=\"235\"></td><td><pre></pre></td></tr><tr><td data-num=\"236\"></td><td><pre><span class=\"token comment\"># Use AdamW optimizer</span></pre></td></tr><tr><td data-num=\"237\"></td><td><pre>optimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>AdamW<span class=\"token punctuation\">(</span>params<span class=\"token operator\">=</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"238\"></td><td><pre>tracked_losses <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"239\"></td><td><pre><span class=\"token keyword\">for</span> step <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>max_iters<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"240\"></td><td><pre>    <span class=\"token keyword\">if</span> step <span class=\"token operator\">%</span> eval_iters <span class=\"token operator\">==</span> <span class=\"token number\">0</span> <span class=\"token keyword\">or</span> step <span class=\"token operator\">==</span> max_iters <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"241\"></td><td><pre>        losses <span class=\"token operator\">=</span> estimate_loss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"242\"></td><td><pre>        tracked_losses<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>losses<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"243\"></td><td><pre>        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Step:'</span><span class=\"token punctuation\">,</span> step<span class=\"token punctuation\">,</span> <span class=\"token string\">'Training Loss:'</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>losses<span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Validation Loss:'</span><span class=\"token punctuation\">,</span></pre></td></tr><tr><td data-num=\"244\"></td><td><pre>              <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>losses<span class=\"token punctuation\">[</span><span class=\"token string\">'valid'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"245\"></td><td><pre></pre></td></tr><tr><td data-num=\"246\"></td><td><pre>    xb<span class=\"token punctuation\">,</span> yb <span class=\"token operator\">=</span> get_batch<span class=\"token punctuation\">(</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"247\"></td><td><pre>    logits<span class=\"token punctuation\">,</span> loss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>xb<span class=\"token punctuation\">,</span> yb<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"248\"></td><td><pre>    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span>set_to_none<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"249\"></td><td><pre>    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"250\"></td><td><pre>    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"251\"></td><td><pre></pre></td></tr><tr><td data-num=\"252\"></td><td><pre><span class=\"token comment\"># Save the model state dictionary</span></pre></td></tr><tr><td data-num=\"253\"></td><td><pre>torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'model-ckpt.pt'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"254\"></td><td><pre></pre></td></tr><tr><td data-num=\"255\"></td><td><pre><span class=\"token comment\"># Generate</span></pre></td></tr><tr><td data-num=\"256\"></td><td><pre>model<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"257\"></td><td><pre>start <span class=\"token operator\">=</span> <span class=\"token string\">'The salesperson'</span></pre></td></tr><tr><td data-num=\"258\"></td><td><pre>start_ids <span class=\"token operator\">=</span> encoding<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>start<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"259\"></td><td><pre>x <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>start_ids<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"260\"></td><td><pre>y <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>generate<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> max_new_tokens<span class=\"token operator\">=</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"261\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'---------------'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"262\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>encoding<span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tolist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"263\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'---------------'</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure>",
            "tags": [
                "AI"
            ]
        },
        {
            "id": "http://qianqiu-cell.github.io/2024/02/01/AI/Neural_networks_classification/",
            "url": "http://qianqiu-cell.github.io/2024/02/01/AI/Neural_networks_classification/",
            "title": "神经网络大致分类",
            "date_published": "2024-01-31T16:00:00.000Z",
            "content_html": "<p>参考文章：<span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNjg3MDk2MTg=\">https://zhuanlan.zhihu.com/p/268709618</span><br>\n<img data-src=\"/images/AI/Neural_networks_classification/0.1.png\" alt=\"\"></p>\n<h1 id=\"一-mp神经元模型\"><a class=\"markdownIt-Anchor\" href=\"#一-mp神经元模型\">#</a> 一、MP 神经元模型</h1>\n<p>  MP 模型是针对生物神经元的一些基本生理特征所提出的形式神经元的数学模型与结构，其权值被认为是不可调整的。MP 神经元模型是其他神经网络的基础。<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.1.png\" alt=\"\"></p>\n<h1 id=\"二-前馈神经网络fnn\"><a class=\"markdownIt-Anchor\" href=\"#二-前馈神经网络fnn\">#</a> 二、前馈神经网络（FNN）</h1>\n<p>  对于前馈网络，根据神经元的传递函数不同，以及学习算法和网络结构上的区别，可以细分类感知器网络、线性网络、BP 网络、径向基网络及 GMDH 网络等不同的网络模型。</p>\n<h2 id=\"21-感知器pla\"><a class=\"markdownIt-Anchor\" href=\"#21-感知器pla\">#</a> 2.1 感知器（PLA）</h2>\n<p>  感知器模型（Percetron Learning Algorithm，简称 PLA）是由美国学者 F.Rosenblatt 于 1958 年提出的。它与 MP 模型的不同之处是它假定神经元的突触权值是可变的，这样就可以进行学习。感知器是最简单形式的前馈神经网络，是一种二元线性分类器。<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.2.png\" alt=\"\"><br>\n  感知器具有如下的局限性：</p>\n<ul>\n<li>感知器神经网络的传输函数一般采用阈值函数，所以输出值只有两种；</li>\n<li>单层感知器网络只能用于解决线性可分的分类问题，而对线性不可分的分类问题无能为力；</li>\n<li>感知器学习算法只适于单层感知器网络，所以一般感知器网络都是单层的。</li>\n</ul>\n<h2 id=\"22-多层感知器mlp\"><a class=\"markdownIt-Anchor\" href=\"#22-多层感知器mlp\">#</a> 2.2 多层感知器（MLP）</h2>\n<p>  多层感知器（Multilayer Perceptron, 简称 MLP）是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。对于非线性函数的模拟，需要采用 MLP，即在最初的输入和输出层之间隐藏着一到多个层。<br>\n  <mark>全连接神经网络（Fully Connected Neural Network，简称 FNN），深度神经网络（ Deep Neural Networks，简称 DNN）和 MLP 的概念相似，只是侧重点不同。一个多层全连接神经网络即使 MLP，又是 FNN，同时也是 DNN。</mark><br>\n  <mark>BP 神经网络是指使用了 BP 算法（ Back Propagation，反向传播）进行训练的 MLP 模型。</mark></p>\n<h2 id=\"23-径向基神经网络rbfnn\"><a class=\"markdownIt-Anchor\" href=\"#23-径向基神经网络rbfnn\">#</a> 2.3 径向基神经网络（RBFNN）</h2>\n<p>参考链接： <span class=\"exturl\" data-url=\"aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDgwMjY3Ni9hcnRpY2xlL2RldGFpbHMvMTAwODA1NTQ1\">https://blog.csdn.net/weixin_40802676/article/details/100805545</span><br>\n  首先介绍径向基函数（ Radial Basis Function，简称 RBF）：<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.3.png\" alt=\"\"><br>\n  最常用的径向基函数是高斯函数（radbas）。<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.4.png\" alt=\"\"><br>\n  RBFNN 的神经网络节后如上图所示。三层的神经网络就可以拟合任何一个函数，RBFNN 即为三层（单隐层）且隐藏层使用径向基函数的神经网络。因此 RBFNN 完全可以拟合任何一个函数（只要隐藏层神经元足够多）。输入层到隐藏层的神经元之间的权重全部为 1。隐藏层是使用径向基函数作为激活函数的神经元。隐藏层与输出层之间就是普通的神经网络的连接关系，他们之间的权重可以训练而改变。<br>\n  RBFNN 的关键就在于径向基函数的确定，中心点在哪，径基宽度多大，多少个径向基函数，都是会影响神经网络的效果的。广义回归神经网络 (General Regression Neural Network，简称 GRNN) 和广义回归神经网络 (General Regression Neural Network，简称 GPNN) 都是 RBFNN 的变化形式。</p>\n<h2 id=\"24-卷积神经网络cnn\"><a class=\"markdownIt-Anchor\" href=\"#24-卷积神经网络cnn\">#</a> 2.4 卷积神经网络（CNN）</h2>\n<p>  卷积神经网络（Convolutional Neural Networks，简称 CNN）是一种深度学习模型或类似于人工神经网络的多层感知器，常用来分析视觉图像。<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.5.png\" alt=\"\"><br>\n  一个卷积神经网络主要由以下 5 层组成：</p>\n<ul>\n<li>数据输入层 / Input layer</li>\n<li>卷积计算层 / CONV layer</li>\n<li>ReLU 激励层 / ReLU layer</li>\n<li>池化层     / Pooling layer</li>\n<li>全连接层   / FC layer</li>\n</ul>\n<h2 id=\"25-线性神经网咯\"><a class=\"markdownIt-Anchor\" href=\"#25-线性神经网咯\">#</a> 2.5 线性神经网咯</h2>\n<p>  线性神经网络与感知器的主要区别在于感知器的激活函数只能输出两种可能值（-1 或 1），而线性神经网络的输出可以取任意值，其激活函数是线性函数。<br>\n  线性神经网络采用 Widrow-Hoff 学习规则（最小均方规则），即 LMS（Least Mean Square）算法来调整网络的权值和偏置值。结构图如下。这里使用 purelin 激活函数进行模型训练，这样可以得到一个更好的效果。输出结果的时候还是使用 sign 激活函数。<br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.6.png\" alt=\"\"><br>\n<img data-src=\"/images/AI/Neural_networks_classification/1.7.png\" alt=\"\"></p>\n<h1 id=\"三-反馈神经网络\"><a class=\"markdownIt-Anchor\" href=\"#三-反馈神经网络\">#</a> 三、反馈神经网络</h1>\n<p>  所谓反馈网络是指在网络中至少含有一个反馈回路的神经网络。反馈网络可以包含一个单层神经元，其中每个神经元将自身的输出信号反馈给其他所有神经元的输入 反馈神经网络中神经元不但可以接收其他神经元的信号，而且可以接收自己的反馈信号。和前馈神经网络相比，反馈神经网络中的神经元具有记忆功能，在不同时刻具有不同的状态。反馈神经网络中的信息传播可以是单向也可以是双向传播，因此可以用一个有向循环图或者无向图来表示。</p>\n<h2 id=\"31-循环神经网络rnn\"><a class=\"markdownIt-Anchor\" href=\"#31-循环神经网络rnn\">#</a> 3.1 循环神经网络（RNN）</h2>\n<p>  循环神经网络（Recurrent Neural Network，简称 RNN）是一种深度学习模型，专门用于处理序列数据和具有时间依赖性的任务。相比于传统神经网络，RNN 具有一种递归结构，使其能够对序列信息进行处理。<br>\n  RNN 的主要特点是它能够保持对之前输入信息的记忆，这使得它在处理时间序列、自然语言处理等任务时非常有效。它的基本结构包括一个隐藏层，其中的神经元可以接收输入数据和前一个时间步的隐藏状态，并输出一个新的隐藏状态。这种递归结构使得 RNN 能够捕捉序列中的上下文信息，从而更好地理解和处理序列数据。<br>\n  然而，传统的 RNN 存在梯度消失和梯度爆炸等问题，导致难以捕捉长期依赖关系。为了解决这些问题，一些改进型的循环神经网络被提出，如长短时记忆网络（Long Short-Term Memory，简称 LSTM）和门控循环单元（Gated Recurrent Unit，简称 GRU）。这些改进模型引入了门控机制，使得网络能够更好地处理长期依赖关系，从而提高了性能。<br>\n  总的来说，循环神经网络在处理序列数据方面具有广泛的应用，包括自然语言处理、语音识别、时间序列预测等领域。</p>\n<h2 id=\"32-hopfield神经网络\"><a class=\"markdownIt-Anchor\" href=\"#32-hopfield神经网络\">#</a> 3.2 Hopfield 神经网络</h2>\n<p>  Hopfield 神经网络是一种反馈型的人工神经网络，最初由物理学家约翰・霍普菲尔德（John Hopfield）于 1982 年提出。它主要用于模拟和处理离散型动力系统，尤其在解决优化问题和模式识别方面应用广泛。根据其激活函数的不同，Hopfield 神经网络有两种：离散 Hopfield 网络（Discrete Hopfield Neural Network，简称 DHNN）和连续 Hopfield 网络（Continues Hopfield Neural Network，简称 CHNN）。</p>\n<h1 id=\"四-对抗神经网络gan\"><a class=\"markdownIt-Anchor\" href=\"#四-对抗神经网络gan\">#</a> 四、对抗神经网络（GAN）</h1>\n<p>  简介：对抗神经网络其实是两个网络的组合，可以理解为一个网络生成模拟数据，另一个网络判断生成的数据是真实的还是模拟的。生成模拟数据的网络要不断优化自己让判别的网络判断不出来，判别的网络也要不断优化自己让判断的更加精确。两者的关系形成对抗，因此叫对抗神经网络。<br>\n  结构：GAN 由 generator（生成模型）和 discriminator（判别式模型）两部分构成。二者结合之后，经过大量次数的迭代训练会使 generator 尽可能模拟出以假乱真的样本，而 discrimator 会有更精确的鉴别真伪数据的能力，最终整个 GAN 会达到所谓的纳什均衡，即 discriminator 对于 generator 的数据鉴别结果为正确率和错误率各占 50%。</p>\n<h1 id=\"五-自组织神经网络\"><a class=\"markdownIt-Anchor\" href=\"#五-自组织神经网络\">#</a> 五、自组织神经网络</h1>\n<p>  在生物神经系统中，存在着一种侧抑制现象，即一个神经细胞兴奋以后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织（竞争型）神经网络就是模拟上述生物神经系统功能的人工神经网络。</p>\n<h1 id=\"六-反馈神经网络\"><a class=\"markdownIt-Anchor\" href=\"#六-反馈神经网络\">#</a> 六、反馈神经网络</h1>\n<p>  一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阙值等参数。与此不同，结构自适应网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最利合数据特点的网络结构</p>\n<h1 id=\"七-反馈神经网络\"><a class=\"markdownIt-Anchor\" href=\"#七-反馈神经网络\">#</a> 七、反馈神经网络</h1>\n<p>  随机神经网络是对神经网络引入随机机制，认为神经元是按照概率的原理进行工作的，这就是说，每个神经元的兴奋或抑制具有随机性，其概率取决于神经元的输入。</p>\n",
            "tags": [
                "AI"
            ]
        }
    ]
}