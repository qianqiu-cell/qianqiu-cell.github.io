



<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="Keep Moving" href="http://qianqiu-cell.github.io/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="Keep Moving" href="http://qianqiu-cell.github.io/atom.xml" />
<link rel="alternate" type="application/json" title="Keep Moving" href="http://qianqiu-cell.github.io/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="AI" />


<link rel="canonical" href="http://qianqiu-cell.github.io/2024/05/16/AI/LLM/">



  <title>
大模型LLM学习 - AI |
唯爱ぺ灬babyル = Keep Moving = 天将降大任于斯人也</title>
<meta name="generator" content="Hexo 6.3.0"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">大模型LLM学习
  </h1>
  
<div class="meta">
  <span class="item" title="创建时间：2024-05-16 00:00:00">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">发表于</span>
    <time itemprop="dateCreated datePublished" datetime="2024-05-16T00:00:00+08:00">2024-05-16</time>
  </span>
  <span class="item" title="本文字数">
    <span class="icon">
      <i class="ic i-pen"></i>
    </span>
    <span class="text">本文字数</span>
    <span>15k</span>
    <span class="text">字</span>
  </span>
  <span class="item" title="阅读时长">
    <span class="icon">
      <i class="ic i-clock"></i>
    </span>
    <span class="text">阅读时长</span>
    <span>14 分钟</span>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="切换导航栏">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">唯爱ぺ灬babyル</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://i.postimg.cc/0yS7gzhX/img-2000891315.jpg"></li>
          <li class="item" data-background-image="https://i.postimg.cc/tRfb1Jmz/img-2000880084.jpg"></li>
          <li class="item" data-background-image="https://i.postimg.cc/nLq3YFxg/img-2000959067.jpg"></li>
          <li class="item" data-background-image="https://i.postimg.cc/DzqTN4Lc/img-2001089444.jpg"></li>
          <li class="item" data-background-image="https://i.postimg.cc/yYQfPpZh/img-2001032858.jpg"></li>
          <li class="item" data-background-image="https://i.postimg.cc/yxJwXq8G/img-2001040529.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">首页</a></span><i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/AI/" itemprop="item" rel="index" title="分类于 AI"><span itemprop="name">AI</span></a>
<meta itemprop="position" content="1" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN">
  <link itemprop="mainEntityOfPage" href="http://qianqiu-cell.github.io/2024/05/16/AI/LLM/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="Ember">
    <meta itemprop="description" content="天将降大任于斯人也, 🌸学习笔记🌸">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keep Moving">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <p><img data-src="/images/AI/LLM/0.1.jpg" alt=""></p>
<p>参考链接：<span class="exturl" data-url="aHR0cHM6Ly93YXlsYW5kemhhbmcuZ2l0aHViLmlvL2VuL3RyYW5zZm9ybWVyLWFyY2hpdGVjdHVyZS5odG1sIzQtNy1jYWxjdWxhdGUtdi1hdHRlbnRpb24=">https://waylandzhang.github.io/en/transformer-architecture.html#4-7-calculate-v-attention</span>、<span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMzU0NjYxMTUyNzQ1MzE2MT9zcG1faWRfZnJvbT0zMzMuNzg4LjAuMA==">https://space.bilibili.com/3546611527453161?spm_id_from=333.788.0.0</span></p>
<p>   <code>Transformer</code>  模型由两部分组成：编码器和解码器。一般来说，仅编码器的架构精通于从文本中提取信息，用于分类和回归等任务，而仅解码器的模型专门用于生成文本。例如，<mark>专注于文本生成的 <code>GPT</code>  属于仅解码器模型的类别</mark>。</p>
<p>   <code>Transformer</code>  的大致过程如下：</p>
<ul>
<li>首先，我们需要一系列输入字符作为训练数据。这些输入被转换成矢量嵌入格式。</li>
<li>接下来，我们将位置编码添加到矢量嵌入中，以捕获序列中每个字符的位置。</li>
<li>随后，该模型通过一系列计算操作处理这些输入嵌入，最终为给定的输入文本生成可能的下一个字符的概率分布。</li>
<li>该模型根据训练数据集中的实际后续特征来评估预测结果，并相应地调整概率或 “权重”。</li>
<li>最后，该模型迭代地细化这个过程，不断更新其参数，以提高未来预测的精度。</li>
</ul>
<h1 id="一-tokenizer"><a class="markdownIt-Anchor" href="#一-tokenizer">#</a> 一、Tokenizer</h1>
<p>   <code>Tokenizer</code>  分词算法是 <code>NLP</code>  大模型最基础的组件，基于 <code>Tokenizer</code>  可以<mark>将文本转换成独立的 <code>token</code>  列表，进而转换成输入的向量成为计算机可以理解的输入形式</mark>。</p>
<p>   <code>tiktoken</code>  是一种快速  <code>BPE</code>  标记器，可与 <code>OpenAI</code>  模型一起使用。</p>
<p>   <code>tiktoken</code>  使用方法为：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> tiktoken</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment"># Using TikToken (Same as GPT3) to tokenize the source text</span></pre></td></tr><tr><td data-num="3"></td><td><pre>encoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span><span class="token string">"cl100k_base"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># text 保存了 str 变量类型的文本内容 --> 输出为以单词为单位的 int 列表</span></pre></td></tr><tr><td data-num="5"></td><td><pre>tokenized_text <span class="token operator">=</span> encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>text<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment"># 将 tokenized_text 转换为 Tensor.int64 类型的张量</span></pre></td></tr><tr><td data-num="7"></td><td><pre>tokenized_text <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>tokenized_text<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span></pre></td></tr></table></figure><p>  上述的代码提供了 <code>encoding</code>  的编码过程，同时还可以根据 <code>tokenized</code>  的编码结果进行解码，只需要通过 <code>decode</code>  函数输入 <code>int</code>  类型的列表集合返回原始的 str 类型文本</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 输出单个编码对应的 str 文本</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment"># 输出多个编码对应的 str 文本</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_text<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="二-embedding"><a class="markdownIt-Anchor" href="#二-embedding">#</a> 二、Embedding</h1>
<p>   <code>Tokenize</code>  完的下一步就是将 <code>token</code>  的 <code>one-hot</code>  编码转换成更 <code>dense</code>  的 <code>embedding</code>  编码。</p>
<p>   <code>Embedding</code>  矩阵的本质就是一个查找表。由于输入向量是 <code>one-hot</code>  的， <code>embedding</code>  矩阵中有且仅有一行被激活。行间互不干扰。如下图所示，假设词汇表一共有 <code>6</code>  个词，则 <code>one-hot</code>  表示的长度为 <code>6</code> 。现在我们有三个单词组成一个句子，则输入矩阵的形状为 <code>(3,6)</code>  。然后我们学出来一个 <code>embedding</code>  矩阵，根据上面的推导，如果我们的 <code>embedding size</code> （编码向量的长度）为 <code>4</code> ，则 <code>embedding</code>  矩阵的形状应该为 <code>(6,4)</code> 。这样乘出来的输出矩阵的形状应为 <code>(3,4)</code> 。</p>
<p><img data-src="/images/AI/LLM/2.1.jpg" alt=""></p>
<p>  对于第一个单词 <code>I</code> ，假设其 <code>one-hot</code>  编码为 <code>[0,0,1,0,0,0]</code> ，将其与 <code>embedding</code>  矩阵相乘，相当于取出 <code>embedding</code>  矩阵的第 <code>3</code>  行（ <code>index</code>  为 <code>2</code> ）。同理，对于单词 <code>love</code> ，相当于取出 <code>embedding</code>  矩阵的第二行（ <code>index</code>  为 <code>1</code> ）。因此 <code>embedding</code>  矩阵的本质是一个查找表，每个单词会定位这个表中的某一行，而这一行就是这个单词学习到的在嵌入空间的语义。</p>
<p>  首先准备所需要的数据，在 transformer 的解码器中，需要输入 <code>n_batch * context_length * d_model</code>  维度的 <code>Tensor</code>  数据，其中 <code>n_batch</code>  表示批次大小， <code>contex_length</code>  表示一次输入的单次数量， <code>d_model</code>  表示编码向量的长度。将 <code>Tokenizer</code>  中获得的 <code>tokenized_text</code>  进行数据预处理：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Split train and validation</span></pre></td></tr><tr><td data-num="2"></td><td><pre>split_idx <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.9</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>train_data <span class="token operator">=</span> tokenized_text<span class="token punctuation">[</span><span class="token punctuation">:</span>split_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>val_data <span class="token operator">=</span> tokenized_text<span class="token punctuation">[</span>split_idx<span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment"># Get input embedding batch</span></pre></td></tr><tr><td data-num="7"></td><td><pre>data <span class="token operator">=</span> train_data</pre></td></tr><tr><td data-num="8"></td><td><pre>idxs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token operator">-</span> context_length<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>x_batch <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">:</span>idx <span class="token operator">+</span> context_length<span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> idxs<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>y_batch <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>data<span class="token punctuation">[</span>idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>idx <span class="token operator">+</span> context_length <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> idxs<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr></table></figure><p>  之后便可以利用 <code>torch</code>  中的 <code>nn.Embedding</code>  函数构造 <code>Embedding</code>  层。其中 <code>Embedding.weight.data</code>  是一个 <code>max_token_value * d_model</code>  维度的 <code>Tensor</code>  变量，是模型需要训练的参数，同时也是上图中对应的 <code>Embedding</code>  查找表。</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># define input embedding table</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment"># 获取 tokenized_text 中的最大值 + 1，用于构造 Embedding 的行</span></pre></td></tr><tr><td data-num="3"></td><td><pre>max_token_value <span class="token operator">=</span> tokenized_text<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># 使用 nn.Embedding 函数构造 Embedding 层</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment"># `Embedding.weight.data` 是一个 `max_token_value * d_model` 维度的 `Tensor` 变量</span></pre></td></tr><tr><td data-num="6"></td><td><pre>token_embedding_lookup_table <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>max_token_value<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>d_model<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token comment"># 通过输入 x_batch 或 y_batch 即可获得对应的 Embedding 编码结果</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment"># x_batch_embedding 和 y_batch_embedding 是 `n_batch * context_length * d_model` 维度的 `Tensor` 数据</span></pre></td></tr><tr><td data-num="9"></td><td><pre>x_batch_embedding <span class="token operator">=</span> token_embedding_lookup_table<span class="token punctuation">(</span>x_batch<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>y_batch_embedding <span class="token operator">=</span> token_embedding_lookup_table<span class="token punctuation">(</span>y_batch<span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="三-position-encoding"><a class="markdownIt-Anchor" href="#三-position-encoding">#</a> 三、Position Encoding</h1>
<p>  在 <code>transformer</code>  的 <code>encoder</code>  和 <code>decoder</code>  的输入层中，均使用了 <code>Positional Encoding</code> ，使得最终的输入满足： <code>input = input_embedding + positional_encoding</code> 。</p>
<p>   <code>Transformer</code>  位置编码的定义为：</p>
<p><img data-src="/images/AI/LLM/3.1.png" alt=""></p>
<p>  实现位置编码的代码为：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># get positional encoding</span></pre></td></tr><tr><td data-num="2"></td><td><pre>position_encoding_lookup_table <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>context_length<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment"># unsqueeze 用来扩充一个维度，为了后面的逐元素计算时的广播机制</span></pre></td></tr><tr><td data-num="4"></td><td><pre>position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> context_length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment"># 根据公式计算位置编码</span></pre></td></tr><tr><td data-num="6"></td><td><pre>div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">-</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>position_encoding_lookup_table<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>position_encoding_lookup_table<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token comment"># 将 context_length*d_model 的矩阵复制 n_epoch 次，形成 n_epoch*context_length*d_model 的矩阵</span></pre></td></tr><tr><td data-num="10"></td><td><pre>position_encoding_lookup_table <span class="token operator">=</span> position_encoding_lookup_table<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>  在获得位置编码之后即可将位置编码与 <code>Embedding</code>  进行相加，获得最终输入至网络的输入：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># add positional encoding to the input_embedding</span></pre></td></tr><tr><td data-num="2"></td><td><pre>x <span class="token operator">=</span> x_batch_embedding <span class="token operator">+</span> position_encoding_lookup_table</pre></td></tr><tr><td data-num="3"></td><td><pre>y <span class="token operator">=</span> y_batch_embedding <span class="token operator">+</span> position_encoding_lookup_table</pre></td></tr></table></figure><h1 id="四-transformer-block"><a class="markdownIt-Anchor" href="#四-transformer-block">#</a> 四、Transformer Block</h1>
<p><img data-src="/images/AI/LLM/4.1.png" alt=""></p>
<p>  通过第三步，我们获得了输入 <code>x</code> ，下一步是开始实现多头注意力块（ <code>Muti-head Attention block</code> ）。</p>
<p>   <code>Transformer</code>  模型的强大来源于 <code>self-attention</code> ，通过 <code>self-attention</code> ， <code>Transformer</code>  模型可以关注到 <code>input</code>  更加重要的部分。</p>
<p>   <code>Multi-head attention</code>  由几个单独的 <code>heads</code>  堆叠在一起组成。所有 heads 都接收到完全相同的输入，尽管它们在计算过程中使用了自己的特定权重集。在处理输入之后，来自所有 <code>heads</code>  的输出被级联，然后通过线性层。</p>
<p>   <code>heads</code>  的工作方式是通过三个独特的层处理，即查询（ <code>Q</code> ）、键（ <code>K</code> ）和值（ <code>V</code> ）。 <code>Attention</code>  的计算公式可以从论文《 <code>Attention is all you need</code> 》中得到：</p>
<p><img data-src="/images/AI/LLM/4.2.png" alt=""></p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># get Q, K, V</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment"># 所谓的多头就是把 d_model 切成多份，每一个头里面有一部分维度，然后去做这一部分的计算，最后再把所有的计算合并在一起</span></pre></td></tr><tr><td data-num="3"></td><td><pre>head_size <span class="token operator">=</span> d_model <span class="token operator">//</span> num_heads  <span class="token comment"># head size should be divisible by d_model</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># (1) 计算 Q,K,V 矩阵</span></pre></td></tr><tr><td data-num="5"></td><td><pre>key_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>query_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>value_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>d_model<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num="9"></td><td><pre>q <span class="token operator">=</span> query_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>k <span class="token operator">=</span> key_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>v <span class="token operator">=</span> value_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token comment"># [batch_size, context_length, num_heads, head_size]</span></pre></td></tr><tr><td data-num="13"></td><td><pre>q <span class="token operator">=</span> q<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> head_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>k <span class="token operator">=</span> k<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> head_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>v <span class="token operator">=</span> v<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> head_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token comment"># [batch_size, num_heads, context_length, head_size]</span></pre></td></tr><tr><td data-num="17"></td><td><pre>q <span class="token operator">=</span> q<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>k <span class="token operator">=</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>v <span class="token operator">=</span> v<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre><span class="token comment"># (2) 通过 Q @ K^T /sqrt (d_k) 计算 Attention</span></pre></td></tr><tr><td data-num="21"></td><td><pre>attention_score <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>head_size<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre><span class="token comment"># (3) 计算 Mask</span></pre></td></tr><tr><td data-num="23"></td><td><pre>mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>context_length<span class="token punctuation">,</span> context_length<span class="token punctuation">)</span><span class="token punctuation">,</span> diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre>attention_score <span class="token operator">=</span> attention_score<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'-inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre><span class="token comment"># (4) 计算 Softmax</span></pre></td></tr><tr><td data-num="26"></td><td><pre><span class="token comment"># [batch_size, num_heads, context_length, context_length]</span></pre></td></tr><tr><td data-num="27"></td><td><pre>attention_score <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre><span class="token comment"># (5) 通过 $V 计算 A</span></pre></td></tr><tr><td data-num="29"></td><td><pre><span class="token comment"># [batch_size, num_heads, context_length, head_size]</span></pre></td></tr><tr><td data-num="30"></td><td><pre>A <span class="token operator">=</span> attention_score @ v</pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token comment"># (6) 计算 Concatenate</span></pre></td></tr><tr><td data-num="32"></td><td><pre><span class="token comment"># [batch_size, context_length, num_heads, head_size]</span></pre></td></tr><tr><td data-num="33"></td><td><pre>A <span class="token operator">=</span> A<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="34"></td><td><pre><span class="token comment"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num="35"></td><td><pre>A <span class="token operator">=</span> A<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre><span class="token comment"># (7) 通过 Wo 计算 Output</span></pre></td></tr><tr><td data-num="37"></td><td><pre><span class="token comment"># Define the output weight matrix</span></pre></td></tr><tr><td data-num="38"></td><td><pre>Wo <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="39"></td><td><pre><span class="token comment"># [batch_size, context_length, d_model]</span></pre></td></tr><tr><td data-num="40"></td><td><pre>output <span class="token operator">=</span> Wo<span class="token punctuation">(</span>A<span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="五-residual-connection-and-layer-normalization"><a class="markdownIt-Anchor" href="#五-residual-connection-and-layer-normalization">#</a> 五、Residual Connection and Layer Normalization</h1>
<p>  残差连接，有时被称为 <code>skip connection</code> ，是让原始输入 <code>X</code>  绕过一个或多个层的连接。通过将原始输入 <code>x</code>  与步骤四多头注意力层的输出 <code>output</code>  相加即可完成操作。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">output = output + x
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span></span></p>
<p>  在残差连接之后，过程进入层归一化。层归一化（ <code>LayerNorm</code> ）是一种用于对网络中每一层的输出进行归一化的技术。其方法是减去输出的均值，并除以输出的标准差。使用这种技术是为了防止某一层的输出变得过大或过小，从而避免网络的不稳定性。</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Add residual connection</span></pre></td></tr><tr><td data-num="2"></td><td><pre>output <span class="token operator">=</span> output <span class="token operator">+</span> X</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># Add Layer Normalization</span></pre></td></tr><tr><td data-num="5"></td><td><pre>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="六-feed-forward-network"><a class="markdownIt-Anchor" href="#六-feed-forward-network">#</a> 六、Feed-Forward Network</h1>
<p>  一旦我们获得了归一化的注意力权重（概率分数），它将被传递到一个位置级前馈网络中进行处理。前馈神经网络（ <code>FFN</code> ）由两个线性层和它们之间的 ReLU 激活函数组成。</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># update x</span></pre></td></tr><tr><td data-num="2"></td><td><pre>x <span class="token operator">=</span> output</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># Define Feed Forward Network</span></pre></td></tr><tr><td data-num="5"></td><td><pre>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>output <span class="token operator">=</span> torch<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>output<span class="token punctuation">,</span> p<span class="token operator">=</span>dropout<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token comment"># Add residual connection</span></pre></td></tr><tr><td data-num="11"></td><td><pre>output <span class="token operator">=</span> output <span class="token operator">+</span> x</pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token comment"># Add Layer Normalization</span></pre></td></tr><tr><td data-num="13"></td><td><pre>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="七-repeat-step-4-to-6"><a class="markdownIt-Anchor" href="#七-repeat-step-4-to-6">#</a> 七、Repeat step 4 to 6</h1>
<p>  以上我们完成的只是一个 <code>transformer</code>  块。在实际应用中，我们会将多个 <code>transformer</code>  块堆叠在一起，形成一个 <code>transformer</code>  解码器。</p>
<p>  实际上，我们应该将代码封装到类中，并使用 <code>PyTorch</code>  的 <code>nn.Module</code>  来构建我们的 <code>transformer</code>  解码器。但为了演示，我们只使用一个块。</p>
<h1 id="八-output-probabilities"><a class="markdownIt-Anchor" href="#八-output-probabilities">#</a> 八、Output Probabilities</h1>
<p>  应用最后一个线性层来获得我们的 <code>logits</code> ：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre>logits <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> max_token_value<span class="token punctuation">)</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></pre></td></tr></table></figure><p>  最后一步是对逻辑回归输出进行 <code>softmax</code>  操作，以获得每个 <code>token</code>  的概率：</p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment"># but for illustration purpose, we'll use torch.softmax here</span></pre></td></tr><tr><td data-num="3"></td><td><pre>probabilities <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h1 id="full-working-code"><a class="markdownIt-Anchor" href="#full-working-code">#</a> Full Working Code</h1>
<p>  完整的代码可以参考 <code>github</code> : <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dheWxhbmR6aGFuZy9UcmFuc2Zvcm1lci1mcm9tLXNjcmF0Y2g=">https://github.com/waylandzhang/Transformer-from-scratch</span></p>
<figure class="highlight py"><figcaption data-lang="Python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> os</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> requests</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> math</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> tiktoken</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token comment"># Hyperparameters</span></pre></td></tr><tr><td data-num="10"></td><td><pre>batch_size <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># How many batches per training step</span></pre></td></tr><tr><td data-num="11"></td><td><pre>context_length <span class="token operator">=</span> <span class="token number">16</span>  <span class="token comment"># Length of the token chunk each batch</span></pre></td></tr><tr><td data-num="12"></td><td><pre>d_model <span class="token operator">=</span> <span class="token number">64</span>  <span class="token comment"># The size of our model token embeddings</span></pre></td></tr><tr><td data-num="13"></td><td><pre>num_blocks <span class="token operator">=</span> <span class="token number">8</span>  <span class="token comment"># Number of transformer blocks</span></pre></td></tr><tr><td data-num="14"></td><td><pre>num_heads <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># Number of heads in Multi-head attention</span></pre></td></tr><tr><td data-num="15"></td><td><pre>learning_rate <span class="token operator">=</span> <span class="token number">1e-3</span>  <span class="token comment"># 0.001</span></pre></td></tr><tr><td data-num="16"></td><td><pre>dropout <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># Dropout rate</span></pre></td></tr><tr><td data-num="17"></td><td><pre>max_iters <span class="token operator">=</span> <span class="token number">5000</span>  <span class="token comment"># Total of training iterations &lt;- Change this to smaller number for testing</span></pre></td></tr><tr><td data-num="18"></td><td><pre>eval_interval <span class="token operator">=</span> <span class="token number">50</span>  <span class="token comment"># How often to evaluate</span></pre></td></tr><tr><td data-num="19"></td><td><pre>eval_iters <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># Number of iterations to average for evaluation</span></pre></td></tr><tr><td data-num="20"></td><td><pre>device <span class="token operator">=</span> <span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span>  <span class="token comment"># Use GPU if it's available.</span></pre></td></tr><tr><td data-num="21"></td><td><pre>TORCH_SEED <span class="token operator">=</span> <span class="token number">1337</span></pre></td></tr><tr><td data-num="22"></td><td><pre>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>TORCH_SEED<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre><span class="token comment"># Load training data</span></pre></td></tr><tr><td data-num="25"></td><td><pre><span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">'data/sales_textbook.txt'</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="26"></td><td><pre>    url <span class="token operator">=</span> <span class="token string">'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'</span></pre></td></tr><tr><td data-num="27"></td><td><pre>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'data/sales_textbook.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="28"></td><td><pre>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre></pre></td></tr><tr><td data-num="30"></td><td><pre><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'data/sales_textbook.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="31"></td><td><pre>    text <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre><span class="token comment"># Using TikToken (Same as GPT3) to tokenize the source text</span></pre></td></tr><tr><td data-num="34"></td><td><pre>encoding <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>get_encoding<span class="token punctuation">(</span><span class="token string">"cl100k_base"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre>tokenized_text <span class="token operator">=</span> encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>text<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre>max_token_value <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>  <span class="token comment"># the maximum value of the tokenized numbers</span></pre></td></tr><tr><td data-num="37"></td><td><pre>tokenized_text <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>tokenized_text<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>  <span class="token comment"># put tokenized text into tensor</span></pre></td></tr><tr><td data-num="38"></td><td><pre></pre></td></tr><tr><td data-num="39"></td><td><pre><span class="token comment"># Split train and validation</span></pre></td></tr><tr><td data-num="40"></td><td><pre>split_idx <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.9</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="41"></td><td><pre>train_data <span class="token operator">=</span> tokenized_text<span class="token punctuation">[</span><span class="token punctuation">:</span>split_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="42"></td><td><pre>val_data <span class="token operator">=</span> tokenized_text<span class="token punctuation">[</span>split_idx<span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="43"></td><td><pre></pre></td></tr><tr><td data-num="44"></td><td><pre></pre></td></tr><tr><td data-num="45"></td><td><pre><span class="token comment"># Define Feed Forward Network</span></pre></td></tr><tr><td data-num="46"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">FeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="47"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="48"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="49"></td><td><pre>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</pre></td></tr><tr><td data-num="50"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</pre></td></tr><tr><td data-num="51"></td><td><pre>        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="52"></td><td><pre>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="53"></td><td><pre>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="54"></td><td><pre>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="55"></td><td><pre>            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="56"></td><td><pre>        <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="57"></td><td><pre></pre></td></tr><tr><td data-num="58"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="59"></td><td><pre>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="60"></td><td><pre></pre></td></tr><tr><td data-num="61"></td><td><pre></pre></td></tr><tr><td data-num="62"></td><td><pre><span class="token comment"># Define Scaled Dot Product Attention</span></pre></td></tr><tr><td data-num="63"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="64"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="65"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="66"></td><td><pre>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</pre></td></tr><tr><td data-num="67"></td><td><pre>        self<span class="token punctuation">.</span>head_size <span class="token operator">=</span> head_size</pre></td></tr><tr><td data-num="68"></td><td><pre>        self<span class="token punctuation">.</span>context_length <span class="token operator">=</span> context_length</pre></td></tr><tr><td data-num="69"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</pre></td></tr><tr><td data-num="70"></td><td><pre></pre></td></tr><tr><td data-num="71"></td><td><pre>        self<span class="token punctuation">.</span>key_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>head_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="72"></td><td><pre>        self<span class="token punctuation">.</span>query_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>head_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="73"></td><td><pre>        self<span class="token punctuation">.</span>value_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>head_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="74"></td><td><pre>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'tril'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tril<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="75"></td><td><pre>            torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>context_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>context_length<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Lower triangular mask</span></pre></td></tr><tr><td data-num="76"></td><td><pre>        self<span class="token punctuation">.</span>dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="77"></td><td><pre></pre></td></tr><tr><td data-num="78"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="79"></td><td><pre>        B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape  <span class="token comment"># Batch size, Time steps(current context_length), Channels(dimensions)</span></pre></td></tr><tr><td data-num="80"></td><td><pre>        <span class="token keyword">assert</span> T <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>context_length</pre></td></tr><tr><td data-num="81"></td><td><pre>        <span class="token keyword">assert</span> C <span class="token operator">==</span> self<span class="token punctuation">.</span>d_model</pre></td></tr><tr><td data-num="82"></td><td><pre>        q <span class="token operator">=</span> self<span class="token punctuation">.</span>query_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="83"></td><td><pre>        k <span class="token operator">=</span> self<span class="token punctuation">.</span>key_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="84"></td><td><pre>        v <span class="token operator">=</span> self<span class="token punctuation">.</span>value_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="85"></td><td><pre></pre></td></tr><tr><td data-num="86"></td><td><pre>        <span class="token comment"># Scaled dot product attention: Q @ K^T / sqrt(d_k)</span></pre></td></tr><tr><td data-num="87"></td><td><pre>        weights <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>k<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="88"></td><td><pre>        <span class="token comment"># Apply masked attention</span></pre></td></tr><tr><td data-num="89"></td><td><pre>        weights <span class="token operator">=</span> weights<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tril<span class="token punctuation">[</span><span class="token punctuation">:</span>T<span class="token punctuation">,</span> <span class="token punctuation">:</span>T<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'-inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="90"></td><td><pre>        weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="91"></td><td><pre>        weights <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_layer<span class="token punctuation">(</span>weights<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="92"></td><td><pre></pre></td></tr><tr><td data-num="93"></td><td><pre>        <span class="token comment"># Apply dot product attention: weights @ V</span></pre></td></tr><tr><td data-num="94"></td><td><pre>        out <span class="token operator">=</span> weights @ v</pre></td></tr><tr><td data-num="95"></td><td><pre>        <span class="token keyword">return</span> out</pre></td></tr><tr><td data-num="96"></td><td><pre></pre></td></tr><tr><td data-num="97"></td><td><pre></pre></td></tr><tr><td data-num="98"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="99"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="100"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="101"></td><td><pre>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads</pre></td></tr><tr><td data-num="102"></td><td><pre>        self<span class="token punctuation">.</span>head_size <span class="token operator">=</span> head_size</pre></td></tr><tr><td data-num="103"></td><td><pre>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</pre></td></tr><tr><td data-num="104"></td><td><pre>        self<span class="token punctuation">.</span>context_length <span class="token operator">=</span> context_length</pre></td></tr><tr><td data-num="105"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</pre></td></tr><tr><td data-num="106"></td><td><pre></pre></td></tr><tr><td data-num="107"></td><td><pre>        self<span class="token punctuation">.</span>heads <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>Attention<span class="token punctuation">(</span>head_size<span class="token operator">=</span>self<span class="token punctuation">.</span>head_size<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="108"></td><td><pre>        self<span class="token punctuation">.</span>projection_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="109"></td><td><pre>        self<span class="token punctuation">.</span>dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="110"></td><td><pre></pre></td></tr><tr><td data-num="111"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="112"></td><td><pre>        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> h <span class="token keyword">in</span> self<span class="token punctuation">.</span>heads<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="113"></td><td><pre>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>projection_layer<span class="token punctuation">(</span>out<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="114"></td><td><pre>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_layer<span class="token punctuation">(</span>out<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="115"></td><td><pre>        <span class="token keyword">return</span> out</pre></td></tr><tr><td data-num="116"></td><td><pre></pre></td></tr><tr><td data-num="117"></td><td><pre></pre></td></tr><tr><td data-num="118"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="119"></td><td><pre></pre></td></tr><tr><td data-num="120"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="121"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="122"></td><td><pre>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</pre></td></tr><tr><td data-num="123"></td><td><pre>        self<span class="token punctuation">.</span>context_length <span class="token operator">=</span> context_length</pre></td></tr><tr><td data-num="124"></td><td><pre>        self<span class="token punctuation">.</span>head_size <span class="token operator">=</span> d_model <span class="token operator">//</span> num_heads  <span class="token comment"># head size should be divisible by d_model</span></pre></td></tr><tr><td data-num="125"></td><td><pre>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads</pre></td></tr><tr><td data-num="126"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</pre></td></tr><tr><td data-num="127"></td><td><pre></pre></td></tr><tr><td data-num="128"></td><td><pre>        self<span class="token punctuation">.</span>multi_head_attention_layer <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>head_size<span class="token operator">=</span>self<span class="token punctuation">.</span>head_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="129"></td><td><pre>        self<span class="token punctuation">.</span>feed_forward_layer <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="130"></td><td><pre>        self<span class="token punctuation">.</span>layer_norm_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="131"></td><td><pre>        self<span class="token punctuation">.</span>layer_norm_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="132"></td><td><pre></pre></td></tr><tr><td data-num="133"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="134"></td><td><pre>        <span class="token comment"># Note: The order of the operations is different from the original Transformer paper</span></pre></td></tr><tr><td data-num="135"></td><td><pre>        <span class="token comment"># The order here is: LayerNorm -> Multi-head attention -> LayerNorm -> Feed forward</span></pre></td></tr><tr><td data-num="136"></td><td><pre>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>multi_head_attention_layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>layer_norm_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Residual connection</span></pre></td></tr><tr><td data-num="137"></td><td><pre>        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>feed_forward_layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>layer_norm_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Residual connection</span></pre></td></tr><tr><td data-num="138"></td><td><pre>        <span class="token keyword">return</span> x</pre></td></tr><tr><td data-num="139"></td><td><pre></pre></td></tr><tr><td data-num="140"></td><td><pre></pre></td></tr><tr><td data-num="141"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">TransformerLanguageModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="142"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="143"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="144"></td><td><pre>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</pre></td></tr><tr><td data-num="145"></td><td><pre>        self<span class="token punctuation">.</span>context_length <span class="token operator">=</span> context_length</pre></td></tr><tr><td data-num="146"></td><td><pre>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads</pre></td></tr><tr><td data-num="147"></td><td><pre>        self<span class="token punctuation">.</span>num_blocks <span class="token operator">=</span> num_blocks</pre></td></tr><tr><td data-num="148"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout</pre></td></tr><tr><td data-num="149"></td><td><pre>        self<span class="token punctuation">.</span>max_token_value <span class="token operator">=</span> max_token_value</pre></td></tr><tr><td data-num="150"></td><td><pre>        <span class="token comment"># Set up token embedding look-up table</span></pre></td></tr><tr><td data-num="151"></td><td><pre>        self<span class="token punctuation">.</span>token_embedding_lookup_table <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>self<span class="token punctuation">.</span>max_token_value <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="152"></td><td><pre></pre></td></tr><tr><td data-num="153"></td><td><pre>        <span class="token comment"># Run all the transformer blocks</span></pre></td></tr><tr><td data-num="154"></td><td><pre>        <span class="token comment"># Different from original paper, here we add a final layer norm after all the blocks</span></pre></td></tr><tr><td data-num="155"></td><td><pre>        self<span class="token punctuation">.</span>transformer_blocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="156"></td><td><pre>                <span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>num_heads<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_blocks<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+</span></pre></td></tr><tr><td data-num="157"></td><td><pre>                <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="158"></td><td><pre>        <span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="159"></td><td><pre>        self<span class="token punctuation">.</span>language_model_out_linear_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> out_features<span class="token operator">=</span>self<span class="token punctuation">.</span>max_token_value<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="160"></td><td><pre></pre></td></tr><tr><td data-num="161"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> targets<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="162"></td><td><pre>        B<span class="token punctuation">,</span> T <span class="token operator">=</span> idx<span class="token punctuation">.</span>shape</pre></td></tr><tr><td data-num="163"></td><td><pre>        <span class="token triple-quoted-string string">"""</pre></td></tr><tr><td data-num="164"></td><td><pre>        # Set up position embedding look-up table</pre></td></tr><tr><td data-num="165"></td><td><pre>        # following the same approach as the original Transformer paper (Sine and Cosine functions)</pre></td></tr><tr><td data-num="166"></td><td><pre>        """</span></pre></td></tr><tr><td data-num="167"></td><td><pre>        position_encoding_lookup_table <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>context_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="168"></td><td><pre>        position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>context_length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="169"></td><td><pre>        div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">-</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="170"></td><td><pre>        position_encoding_lookup_table<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="171"></td><td><pre>        position_encoding_lookup_table<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="172"></td><td><pre>        <span class="token comment"># change position_encoding_lookup_table from (context_length, d_model) to (T, d_model)</span></pre></td></tr><tr><td data-num="173"></td><td><pre>        position_embedding <span class="token operator">=</span> position_encoding_lookup_table<span class="token punctuation">[</span><span class="token punctuation">:</span>T<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="174"></td><td><pre>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>token_embedding_lookup_table<span class="token punctuation">(</span>idx<span class="token punctuation">)</span> <span class="token operator">+</span> position_embedding</pre></td></tr><tr><td data-num="175"></td><td><pre>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer_blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="176"></td><td><pre>        <span class="token comment"># The "logits" are the output values of our model before applying softmax</span></pre></td></tr><tr><td data-num="177"></td><td><pre>        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>language_model_out_linear_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="178"></td><td><pre></pre></td></tr><tr><td data-num="179"></td><td><pre>        <span class="token keyword">if</span> targets <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="180"></td><td><pre>            B<span class="token punctuation">,</span> T<span class="token punctuation">,</span> C <span class="token operator">=</span> logits<span class="token punctuation">.</span>shape</pre></td></tr><tr><td data-num="181"></td><td><pre>            logits_reshaped <span class="token operator">=</span> logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B <span class="token operator">*</span> T<span class="token punctuation">,</span> C<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="182"></td><td><pre>            targets_reshaped <span class="token operator">=</span> targets<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B <span class="token operator">*</span> T<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="183"></td><td><pre>            loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>logits_reshaped<span class="token punctuation">,</span> target<span class="token operator">=</span>targets_reshaped<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="184"></td><td><pre>        <span class="token keyword">else</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="185"></td><td><pre>            loss <span class="token operator">=</span> <span class="token boolean">None</span></pre></td></tr><tr><td data-num="186"></td><td><pre>        <span class="token keyword">return</span> logits<span class="token punctuation">,</span> loss</pre></td></tr><tr><td data-num="187"></td><td><pre></pre></td></tr><tr><td data-num="188"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> max_new_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="189"></td><td><pre>        <span class="token comment"># idx is (B,T) array of indices in the current context</span></pre></td></tr><tr><td data-num="190"></td><td><pre>        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_new_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="191"></td><td><pre>            <span class="token comment"># Crop idx to the max size of our positional embeddings table</span></pre></td></tr><tr><td data-num="192"></td><td><pre>            idx_crop <span class="token operator">=</span> idx<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>context_length<span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="193"></td><td><pre>            <span class="token comment"># Get predictions</span></pre></td></tr><tr><td data-num="194"></td><td><pre>            logits<span class="token punctuation">,</span> loss <span class="token operator">=</span> self<span class="token punctuation">(</span>idx_crop<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="195"></td><td><pre>            <span class="token comment"># Get the last time step from logits where the dimensions of the logits are (B,T,C)</span></pre></td></tr><tr><td data-num="196"></td><td><pre>            logits_last_timestep <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="197"></td><td><pre>            <span class="token comment"># Apply softmax to get probabilities</span></pre></td></tr><tr><td data-num="198"></td><td><pre>            probs <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>logits_last_timestep<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="199"></td><td><pre>            <span class="token comment"># Sample from the probabilities' distribution.</span></pre></td></tr><tr><td data-num="200"></td><td><pre>            idx_next <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="201"></td><td><pre>            <span class="token comment"># Append the sampled indexes idx_next to idx</span></pre></td></tr><tr><td data-num="202"></td><td><pre>            idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> idx_next<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="203"></td><td><pre>        <span class="token keyword">return</span> idx</pre></td></tr><tr><td data-num="204"></td><td><pre></pre></td></tr><tr><td data-num="205"></td><td><pre></pre></td></tr><tr><td data-num="206"></td><td><pre><span class="token comment"># Initialize the model</span></pre></td></tr><tr><td data-num="207"></td><td><pre>model <span class="token operator">=</span> TransformerLanguageModel<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="208"></td><td><pre>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="209"></td><td><pre></pre></td></tr><tr><td data-num="210"></td><td><pre></pre></td></tr><tr><td data-num="211"></td><td><pre><span class="token comment"># Get input embedding batch</span></pre></td></tr><tr><td data-num="212"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_batch</span><span class="token punctuation">(</span>split<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="213"></td><td><pre>    data <span class="token operator">=</span> train_data <span class="token keyword">if</span> split <span class="token operator">==</span> <span class="token string">'train'</span> <span class="token keyword">else</span> val_data</pre></td></tr><tr><td data-num="214"></td><td><pre>    idxs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> high<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token operator">-</span> context_length<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="215"></td><td><pre>    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>data<span class="token punctuation">[</span>idx<span class="token punctuation">:</span>idx <span class="token operator">+</span> context_length<span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> idxs<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="216"></td><td><pre>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>data<span class="token punctuation">[</span>idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">:</span>idx <span class="token operator">+</span> context_length <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> idxs<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="217"></td><td><pre>    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y</pre></td></tr><tr><td data-num="218"></td><td><pre></pre></td></tr><tr><td data-num="219"></td><td><pre></pre></td></tr><tr><td data-num="220"></td><td><pre><span class="token comment"># Calculate loss</span></pre></td></tr><tr><td data-num="221"></td><td><pre><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="222"></td><td><pre><span class="token keyword">def</span> <span class="token function">estimate_loss</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="223"></td><td><pre>    out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="224"></td><td><pre>    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="225"></td><td><pre>    <span class="token keyword">for</span> split <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'valid'</span><span class="token punctuation">]</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="226"></td><td><pre>        losses <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>eval_iters<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="227"></td><td><pre>        <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>eval_iters<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="228"></td><td><pre>            x_batch<span class="token punctuation">,</span> y_batch <span class="token operator">=</span> get_batch<span class="token punctuation">(</span>split<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="229"></td><td><pre>            logits<span class="token punctuation">,</span> loss <span class="token operator">=</span> model<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> y_batch<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="230"></td><td><pre>            losses<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="231"></td><td><pre>        out<span class="token punctuation">[</span>split<span class="token punctuation">]</span> <span class="token operator">=</span> losses<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="232"></td><td><pre>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="233"></td><td><pre>    <span class="token keyword">return</span> out</pre></td></tr><tr><td data-num="234"></td><td><pre></pre></td></tr><tr><td data-num="235"></td><td><pre></pre></td></tr><tr><td data-num="236"></td><td><pre><span class="token comment"># Use AdamW optimizer</span></pre></td></tr><tr><td data-num="237"></td><td><pre>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>params<span class="token operator">=</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="238"></td><td><pre>tracked_losses <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="239"></td><td><pre><span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="240"></td><td><pre>    <span class="token keyword">if</span> step <span class="token operator">%</span> eval_iters <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> step <span class="token operator">==</span> max_iters <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="241"></td><td><pre>        losses <span class="token operator">=</span> estimate_loss<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="242"></td><td><pre>        tracked_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>losses<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="243"></td><td><pre>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Step:'</span><span class="token punctuation">,</span> step<span class="token punctuation">,</span> <span class="token string">'Training Loss:'</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>losses<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'Validation Loss:'</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="244"></td><td><pre>              <span class="token builtin">round</span><span class="token punctuation">(</span>losses<span class="token punctuation">[</span><span class="token string">'valid'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="245"></td><td><pre></pre></td></tr><tr><td data-num="246"></td><td><pre>    xb<span class="token punctuation">,</span> yb <span class="token operator">=</span> get_batch<span class="token punctuation">(</span><span class="token string">'train'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="247"></td><td><pre>    logits<span class="token punctuation">,</span> loss <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">,</span> yb<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="248"></td><td><pre>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span>set_to_none<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="249"></td><td><pre>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="250"></td><td><pre>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="251"></td><td><pre></pre></td></tr><tr><td data-num="252"></td><td><pre><span class="token comment"># Save the model state dictionary</span></pre></td></tr><tr><td data-num="253"></td><td><pre>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model-ckpt.pt'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="254"></td><td><pre></pre></td></tr><tr><td data-num="255"></td><td><pre><span class="token comment"># Generate</span></pre></td></tr><tr><td data-num="256"></td><td><pre>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="257"></td><td><pre>start <span class="token operator">=</span> <span class="token string">'The salesperson'</span></pre></td></tr><tr><td data-num="258"></td><td><pre>start_ids <span class="token operator">=</span> encoding<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>start<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="259"></td><td><pre>x <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>start_ids<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="260"></td><td><pre>y <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>x<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="261"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'---------------'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="262"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>encoding<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="263"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'---------------'</span><span class="token punctuation">)</span></pre></td></tr></table></figure>
      <div class="tags">
          <a href="/tags/AI/" rel="tag"><i class="ic i-tag"></i> AI</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">更新于</span>
    <time title="修改时间：2024-05-27 21:10:21" itemprop="dateModified" datetime="2024-05-27T21:10:21+08:00">2024-05-27</time>
  </span>
  <span id="2024/05/16/AI/LLM/" class="item leancloud_visitors" data-flag-title="大模型LLM学习" title="阅读次数">
      <span class="icon">
        <i class="ic i-eye"></i>
      </span>
      <span class="text">阅读次数</span>
      <span class="leancloud-visitors-count"></span>
      <span class="text">次</span>
  </span>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2024/05/16/AI/LLM_finetune/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;i.postimg.cc&#x2F;13fT61f1&#x2F;img-2000933420.jpg" title="大模型微调">
  <span class="type">上一篇</span>
  <span class="category"><i class="ic i-flag"></i> AI</span>
  <h3>大模型微调</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/2024/07/23/math/least_square_method/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;i.postimg.cc&#x2F;hG40x9Cz&#x2F;img-2000982503.jpg" title="最小二乘法">
  <span class="type">下一篇</span>
  <span class="category"><i class="ic i-flag"></i> 数学基础</span>
  <h3>最小二乘法</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="文章目录">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-tokenizer"><span class="toc-number">1.</span> <span class="toc-text"> 一、Tokenizer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-embedding"><span class="toc-number">2.</span> <span class="toc-text"> 二、Embedding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-position-encoding"><span class="toc-number">3.</span> <span class="toc-text"> 三、Position Encoding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-transformer-block"><span class="toc-number">4.</span> <span class="toc-text"> 四、Transformer Block</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-residual-connection-and-layer-normalization"><span class="toc-number">5.</span> <span class="toc-text"> 五、Residual Connection and Layer Normalization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD-feed-forward-network"><span class="toc-number">6.</span> <span class="toc-text"> 六、Feed-Forward Network</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83-repeat-step-4-to-6"><span class="toc-number">7.</span> <span class="toc-text"> 七、Repeat step 4 to 6</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB-output-probabilities"><span class="toc-number">8.</span> <span class="toc-text"> 八、Output Probabilities</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#full-working-code"><span class="toc-number">9.</span> <span class="toc-text"> Full Working Code</span></a></li></ol>
      </div>
      <div class="related panel pjax" data-title="系列文章">
        <ul>
          <li><a href="/2024/02/01/AI/Neural_networks_classification/" rel="bookmark" title="神经网络大致分类">神经网络大致分类</a></li><li class="active"><a href="/2024/05/16/AI/LLM/" rel="bookmark" title="大模型LLM学习">大模型LLM学习</a></li><li><a href="/2024/05/16/AI/LLM_finetune/" rel="bookmark" title="大模型微调">大模型微调</a></li><li><a href="/2024/07/23/AI/Prompt_engineering/" rel="bookmark" title="大模型提示词工程（Prompt Engineering）">大模型提示词工程（Prompt Engineering）</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="站点概览">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="Ember"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">Ember</p>
  <div class="description" itemprop="description">🌸学习笔记🌸</div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">77</span>
        <span class="name">文章</span>
      </a>
    </div>
    <div class="item categories">
      <a href="/categories/">
        <span class="count">14</span>
        <span class="name">分类</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">23</span>
        <span class="name">标签</span>
      </a>
    </div>
</nav>

<div class="social">
      <span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3FpYW5xaXUtY2VsbA==" title="https:&#x2F;&#x2F;github.com&#x2F;qianqiu-cell"><i class="ic i-github"></i></span>
      <span class="exturl item email" data-url="bWFpbHRvOjI4MzI1Njc4NTFAcXEuY29t" title="mailto:2832567851@qq.com"><i class="ic i-envelope"></i></span>
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>首页</a>
  </li>

    
  <li class="item">
    <a href="/about/" rel="section"><i class="ic i-user"></i>关于</a>
  </li>

        
  <li class="item dropdown">
      <a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a>
    <ul class="submenu">

        
  <li class="item">
    <a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a>
  </li>

        
  <li class="item">
    <a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a>
  </li>

        
  <li class="item">
    <a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a>
  </li>

  </ul>
    
  <li class="item">
    <a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a>
  </li>

    
  <li class="item">
    <a href="/links/" rel="section"><i class="ic i-magic"></i>links</a>
  </li>


</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/2024/05/16/AI/LLM_finetune/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/2024/07/23/math/least_square_method/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          
<div class="rpost pjax">
  <h2>随机文章</h2>
  <ul>
      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2023/03/01/code/python/python_test/" title="Python test in xxx问题">Python test in xxx问题</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/blog/" title="分类于 博客搭建">博客搭建</a>
</div>

    <span><a href="/2023/01/02/blog/hexo_image/" title="Hexo图片显示异常问题">Hexo图片显示异常问题</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/latex/" title="分类于 latex">latex</a>
</div>

    <span><a href="/2024/03/01/latex/visio_eps/" title="Visio图片导出PDF转EPS格式">Visio图片导出PDF转EPS格式</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2023/03/02/code/python/python_numba/" title="使用numba对python进行加速">使用numba对python进行加速</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2024/02/20/code/python/python_pytorch_transforms/" title="Pytorch数据预处理：transforms的使用">Pytorch数据预处理：transforms的使用</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2023/02/06/code/python/python_note/" title="Pycharm中的特殊注释">Pycharm中的特殊注释</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/blog/" title="分类于 博客搭建">博客搭建</a>
</div>

    <span><a href="/2022/12/24/blog/hexo_change/" title="Hexo文件迁移">Hexo文件迁移</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2023/02/06/code/python/python_bitwise_operators/" title="Python按位运算符">Python按位运算符</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/math/" title="分类于 数学基础">数学基础</a>
</div>

    <span><a href="/2024/07/23/math/least_square_method/" title="最小二乘法">最小二乘法</a></span>
  </li>

      
  <li class="item">
    
<div class="breadcrumb">
<a href="/categories/python/" title="分类于 python">python</a>
</div>

    <span><a href="/2023/03/12/code/python/python_class_inherit/" title="python类的继承">python类的继承</a></span>
  </li>

  </ul>
</div>
<div>
  <h2>最新评论</h2>
  <ul class="leancloud-recent-comment"></ul>
</div>

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2022 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ember @ 唯爱ぺ灬babyル</span>
  </div>
  <div class="count">
    <span class="post-meta-item-icon">
      <i class="ic i-chart-area"></i>
    </span>
    <span title="站点总字数">244k 字</span>

    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="ic i-coffee"></i>
    </span>
    <span title="站点阅读时长">3:41</span>
  </div>
  <div class="powered-by">
    基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2024/05/16/AI/LLM/',
    favicon: {
      show: "（●´3｀●）やれやれだぜ",
      hide: "(´Д｀)大変だ！"
    },
    search : {
      placeholder: "文章搜索",
      empty: "关于 「 ${query} 」，什么也没搜到",
      stats: "${time} ms 内找到 ${hits} 条结果"
    },
    valine: true,fancybox: true,
    copyright: '复制成功，转载请遵守 <i class="ic i-creative-commons"></i> 协议。',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




</body>
</html>
